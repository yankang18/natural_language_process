{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sentiment-network/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('../sentiment-network/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \\nhomelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t    story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly    homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the st'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    }
   ],
   "source": [
    "# Create your dictionary that maps vocab words to integers here\n",
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "# print(counts)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word:i for i, word in enumerate(vocab, 1)}\n",
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of vocab <class 'list'>\n",
      "first word: the\n",
      "last word: sensate\n",
      "1\n",
      "74072\n"
     ]
    }
   ],
   "source": [
    "print(\"type of vocab\", type(vocab))\n",
    "print(\"first word:\", vocab[0])\n",
    "print(\"last word:\", vocab[-1])\n",
    "print(vocab_to_int[\"the\"])\n",
    "print(vocab_to_int[\"sensate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n"
     ]
    }
   ],
   "source": [
    "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "    \n",
    "print(len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "labels = labels.split('\\n')\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to 1s and 0s for 'positive' and 'negative'\n",
    "labels = np.array([1 if label == 'positive' else 0 for label in labels])\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out that review with 0 length\n",
    "non_zero_idx = [idx for idx, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "len(non_zero_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ints[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = [reviews_ints[idx] for idx in non_zero_idx]\n",
    "labels = np.array([labels[idx] for idx in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "140\n",
      "114\n",
      "447\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_ints))\n",
    "print(len(reviews_ints[0]))\n",
    "print(len(reviews_ints[1]))\n",
    "print(len(reviews_ints[2]))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "split = int(len(features) * split_frac)\n",
    "train_x, val_x = features[:split], features[split:]\n",
    "train_y, val_y = labels[:split], labels[split:]\n",
    "\n",
    "split = int(len(val_x) * 0.5)\n",
    "val_x, test_x = val_x[:split], val_x[split:]\n",
    "val_y, test_y = val_y[:split], val_y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
    "      \"Train set: \\t\\t{}\".format(len(train_y)), \n",
    "      \"\\nValidation set: \\t{}\".format(len(val_x)),\n",
    "      \"\\nTest set: \\t\\t{}\".format(len(test_x)))\n",
    "\n",
    "print(len(train_y))\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph\n",
    "\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256 \n",
    "lstm_layers = 1\n",
    "# batch_size = 500\n",
    "batch_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [batch_size, None], name = \"inputs\")\n",
    "    labels_ = tf.placeholder(tf.int32, [batch_size, None], name = \"labels\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_dim = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_dim), -1, 1), name = \"embedding\")\n",
    "    \n",
    "    # What the embedding parameter really is ?\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    # The results of the lookup are concatenated into a dense tensor. \n",
    "    # The returned tensor has shape shape(ids) + shape(params)[1:].\n",
    "    # If    the inputs_ has shape (batch_size, step_num), each element is an id to be looked up in params\n",
    "    #       the params (embedding tensor or lookup table) has shape [n_words, embed_dim], each row is an embedding \n",
    "    #           vector and the index for each row (i.e., the first dimension) is the id of that row.\n",
    "    # Then\n",
    "    #       the returned tensor would have shape (bacth_size, step_num, embed_dim), since each id in inputs is\n",
    "    #       is replaced by an embeding vector looked-up by that id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Your basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    # Note that the cell need the batch size to initialize the cell's internal state\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    # the dynamic rnn takes an arguments named 'inputs' that should have shape of [batch_size, max_time, ...]\n",
    "    # the max_time is the step_num\n",
    "    # The first two dimensions must match across all the inputs, \n",
    "    # but otherwise the ranks and other shape components may differ.\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4 Iteration: 2000 Train loss: 0.028\n",
      "Epoch: 0/4 Iteration: 4000 Train loss: 0.379\n",
      "Epoch: 0/4 Iteration: 6000 Train loss: 0.011\n",
      "Epoch: 0/4 Iteration: 8000 Train loss: 0.021\n",
      "Epoch: 0/4 Iteration: 10000 Train loss: 0.014\n",
      "Val acc: 0.792\n",
      "Epoch: 0/4 Iteration: 12000 Train loss: 0.106\n",
      "Epoch: 0/4 Iteration: 14000 Train loss: 0.393\n",
      "Epoch: 0/4 Iteration: 16000 Train loss: 0.047\n",
      "Epoch: 0/4 Iteration: 18000 Train loss: 0.022\n",
      "Epoch: 0/4 Iteration: 20000 Train loss: 0.030\n",
      "Val acc: 0.817\n",
      "Epoch: 1/4 Iteration: 22000 Train loss: 0.005\n",
      "Epoch: 1/4 Iteration: 24000 Train loss: 0.007\n",
      "Epoch: 1/4 Iteration: 26000 Train loss: 0.000\n",
      "Epoch: 1/4 Iteration: 28000 Train loss: 0.001\n",
      "Epoch: 1/4 Iteration: 30000 Train loss: 0.001\n",
      "Val acc: 0.821\n",
      "Epoch: 1/4 Iteration: 32000 Train loss: 0.054\n",
      "Epoch: 1/4 Iteration: 34000 Train loss: 0.000\n",
      "Epoch: 1/4 Iteration: 36000 Train loss: 0.008\n",
      "Epoch: 1/4 Iteration: 38000 Train loss: 0.002\n",
      "Epoch: 1/4 Iteration: 40000 Train loss: 0.000\n",
      "Val acc: 0.820\n",
      "Epoch: 2/4 Iteration: 42000 Train loss: 0.008\n",
      "Epoch: 2/4 Iteration: 44000 Train loss: 0.020\n",
      "Epoch: 2/4 Iteration: 46000 Train loss: 0.000\n",
      "Epoch: 2/4 Iteration: 48000 Train loss: 0.000\n",
      "Epoch: 2/4 Iteration: 50000 Train loss: 0.000\n",
      "Val acc: 0.822\n",
      "Epoch: 2/4 Iteration: 52000 Train loss: 0.019\n",
      "Epoch: 2/4 Iteration: 54000 Train loss: 0.000\n",
      "Epoch: 2/4 Iteration: 56000 Train loss: 0.000\n",
      "Epoch: 2/4 Iteration: 58000 Train loss: 0.000\n",
      "Epoch: 2/4 Iteration: 60000 Train loss: 0.000\n",
      "Val acc: 0.818\n",
      "Epoch: 3/4 Iteration: 62000 Train loss: 0.000\n",
      "Epoch: 3/4 Iteration: 64000 Train loss: 0.001\n",
      "Epoch: 3/4 Iteration: 66000 Train loss: 0.000\n",
      "Epoch: 3/4 Iteration: 68000 Train loss: 0.000\n",
      "Epoch: 3/4 Iteration: 70000 Train loss: 0.000\n",
      "Val acc: 0.814\n",
      "Epoch: 3/4 Iteration: 72000 Train loss: 0.026\n",
      "Epoch: 3/4 Iteration: 74000 Train loss: 0.000\n",
      "Epoch: 3/4 Iteration: 76000 Train loss: 0.000\n",
      "Epoch: 3/4 Iteration: 78000 Train loss: 0.000\n",
      "Epoch: 3/4 Iteration: 80000 Train loss: 0.000\n",
      "Val acc: 0.819\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii in range(len(train_x)):\n",
    "            \n",
    "            # train_x[ii] is a list of words of a review, \n",
    "            # here we convert it to a numpy array and reshape it to a 2D array with first dimention has size 1\n",
    "            # train_y[li] is a scalar value,\n",
    "            # here we convert it to a numpy array and reshape it to a 2D array with first dimention has size 1\n",
    "            x = np.asarray(train_x[ii]).reshape(1, -1)\n",
    "            y = np.asarray(train_y[ii]).reshape(1, -1)\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y,   # change ths shape of y to [batch_size, 1]\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%2000==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%10000==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for jj in range(len(val_y)):\n",
    "                    x = np.asarray(val_x[jj]).reshape(1, -1)\n",
    "                    y = np.asarray(val_y[jj]).reshape(1, -1)\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y,\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test accuracy: 0.814\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for zz in range(len(test_x)):\n",
    "        x = np.asarray(test_x[zz]).reshape(1, -1)\n",
    "        y = np.asarray(test_y[zz]).reshape(1, -1)\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
