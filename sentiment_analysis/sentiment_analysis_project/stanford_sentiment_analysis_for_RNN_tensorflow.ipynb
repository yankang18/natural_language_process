{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file=None):\n",
    "    if data_file == None:\n",
    "        return\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/large_files/stanford_sentiment/parsed_data/'\n",
    "word2idx = load_data(folder + \"sentiment_word2idx.json\")\n",
    "sentiment_binary_train = load_data(folder + \"sentiment_binary_train.json\")\n",
    "sentiment_train = load_data(folder + \"sentiment_train.json\")\n",
    "sentiment_binary_test = load_data(folder + \"sentiment_binary_test.json\")\n",
    "sentiment_test = load_data(folder + \"sentiment_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: # of training samples and # of test samples\n",
      "# of traing samples:  6920\n",
      "# of test samples:  1821\n"
     ]
    }
   ],
   "source": [
    "# the loaded samples has three type of labels -1,0,1, in which -1 indicates neutral sentiment.\n",
    "# We exclude samples with neutral sentiment.\n",
    "def exclude_neutral_sample(samples:dict):\n",
    "    ssamples = {}\n",
    "    for k, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            ssamples[k] = v\n",
    "    return ssamples\n",
    "        \n",
    "train_b = exclude_neutral_sample(sentiment_binary_train)\n",
    "test_b = exclude_neutral_sample(sentiment_binary_test)\n",
    "\n",
    "print(\"After filtering: # of training samples and # of test samples\")\n",
    "print(\"# of traing samples: \", len(train_b))\n",
    "print(\"# of test samples: \", len(test_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(wordidx, idx2word:dict):\n",
    "    wordlist = []\n",
    "    for idx in wordidx:\n",
    "        if idx != -1:\n",
    "            token = idx2word[idx]\n",
    "            if token not in string.punctuation:\n",
    "                wordlist.append(token)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_samples(samples:dict, idx2word:dict):\n",
    "    comments = []\n",
    "    targets = []\n",
    "    for _, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            comment = \" \".join(get_comment(v[0], idx2word))\n",
    "            label = v[3][-1]\n",
    "            comments.append(comment)\n",
    "            targets.append(label) \n",
    "    return comments, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size 18647\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "train_comments_o, train_targets = get_comments_samples(train_b, idx2word)\n",
    "test_comments_o, test_targets = get_comments_samples(test_b, idx2word)\n",
    "\n",
    "vocabulary_size = len(idx2word)\n",
    "print('vocabulary_size' , vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(train_comments_o))\n",
    "# print(len(train_comments_o))\n",
    "# print(len(train_targets))\n",
    "# print(len(test_comments_o))\n",
    "# print(len(test_targets))\n",
    "# print(train_comments_o[1])\n",
    "# print(train_targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and analyzing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(samples):\n",
    "    filtered_samples = []\n",
    "    for i in samples:\n",
    "        filtered_samples.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return filtered_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# remove_punctuation([\"Today's so beautiful!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    tokenized_samples = []\n",
    "    for s in samples:\n",
    "        tokens = word_tokenize(s)\n",
    "        tokenized_samples.append(tokens)\n",
    "    return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# tokenize(['Todays so beautiful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ourselves', 'after', 'few', 'was', 'had', 'off', 'under', \"hasn't\", 'why', 'mustn', 'before', 'yourself', 'own', 'because', 'now', 'between', 'above', 'hers', \"you'd\", 'o', 'if', \"won't\", 'they', 'y', 'over', 'his', 'when', 'her', 'once', 'while', 'them', 'itself', 'down', 'mightn', \"that'll\", 'hasn', 'me', 'did', 'both', 'have', \"haven't\", \"needn't\", 'same', 'then', 'we', 'into', 'which', 'no', 'he', 'who', \"couldn't\", 'shan', 'weren', 'ma', 're', \"wasn't\", 'of', 'there', 'an', \"you've\", 'should', \"weren't\", 'this', 'the', 'through', \"don't\", 'for', 'our', 'herself', 'on', 'can', 'doesn', 'hadn', \"hadn't\", 'is', 'or', 'with', 'all', 'more', 'needn', 'whom', 'and', 'yours', 'most', 'isn', 'you', 'been', 'myself', 'a', \"shouldn't\", 'their', 'don', \"aren't\", 'it', 'be', 'other', 'from', 'not', 'do', 'him', \"you'll\", 'where', 'those', 'ain', \"didn't\", 'about', 'how', 's', 'here', 'my', 'yourselves', 'its', 'does', 'than', 'haven', 'couldn', 'will', 'each', 'until', 'too', \"you're\", 'but', 'theirs', 'so', 'at', 'themselves', 'up', 'during', 't', \"wouldn't\", \"mightn't\", 'having', \"should've\", 'being', 'she', 'to', \"shan't\", 'out', 'again', 'shouldn', 'are', 'wouldn', \"mustn't\", 'below', 'll', 'any', 'in', 'didn', \"it's\", 'himself', 'some', \"isn't\", 'that', 'as', 'won', 'd', 'were', 'by', 'your', 'ours', \"doesn't\", 'i', 'against', 'very', 've', 'aren', 'wasn', 'further', 'nor', 'what', 'these', \"she's\", 'doing', 'such', 'only', 'has', 'm', 'am', 'just'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords_revised = set(('at', 'how', 'each', 's', 'those', 'from','whom', 'if', 're', 'we', 'by','into', 'it', 'ma', 'than', \"you'll\", \n",
    "             'very', 'was', 'is', 'be', 'had', 'you', 'hers', 'off', 'her', 'your', 'other', 'on', 'down', 'its', 'should', \n",
    "             'which', 'now', 'ours', 'in', \"you've\", 'before', 'further', 'below', 'did',  'who', 'once', 'some', 'being', \n",
    "             'does', 'too', 'herself', 'about', 'my', 'are', 'during', 'few', 'an', 'do', 'over',  'themselves', 'the', 'why', 'a', 'same', 'all', \n",
    "             'own', 'with', 'under', 'myself', 'he', 'because', 'again', 'himself', 'these', 'that', 'am', 'through', 'll', 'so', 've', \"you're\", 'doing', 'between', \n",
    "             'when', 'ourselves', 'been', 'of', 'our', 'them', 'their', 'while', 'as', 'can', 'where', 'such', 'yourself', 'haven', 'they', 'theirs', 'm', 'both', \n",
    "                     \"that'll\", 'or', 'were', 'up', 'will', 'me', 'yours', 'itself', 'has', 'more', \n",
    "                'd', 'o', 'what', 'having', 't', 'this', 'after', 'no', 'then', 'above', 'out', 'nor', \"should've\", 'his', \n",
    "               \"you'd\", \"she's\", 'and', 'shan', 'until', 'here', 'for', 'just', 'him', 'to', 'have', 'she', 'yourselves', \"it's\", 'y', 'i', 'there'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords_revised_2 = set(('at', 'each', 's', 'those', 'from', 'if', 're', 'we', 'by','into', 'it', 'ma', \"you'll\", \n",
    "             'very', 'was', 'is', 'be', 'had', 'you', 'hers', 'off', 'her', 'your', 'other', 'on', 'down', 'its', 'should', \n",
    "              'now', 'ours', 'in', \"you've\", 'below', 'did', 'being', \n",
    "             'does', 'herself', 'about', 'my', 'are', 'an', 'do', 'themselves', 'the',  'a',  \n",
    "             'own', 'myself', 'he',  'himself', 'these', 'that', 'am', 'll', 'so', 've', \"you're\", 'doing', \n",
    "            'ourselves', 'been', 'of', 'our', 'them', 'their', 'can', 'yourself', 'they', 'theirs', 'm', \n",
    "                     \"that'll\", 'or', 'were', 'up', 'will', 'me', 'yours', 'itself', 'has', \n",
    "                'd', 'o', 'having', 't', 'this', 'after', 'then', 'out', \"should've\", 'his', \n",
    "               \"you'd\", \"she's\", 'shan',  'here', 'for', 'him', 'to', 'have', 'she', 'yourselves', \"it's\", 'y', 'i', 'there'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(samples):\n",
    "    filtered_samples = []\n",
    "    for s in samples:\n",
    "        filstered_tokens = []\n",
    "        for w in s:\n",
    "            if w not in stopWords_revised_2:\n",
    "                filstered_tokens.append(w)\n",
    "        filtered_samples.append(filstered_tokens)\n",
    "    return filtered_samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# remove_stopwords([['Todays', 'so', 'not', 'beautiful']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start preprocessing data\n",
    "* Remove stopwords\n",
    "* Remove punctuation\n",
    "* Tokenization\n",
    "* Change the representation of training/test data to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orginal Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_o[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perserve n't**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments_1 = []\n",
    "for comment in train_comments_o:\n",
    "    new_str = comment.replace(\"n't\", 'not')\n",
    "    train_comments_1.append(new_str)\n",
    "    \n",
    "test_comments_1 = []\n",
    "for comment in test_comments_o:\n",
    "    new_str = comment.replace(\"n't\", 'not')\n",
    "    test_comments_1.append(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation removed\n"
     ]
    }
   ],
   "source": [
    "train_comments_punc = remove_punctuation(train_comments_1)\n",
    "test_comments_punc = remove_punctuation(test_comments_1)\n",
    "print(\"punctuation removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_punc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized\n"
     ]
    }
   ],
   "source": [
    "train_comments_tokenized = tokenize(train_comments_punc)\n",
    "test_comments_tokenized = tokenize(test_comments_punc)\n",
    "print(\"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_tokenized[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords removed\n"
     ]
    }
   ],
   "source": [
    "train_comments_stopwords = remove_stopwords(train_comments_tokenized)\n",
    "test_comments_stopwords = remove_stopwords(test_comments_tokenized)\n",
    "print(\"stopwords removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_stopwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(tokenized_comments):\n",
    "    text = []\n",
    "    for comment in tokenized_comments:\n",
    "        text += comment\n",
    "    return text\n",
    "\n",
    "all_words = combine(train_comments_stopwords) + combine(test_comments_stopwords)\n",
    "# print(all_words)\n",
    "# all_text1 = ' '.join(train_comments_stopwords)\n",
    "# all_text2 = ' '.join(test_comments_stopwords)\n",
    "# words1 = all_text1.split()\n",
    "# words2 = all_text2.split()\n",
    "# all_words = words1 + words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "# Create your dictionary that maps vocab words to integers here\n",
    "from collections import Counter\n",
    "counts = Counter(all_words)\n",
    "print(type(counts))\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Create words-to-index map. \n",
    "# Note that index start from 1\n",
    "vocab_to_int = {word:i for i, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of vocab <class 'list'>\n",
      "total # of words:  16547\n",
      "first word: and\n",
      "last word: fizzles\n",
      "first word index: 1\n",
      "last word index 16547\n"
     ]
    }
   ],
   "source": [
    "print(\"type of vocab\", type(vocab))\n",
    "print('total # of words: ', len(vocab_to_int))\n",
    "print(\"first word:\", vocab[0])\n",
    "print(\"last word:\", vocab[-1])\n",
    "print(\"first word index:\", vocab_to_int[vocab[0]])\n",
    "print(\"last word index\", vocab_to_int[vocab[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index-to-words map.\n",
    "index2word = {idx:word for word, idx in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(reviews, vocab_to_int):\n",
    "    # Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "    print('# of reviews before index: ', len(reviews))\n",
    "    reviews_ints = []\n",
    "    for review in reviews:\n",
    "        reviews_ints.append([vocab_to_int[word] for word in review])\n",
    "\n",
    "    print('# of reviews after index: ', len(reviews_ints))\n",
    "    return reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reviews before index:  6920\n",
      "# of reviews after index:  6920\n"
     ]
    }
   ],
   "source": [
    "# convert the representation of training examples from text to integer\n",
    "x_train = convert_to_int(train_comments_stopwords, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1119, 305, 5, 10031, 447, 4621, 15249, 13620, 5536, 90, 900, 2187, 10677, 5, 154, 3015, 6135, 305, 238, 397, 74, 692]\n",
      "aside from the fact that the film idiotically uses the website feardotcom.com or the improperly hammy performance from poor stephen rea the film gets added disdain for the fact that it is nearly impossible to look at or understand\n",
      "['aside', 'fact', 'film', 'idiotically', 'uses', 'website', 'feardotcomcom', 'improperly', 'hammy', 'performance', 'poor', 'stephen', 'rea', 'film', 'gets', 'added', 'disdain', 'fact', 'nearly', 'impossible', 'look', 'understand']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "print(x_train[7])\n",
    "print(train_comments_o[7])\n",
    "text = [index2word[idx] for idx in x_train[7]]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reviews before index:  1821\n",
      "# of reviews after index:  1821\n"
     ]
    }
   ],
   "source": [
    "# convert the representation of test examples from text to integer\n",
    "x_test = convert_to_int(test_comments_stopwords, vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distribution over training/test example length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 2\n",
      "Maximum train example length: 36\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create length to frequency map\n",
    "x_train_lens_map = Counter([len(x) for x in x_train])\n",
    "x_train_lens = [len(x) for x in x_train]\n",
    "print(\"Zero-length reviews: {}\".format(x_train_lens_map[0]))\n",
    "print(\"Maximum train example length: {}\".format(max(x_train_lens_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum test example length: 31\n",
      "11.324566473988439\n",
      "11.314662273476111\n"
     ]
    }
   ],
   "source": [
    "# Create length to frequency map\n",
    "x_test_lens_map = Counter([len(x) for x in x_test])\n",
    "x_test_lens = [len(x) for x in x_test]\n",
    "print(\"Zero-length reviews: {}\".format(x_test_lens_map[0]))\n",
    "print(\"Maximum test example length: {}\".format(max(x_test_lens_map)))\n",
    "\n",
    "\n",
    "ave_len = 0\n",
    "for i in x_train:\n",
    "    ave_len += len(i) \n",
    "ave_len = ave_len / len(x_train)\n",
    "print(ave_len)\n",
    "\n",
    "ave_len = 0\n",
    "for i in x_test:\n",
    "#     print(i)\n",
    "    ave_len += len(i)  \n",
    "ave_len = ave_len / len(x_test)\n",
    "print(ave_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d8a3ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nToNeKaX8nAa9Ukr5uf8PX//QcGM9dL8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f2bfe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(x_train_lens_map.keys()), list(x_train_lens_map.values()))\n",
    "# plt.axis([1000, 1500, 0, 55])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(x_test_lens_map.keys()), list(x_test_lens_map.values()))\n",
    "# plt.axis([1000, 1500, 0, 55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_padding(reviews_ints, seq_len):\n",
    "    \n",
    "    # The features created here are the data that we are going to train and test the network\n",
    "\n",
    "    # Create features with shape (len(reviews_ints), seq_len) and initialized with zeros\n",
    "    features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "\n",
    "    print(features.shape)\n",
    "    # Create list holding the length for each review\n",
    "    lengths = []\n",
    "\n",
    "    # row is the review in forms of a list of integers\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "\n",
    "        # left padding\n",
    "        features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "        \n",
    "        # record the length of each review. This might be useful when we want to use sequence_length argument\n",
    "        # of tf.nn.dynamic_rnn(...)\n",
    "        lengths.append(len(row) if len(row) < seq_len else seq_len)\n",
    "        \n",
    "    return features, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_padding(reviews_ints, seq_len):\n",
    "    \n",
    "    # The features created here are the data that we are going to train and test the network\n",
    "\n",
    "    # Create features with shape (len(reviews_ints), seq_len) and initialized with zeros\n",
    "    features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "\n",
    "    print(features.shape)\n",
    "    # Create list holding the length for each review\n",
    "    lengths = []\n",
    "\n",
    "    # row is the review in forms of a list of integers\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "\n",
    "        # right padding\n",
    "        features[i, :len(row)] = np.array(row)[:seq_len]\n",
    "\n",
    "        # record the length of each review. This might be useful when we want to use sequence_length argument\n",
    "        # of tf.nn.dynamic_rnn(...)\n",
    "        lengths.append(len(row) if len(row) < seq_len else seq_len)\n",
    "        \n",
    "    return features, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "\n",
    "# sample1 = [4,6,7,2,3,5,7,8,1]\n",
    "# sample2 = [1,2,3,4,6,7,2,3,5,7]\n",
    "# sample3 = [1,2,3,4,6,7,2,3,26,1, 11, 12]\n",
    "# sample4 = [8,7,3]\n",
    "# samples = []\n",
    "# samples.append(sample1)\n",
    "# samples.append(sample2)\n",
    "# samples.append(sample3)\n",
    "# samples.append(sample4)\n",
    "# features_, lengths_ = left_padding(samples, 10)\n",
    "\n",
    "# print(features_)\n",
    "# print(lengths_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude training/test examples with 0 length. It is possible that some examples may have 0 length since we did \n",
    "# punctuation and stopword removing. \n",
    "\n",
    "x_train_f = []\n",
    "y_train_f = []\n",
    "for idx in range(len(x_train)):\n",
    "    if len(x_train[idx]) != 0:\n",
    "        x_train_f.append(x_train[idx])\n",
    "        y_train_f.append(train_targets[idx])\n",
    "\n",
    "x_test_f = []\n",
    "y_test_f = []\n",
    "for idx in range(len(x_test)):\n",
    "    if len(x_test[idx]) != 0:\n",
    "        x_test_f.append(x_test[idx])\n",
    "        y_test_f.append(test_targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6918, 36)\n",
      "(1821, 36)\n"
     ]
    }
   ],
   "source": [
    "# perform padding\n",
    "seq_len = 36\n",
    "x_train_p, x_train_len = left_padding(x_train_f, seq_len)\n",
    "x_test_p, x_test_len = left_padding(x_test_f, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# print(x_train_p[:5,:seq_len])\n",
    "# print(x_train_len[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Resplit training data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004\n",
      "735\n",
      "8004\n",
      "735\n",
      "8004\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "# Add more data for training, since RNN consumes lots of data\n",
    "\n",
    "x_train_ = np.append(x_train_p, x_test_p[:1086], axis=0)\n",
    "x_test_ = x_test_p[1086:]\n",
    "\n",
    "y_train_ = np.array(np.append(y_train_f, y_test_f[:1086], axis=0))\n",
    "y_test_ = np.array(y_test_f[1086:])\n",
    "\n",
    "x_train_len_ = np.append(x_train_len, x_test_len[:1086], axis=0)\n",
    "x_test_len_ = x_test_len[1086:]\n",
    "\n",
    "print(len(x_train_))\n",
    "print(len(x_test_))\n",
    "print(len(y_train_))\n",
    "print(len(y_test_))\n",
    "print(len(x_train_len_))\n",
    "print(len(x_test_len_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_.shape)\n",
    "# print(x_test_.shape)\n",
    "# print(y_train_.shape)\n",
    "# print(y_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, lengths, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y)\n",
    "    n_batches = len(x)//batch_size\n",
    "\n",
    "    # Only get full batches\n",
    "    features, labels = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    \n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "#         print(\"start_i\", start_i)\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i], lengths[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM_RNN_Model(vocab_size, learning_rate = 0.001, embedding_dim = 300, \n",
    "                   lstm_size = 256, num_lstm_layer = 1, batch_size = 200, keep_probability=0.8, epochs=400):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # input placeholders\n",
    "        inputs = tf.placeholder(tf.int32, shape=[None, None], name='inputs')\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, None], name='labels')\n",
    "    #     seq_len = tf.placeholder(tf.int32, shape=[None], name='seq_len')\n",
    "        keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "        # embedding layer\n",
    "        embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_dim), -1, 1), name = 'embedding')\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "\n",
    "        # LSTM layer\n",
    "        with tf.name_scope(\"biLSTM\"):\n",
    "            with tf.variable_scope(\"forward\"):\n",
    "                fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=lstm_size)\n",
    "#                 fw_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "                fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob = keep_prob)\n",
    "            with tf.variable_scope(\"backward\"):\n",
    "                bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=lstm_size)\n",
    "#                 bw_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "                bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob = keep_prob)\n",
    "\n",
    "        initial_state_fw = fw_cell.zero_state(batch_size, tf.float32)\n",
    "        initial_state_bw = bw_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, cell_bw=bw_cell,\n",
    "                                                    inputs=embed,\n",
    "    #                                                 sequence_length=seq_len,\n",
    "                                                    initial_state_fw=initial_state_fw,\n",
    "                                                    initial_state_bw=initial_state_bw,\n",
    "                                                    scope=\"biLSTM\")\n",
    "\n",
    "\n",
    "        outputs = tf.concat(axis = 2, values = outputs)\n",
    "\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    #     cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=predictions)\n",
    "        entroy = tf.reduce_mean(cost)\n",
    "        \n",
    "\n",
    "#         # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "#         tvars = tf.trainable_variables()\n",
    "#         # grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "#         grads = tf.gradients(entroy, tvars)\n",
    "#         clip_grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "#         train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "#         optimizer = train_op.apply_gradients(zip(clip_grads, tvars))\n",
    "    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(entroy)\n",
    "\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), tf.cast(tf.round(labels), tf.int32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 1\n",
    "        \n",
    "        for e in range(epochs):\n",
    "\n",
    "    #         state = sess.run(initial_state)\n",
    "            state_fw = sess.run(initial_state_fw)\n",
    "            state_bw = sess.run(initial_state_bw)\n",
    "\n",
    "            for batch_features, batch_labels, batch_len in get_batches(x_train_, y_train_, x_train_len_, batch_size):\n",
    "    #             batch_len = np.array(batch_len)\n",
    "\n",
    "                feed = {inputs: batch_features,\n",
    "                        labels: batch_labels[:, None],   # change ths shape of y to [batch_size, 1]\n",
    "    #                     seq_len: batch_len,\n",
    "                        keep_prob: keep_probability,\n",
    "                        initial_state_fw: state_fw,\n",
    "                        initial_state_bw: state_bw}\n",
    "                loss, state, _ = sess.run([entroy, final_state, optimizer], feed_dict=feed)\n",
    "                state_fw, state_bw = state\n",
    "\n",
    "                if iteration%5==0:\n",
    "    #                   print(loss)\n",
    "                        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                              \"Iteration: {}\".format(iteration),\n",
    "                              \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "                if iteration%25==0:\n",
    "                    val_acc = []\n",
    "                    val_state_fw = sess.run(fw_cell.zero_state(batch_size, tf.float32))\n",
    "                    val_state_bw = sess.run(bw_cell.zero_state(batch_size, tf.float32))\n",
    "\n",
    "                    for x, y, batch_len in get_batches(x_test_, y_test_, x_test_len_, batch_size):\n",
    "                        feed = {inputs: x,\n",
    "                                labels: y[:, None],\n",
    "    #                             seq_len: batch_len,\n",
    "                                keep_prob: 1,  # note the keep probablity is 1 here\n",
    "                                initial_state_fw: val_state_fw,\n",
    "                                initial_state_fw: val_state_bw}\n",
    "                        batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                        val_acc.append(batch_acc)\n",
    "                    print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "                iteration +=1\n",
    "\n",
    "        saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.001\n",
      "keep_probability 0.7\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.694\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.694\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.692\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.685\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.676\n",
      "Val acc: 0.642\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.691\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.677\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.655\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.638\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.654\n",
      "Val acc: 0.707\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.638\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.615\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.603\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.611\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.603\n",
      "Val acc: 0.738\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.574\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.575\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.583\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.577\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.546\n",
      "Val acc: 0.755\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.548\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.557\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.564\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.541\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.538\n",
      "Val acc: 0.730\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.554\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.560\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.545\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.532\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.547\n",
      "Val acc: 0.723\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.558\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.531\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.526\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.541\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.546\n",
      "Val acc: 0.755\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.526\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.522\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.532\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.541\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.517\n",
      "Val acc: 0.760\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.513\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.530\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.534\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.519\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.508\n",
      "Val acc: 0.757\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.527\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.533\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.514\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.507\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.529\n",
      "Val acc: 0.775\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.531\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.510\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.511\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.523\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.530\n",
      "Val acc: 0.770\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.508\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.506\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.525\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.530\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.509\n",
      "Val acc: 0.755\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.505\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.522\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.529\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.508\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.505\n",
      "Val acc: 0.750\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.519\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.529\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.507\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.504\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.518\n",
      "Val acc: 0.757\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.528\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.506\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.504\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.518\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.528\n",
      "Val acc: 0.743\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.506\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.503\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.518\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.527\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.506\n",
      "Val acc: 0.735\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.501\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.517\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.527\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.506\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.501\n",
      "Val acc: 0.735\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.517\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.527\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.505\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.501\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.517\n",
      "Val acc: 0.735\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.527\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.505\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.501\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.517\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.527\n",
      "Val acc: 0.733\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.505\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.501\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.517\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.527\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.505\n",
      "Val acc: 0.743\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.501\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.517\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.526\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.505\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.501\n",
      "Val acc: 0.735\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.517\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.526\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.505\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.501\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.517\n",
      "Val acc: 0.743\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.526\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.505\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.501\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.517\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.525\n",
      "Val acc: 0.740\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.505\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.501\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.517\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.525\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.505\n",
      "Val acc: 0.738\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.501\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.517\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.525\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.505\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.501\n",
      "Val acc: 0.735\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.517\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.525\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.505\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.501\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.517\n",
      "Val acc: 0.740\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.525\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.505\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.501\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.517\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.525\n",
      "Val acc: 0.740\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.505\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.501\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.517\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.525\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.505\n",
      "Val acc: 0.745\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.501\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.517\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.525\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.505\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.501\n",
      "Val acc: 0.733\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.517\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.525\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.505\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.501\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.517\n",
      "Val acc: 0.748\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.525\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.505\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.501\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.517\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.525\n",
      "Val acc: 0.752\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.506\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.501\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.517\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.525\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.505\n",
      "Val acc: 0.738\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.001\n",
      "keep_probability 0.8\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.696\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/40 Iteration: 15 Train loss: 0.691\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.685\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.671\n",
      "Val acc: 0.630\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.671\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.661\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.632\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.645\n",
      "Val acc: 0.695\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.632\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.619\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.589\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.606\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.591\n",
      "Val acc: 0.733\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.578\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.556\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.579\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.570\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.556\n",
      "Val acc: 0.757\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.534\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.562\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.564\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.548\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.527\n",
      "Val acc: 0.740\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.554\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.548\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.538\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.522\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.545\n",
      "Val acc: 0.750\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.551\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.534\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.520\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.539\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.548\n",
      "Val acc: 0.738\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.539\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.512\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.538\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.541\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.528\n",
      "Val acc: 0.752\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.514\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.528\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.544\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.523\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.508\n",
      "Val acc: 0.743\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.529\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.533\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.518\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.504\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.528\n",
      "Val acc: 0.755\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.529\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.519\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.503\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.527\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.528\n",
      "Val acc: 0.760\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.515\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.508\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.525\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.527\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.514\n",
      "Val acc: 0.760\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.502\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.524\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.527\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.511\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.501\n",
      "Val acc: 0.748\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.524\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.526\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.510\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.500\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.523\n",
      "Val acc: 0.748\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.526\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.510\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.500\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.523\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.525\n",
      "Val acc: 0.745\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.510\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.500\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.523\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.525\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.510\n",
      "Val acc: 0.745\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.499\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.523\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.524\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.510\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.499\n",
      "Val acc: 0.740\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.522\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.524\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.510\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.499\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.522\n",
      "Val acc: 0.755\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.524\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.510\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.499\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.522\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.524\n",
      "Val acc: 0.757\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.510\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.499\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.521\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.523\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.510\n",
      "Val acc: 0.748\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.499\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.521\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.523\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.510\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.499\n",
      "Val acc: 0.760\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.520\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.523\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.510\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.498\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.520\n",
      "Val acc: 0.762\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.523\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.510\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.498\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.520\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.522\n",
      "Val acc: 0.757\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.510\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.498\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.520\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.522\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.510\n",
      "Val acc: 0.762\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.498\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.520\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.522\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.510\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.498\n",
      "Val acc: 0.760\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.520\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.522\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.509\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.498\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.520\n",
      "Val acc: 0.765\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.522\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.508\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.498\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.520\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.523\n",
      "Val acc: 0.743\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.507\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.498\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.520\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.522\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.507\n",
      "Val acc: 0.743\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.498\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.520\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.522\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.507\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.498\n",
      "Val acc: 0.748\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.520\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.522\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.507\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.498\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.520\n",
      "Val acc: 0.750\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.522\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.506\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.498\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.520\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.522\n",
      "Val acc: 0.762\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.505\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.498\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.522\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.523\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.506\n",
      "Val acc: 0.760\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.005\n",
      "keep_probability 0.7\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.692\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.689\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.680\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.652\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.616\n",
      "Val acc: 0.723\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.605\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.586\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/40 Iteration: 45 Train loss: 0.557\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.557\n",
      "Val acc: 0.733\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.543\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.542\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.528\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.534\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.532\n",
      "Val acc: 0.738\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.527\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.516\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.526\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.526\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.530\n",
      "Val acc: 0.740\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.510\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.519\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.525\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.515\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.506\n",
      "Val acc: 0.765\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.518\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.522\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.512\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.505\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.520\n",
      "Val acc: 0.750\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.519\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.511\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.501\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.518\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.520\n",
      "Val acc: 0.757\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.508\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.500\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.518\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.517\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.507\n",
      "Val acc: 0.752\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.499\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.518\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.516\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.505\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.499\n",
      "Val acc: 0.762\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.517\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.516\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.504\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.497\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.516\n",
      "Val acc: 0.767\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.516\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.504\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.497\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.516\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.516\n",
      "Val acc: 0.767\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.504\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.497\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.516\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.516\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.504\n",
      "Val acc: 0.765\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.496\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.516\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.516\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.504\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.496\n",
      "Val acc: 0.762\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.516\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.516\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.504\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.496\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.516\n",
      "Val acc: 0.767\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.516\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.504\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.496\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.516\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.516\n",
      "Val acc: 0.775\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.504\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.496\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.516\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.516\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.504\n",
      "Val acc: 0.760\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.496\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.516\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.516\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.504\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.496\n",
      "Val acc: 0.765\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.516\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.516\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.505\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.496\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.516\n",
      "Val acc: 0.755\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.516\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.504\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.496\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.516\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.516\n",
      "Val acc: 0.743\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.504\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.496\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.516\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.516\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.504\n",
      "Val acc: 0.743\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.496\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.516\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.516\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.504\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.496\n",
      "Val acc: 0.743\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.516\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.516\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.504\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.496\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.516\n",
      "Val acc: 0.748\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.516\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.504\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.496\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.516\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.516\n",
      "Val acc: 0.743\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.504\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.496\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.516\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.516\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.504\n",
      "Val acc: 0.740\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.496\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.515\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.516\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.504\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.496\n",
      "Val acc: 0.752\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.515\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.516\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.504\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.496\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.515\n",
      "Val acc: 0.755\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.516\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.504\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.497\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.515\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.516\n",
      "Val acc: 0.762\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.504\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.496\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.515\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.517\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.501\n",
      "Val acc: 0.765\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.496\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.515\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.516\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.501\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.496\n",
      "Val acc: 0.743\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.515\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.518\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.502\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.496\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.515\n",
      "Val acc: 0.735\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.519\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.503\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.497\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.515\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.519\n",
      "Val acc: 0.767\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.503\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.495\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.516\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.521\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.504\n",
      "Val acc: 0.770\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.005\n",
      "keep_probability 0.8\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.691\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.674\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.647\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.624\n",
      "Val acc: 0.745\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.584\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.591\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.567\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.561\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.540\n",
      "Val acc: 0.777\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.553\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.534\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.534\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/40 Iteration: 75 Train loss: 0.532\n",
      "Val acc: 0.798\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.524\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.516\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.521\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.533\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.519\n",
      "Val acc: 0.808\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.508\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.518\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.522\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.517\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.510\n",
      "Val acc: 0.777\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.515\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.522\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.516\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.509\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.520\n",
      "Val acc: 0.793\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.519\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.519\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.509\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.519\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.522\n",
      "Val acc: 0.785\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.512\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.504\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.516\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.519\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.512\n",
      "Val acc: 0.783\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.501\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.515\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.518\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.511\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.500\n",
      "Val acc: 0.790\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.515\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.516\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.510\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.500\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.515\n",
      "Val acc: 0.752\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.521\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.509\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.499\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.515\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.516\n",
      "Val acc: 0.765\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.511\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.499\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.514\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.516\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.509\n",
      "Val acc: 0.770\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.501\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.513\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.518\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.509\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.499\n",
      "Val acc: 0.767\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.513\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.516\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.509\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.499\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.512\n",
      "Val acc: 0.785\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.515\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.509\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.499\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.512\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.515\n",
      "Val acc: 0.780\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.509\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.499\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.512\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.515\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.509\n",
      "Val acc: 0.777\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.499\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.512\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.515\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.509\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.499\n",
      "Val acc: 0.788\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.512\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.515\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.509\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.499\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.512\n",
      "Val acc: 0.783\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.515\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.509\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.499\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.512\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.515\n",
      "Val acc: 0.783\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.509\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.499\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.512\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.515\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.509\n",
      "Val acc: 0.790\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.499\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.512\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.514\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.509\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.499\n",
      "Val acc: 0.785\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.512\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.514\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.509\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.499\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.513\n",
      "Val acc: 0.783\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.514\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.509\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.499\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.513\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.513\n",
      "Val acc: 0.790\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.509\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.499\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.513\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.513\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.509\n",
      "Val acc: 0.780\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.499\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.513\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.513\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.509\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.499\n",
      "Val acc: 0.780\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.512\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.513\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.509\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.499\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.512\n",
      "Val acc: 0.777\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.513\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.509\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.499\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.512\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.513\n",
      "Val acc: 0.777\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.509\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.499\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.512\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.513\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.509\n",
      "Val acc: 0.775\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.499\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.512\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.513\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.509\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.499\n",
      "Val acc: 0.777\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.511\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.513\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.509\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.499\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.511\n",
      "Val acc: 0.780\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.513\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.509\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.499\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.511\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.513\n",
      "Val acc: 0.775\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.509\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.499\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.511\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.513\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.509\n",
      "Val acc: 0.780\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.001\n",
      "keep_probability 0.7\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.691\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.692\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.691\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.682\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.670\n",
      "Val acc: 0.640\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.674\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.658\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.641\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.627\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.629\n",
      "Val acc: 0.702\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.623\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.606\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.598\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.595\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.592\n",
      "Val acc: 0.707\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.581\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.566\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.571\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.566\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.728\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.555\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.590\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.567\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.554\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.537\n",
      "Val acc: 0.735\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.553\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.547\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.548\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.535\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.545\n",
      "Val acc: 0.733\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.547\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.539\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.535\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.536\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.542\n",
      "Val acc: 0.738\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.540\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.524\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.531\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.533\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.536\n",
      "Val acc: 0.757\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.516\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.528\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.529\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.531\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.514\n",
      "Val acc: 0.760\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.526\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.528\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.526\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.509\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.526\n",
      "Val acc: 0.775\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.527\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.524\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.507\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.525\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.527\n",
      "Val acc: 0.772\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.523\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.507\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.523\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.527\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.522\n",
      "Val acc: 0.772\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.504\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.523\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.526\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.522\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.503\n",
      "Val acc: 0.760\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.522\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.525\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.523\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.503\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.521\n",
      "Val acc: 0.757\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.527\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.522\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.504\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.520\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.525\n",
      "Val acc: 0.762\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.521\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.502\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.520\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.524\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.521\n",
      "Val acc: 0.772\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.502\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.520\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.524\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.520\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.502\n",
      "Val acc: 0.772\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.519\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.524\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.520\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.501\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.519\n",
      "Val acc: 0.770\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.524\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.520\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.501\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.519\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.524\n",
      "Val acc: 0.765\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.520\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.501\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.519\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.524\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.519\n",
      "Val acc: 0.765\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.501\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.518\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.524\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.519\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.501\n",
      "Val acc: 0.762\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.518\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.523\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.519\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.501\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.518\n",
      "Val acc: 0.762\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.523\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.519\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.501\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.518\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.523\n",
      "Val acc: 0.765\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.519\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.501\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.518\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.523\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.519\n",
      "Val acc: 0.762\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.501\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.518\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.523\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.519\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.501\n",
      "Val acc: 0.757\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.518\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.523\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.519\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.501\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.518\n",
      "Val acc: 0.760\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.523\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.519\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.501\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.518\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.523\n",
      "Val acc: 0.765\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.519\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.501\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.518\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.523\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.519\n",
      "Val acc: 0.762\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.501\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.518\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.523\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.519\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.501\n",
      "Val acc: 0.762\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.518\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.523\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.519\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.501\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.518\n",
      "Val acc: 0.762\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.523\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.519\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.501\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.518\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.523\n",
      "Val acc: 0.772\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.518\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.501\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.518\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.524\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.518\n",
      "Val acc: 0.767\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.001\n",
      "keep_probability 0.8\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.692\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.688\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.675\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.656\n",
      "Val acc: 0.670\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.663\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.640\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.628\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.619\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.627\n",
      "Val acc: 0.712\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.602\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.588\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.591\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.596\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.574\n",
      "Val acc: 0.720\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.589\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.577\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.579\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.567\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.552\n",
      "Val acc: 0.765\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.554\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.572\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.556\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.541\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.755\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.551\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.558\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.540\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.532\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.545\n",
      "Val acc: 0.745\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.552\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.539\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.531\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.543\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.543\n",
      "Val acc: 0.735\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.539\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.518\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.544\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.538\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.522\n",
      "Val acc: 0.728\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.517\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.531\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.534\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.519\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.512\n",
      "Val acc: 0.733\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.528\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.535\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.517\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.510\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.526\n",
      "Val acc: 0.740\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.532\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.517\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.517\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.526\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.533\n",
      "Val acc: 0.750\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.517\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.514\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.524\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.533\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.516\n",
      "Val acc: 0.762\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.514\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.523\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.531\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.516\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.509\n",
      "Val acc: 0.770\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.529\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.528\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.516\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.508\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.523\n",
      "Val acc: 0.777\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.529\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.512\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.506\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.522\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.528\n",
      "Val acc: 0.775\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.511\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.506\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.521\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.527\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.511\n",
      "Val acc: 0.777\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.506\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.521\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.527\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.511\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.506\n",
      "Val acc: 0.780\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.521\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.527\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.511\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.506\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.521\n",
      "Val acc: 0.780\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.527\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.511\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.506\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.521\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.527\n",
      "Val acc: 0.772\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.511\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.505\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.521\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.527\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.511\n",
      "Val acc: 0.777\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.505\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.521\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.527\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.512\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.506\n",
      "Val acc: 0.767\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.521\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.527\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.511\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.506\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.521\n",
      "Val acc: 0.767\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.527\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.511\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.506\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.521\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.527\n",
      "Val acc: 0.770\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.511\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.506\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.521\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.527\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.511\n",
      "Val acc: 0.780\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.505\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.521\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.527\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.511\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.505\n",
      "Val acc: 0.780\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.521\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.527\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.511\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.505\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.521\n",
      "Val acc: 0.777\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.527\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.511\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.505\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.521\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.527\n",
      "Val acc: 0.780\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.511\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.505\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.521\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.527\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.510\n",
      "Val acc: 0.783\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.505\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.521\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.527\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.510\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.505\n",
      "Val acc: 0.783\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.521\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.527\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.510\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.505\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.521\n",
      "Val acc: 0.780\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.527\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.510\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.505\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.521\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.527\n",
      "Val acc: 0.777\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.510\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.505\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.521\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.527\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.510\n",
      "Val acc: 0.780\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.005\n",
      "keep_probability 0.7\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.693\n",
      "Val acc: 0.572\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.693\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.693\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.692\n",
      "Val acc: 0.572\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.693\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.678\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.659\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.649\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.629\n",
      "Val acc: 0.690\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.605\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.563\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.557\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.573\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.560\n",
      "Val acc: 0.760\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.529\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.544\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.541\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.525\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.508\n",
      "Val acc: 0.777\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.529\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.528\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.513\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.504\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.770\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.523\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.509\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.502\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.522\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.521\n",
      "Val acc: 0.775\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.507\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.499\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.519\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.520\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.506\n",
      "Val acc: 0.793\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.498\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.519\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.518\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.506\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.499\n",
      "Val acc: 0.788\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.518\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.516\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.503\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.498\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.518\n",
      "Val acc: 0.780\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.516\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.503\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.498\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.518\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.516\n",
      "Val acc: 0.767\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.502\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.498\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.518\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.515\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.502\n",
      "Val acc: 0.772\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.498\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.518\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.515\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.502\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.498\n",
      "Val acc: 0.772\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.518\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.515\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.502\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.498\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.518\n",
      "Val acc: 0.780\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.515\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.502\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.498\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.518\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.515\n",
      "Val acc: 0.777\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.502\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.498\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.518\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.515\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.502\n",
      "Val acc: 0.777\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.498\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.518\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.515\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.502\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.498\n",
      "Val acc: 0.777\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.518\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.515\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.502\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.498\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.518\n",
      "Val acc: 0.790\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.515\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.501\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.498\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.517\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.515\n",
      "Val acc: 0.772\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.501\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.498\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.517\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.515\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.501\n",
      "Val acc: 0.780\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.498\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.517\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.515\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.501\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.498\n",
      "Val acc: 0.780\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.517\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.515\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.501\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.498\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.517\n",
      "Val acc: 0.775\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.515\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.501\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.498\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.517\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.515\n",
      "Val acc: 0.775\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.501\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.498\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.517\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.515\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.501\n",
      "Val acc: 0.770\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.498\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.517\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.515\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.501\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.498\n",
      "Val acc: 0.767\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.517\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.515\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.501\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.498\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.517\n",
      "Val acc: 0.775\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.515\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.501\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.498\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.517\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.514\n",
      "Val acc: 0.775\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.500\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.498\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.518\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.514\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.502\n",
      "Val acc: 0.772\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.498\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.518\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.514\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.500\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.498\n",
      "Val acc: 0.772\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.519\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.514\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.503\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.497\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.520\n",
      "Val acc: 0.775\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.515\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.501\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.497\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.518\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.514\n",
      "Val acc: 0.772\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.501\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.497\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.519\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.514\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.500\n",
      "Val acc: 0.772\n",
      "---------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.005\n",
      "keep_probability 0.8\n",
      "Epoch: 0/40 Iteration: 5 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 10 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 15 Train loss: 0.693\n",
      "Epoch: 0/40 Iteration: 20 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 25 Train loss: 0.693\n",
      "Val acc: 0.553\n",
      "Epoch: 1/40 Iteration: 30 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 35 Train loss: 0.693\n",
      "Epoch: 1/40 Iteration: 40 Train loss: 0.693\n",
      "Epoch: 2/40 Iteration: 45 Train loss: 0.693\n",
      "Epoch: 2/40 Iteration: 50 Train loss: 0.693\n",
      "Val acc: 0.553\n",
      "Epoch: 2/40 Iteration: 55 Train loss: 0.693\n",
      "Epoch: 2/40 Iteration: 60 Train loss: 0.693\n",
      "Epoch: 3/40 Iteration: 65 Train loss: 0.693\n",
      "Epoch: 3/40 Iteration: 70 Train loss: 0.693\n",
      "Epoch: 3/40 Iteration: 75 Train loss: 0.693\n",
      "Val acc: 0.472\n",
      "Epoch: 3/40 Iteration: 80 Train loss: 0.693\n",
      "Epoch: 4/40 Iteration: 85 Train loss: 0.694\n",
      "Epoch: 4/40 Iteration: 90 Train loss: 0.689\n",
      "Epoch: 4/40 Iteration: 95 Train loss: 0.691\n",
      "Epoch: 4/40 Iteration: 100 Train loss: 0.691\n",
      "Val acc: 0.565\n",
      "Epoch: 5/40 Iteration: 105 Train loss: 0.664\n",
      "Epoch: 5/40 Iteration: 110 Train loss: 0.633\n",
      "Epoch: 5/40 Iteration: 115 Train loss: 0.624\n",
      "Epoch: 5/40 Iteration: 120 Train loss: 0.646\n",
      "Epoch: 6/40 Iteration: 125 Train loss: 0.605\n",
      "Val acc: 0.600\n",
      "Epoch: 6/40 Iteration: 130 Train loss: 0.598\n",
      "Epoch: 6/40 Iteration: 135 Train loss: 0.611\n",
      "Epoch: 6/40 Iteration: 140 Train loss: 0.619\n",
      "Epoch: 7/40 Iteration: 145 Train loss: 0.584\n",
      "Epoch: 7/40 Iteration: 150 Train loss: 0.599\n",
      "Val acc: 0.620\n",
      "Epoch: 7/40 Iteration: 155 Train loss: 0.601\n",
      "Epoch: 7/40 Iteration: 160 Train loss: 0.597\n",
      "Epoch: 8/40 Iteration: 165 Train loss: 0.565\n",
      "Epoch: 8/40 Iteration: 170 Train loss: 0.578\n",
      "Epoch: 8/40 Iteration: 175 Train loss: 0.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.720\n",
      "Epoch: 8/40 Iteration: 180 Train loss: 0.568\n",
      "Epoch: 9/40 Iteration: 185 Train loss: 0.538\n",
      "Epoch: 9/40 Iteration: 190 Train loss: 0.554\n",
      "Epoch: 9/40 Iteration: 195 Train loss: 0.547\n",
      "Epoch: 9/40 Iteration: 200 Train loss: 0.550\n",
      "Val acc: 0.725\n",
      "Epoch: 10/40 Iteration: 205 Train loss: 0.534\n",
      "Epoch: 10/40 Iteration: 210 Train loss: 0.533\n",
      "Epoch: 10/40 Iteration: 215 Train loss: 0.541\n",
      "Epoch: 10/40 Iteration: 220 Train loss: 0.532\n",
      "Epoch: 11/40 Iteration: 225 Train loss: 0.517\n",
      "Val acc: 0.730\n",
      "Epoch: 11/40 Iteration: 230 Train loss: 0.529\n",
      "Epoch: 11/40 Iteration: 235 Train loss: 0.538\n",
      "Epoch: 11/40 Iteration: 240 Train loss: 0.528\n",
      "Epoch: 12/40 Iteration: 245 Train loss: 0.508\n",
      "Epoch: 12/40 Iteration: 250 Train loss: 0.523\n",
      "Val acc: 0.770\n",
      "Epoch: 12/40 Iteration: 255 Train loss: 0.529\n",
      "Epoch: 12/40 Iteration: 260 Train loss: 0.521\n",
      "Epoch: 13/40 Iteration: 265 Train loss: 0.509\n",
      "Epoch: 13/40 Iteration: 270 Train loss: 0.523\n",
      "Epoch: 13/40 Iteration: 275 Train loss: 0.527\n",
      "Val acc: 0.772\n",
      "Epoch: 13/40 Iteration: 280 Train loss: 0.519\n",
      "Epoch: 14/40 Iteration: 285 Train loss: 0.506\n",
      "Epoch: 14/40 Iteration: 290 Train loss: 0.518\n",
      "Epoch: 14/40 Iteration: 295 Train loss: 0.524\n",
      "Epoch: 14/40 Iteration: 300 Train loss: 0.513\n",
      "Val acc: 0.750\n",
      "Epoch: 15/40 Iteration: 305 Train loss: 0.506\n",
      "Epoch: 15/40 Iteration: 310 Train loss: 0.516\n",
      "Epoch: 15/40 Iteration: 315 Train loss: 0.523\n",
      "Epoch: 15/40 Iteration: 320 Train loss: 0.509\n",
      "Epoch: 16/40 Iteration: 325 Train loss: 0.504\n",
      "Val acc: 0.760\n",
      "Epoch: 16/40 Iteration: 330 Train loss: 0.517\n",
      "Epoch: 16/40 Iteration: 335 Train loss: 0.521\n",
      "Epoch: 16/40 Iteration: 340 Train loss: 0.510\n",
      "Epoch: 17/40 Iteration: 345 Train loss: 0.501\n",
      "Epoch: 17/40 Iteration: 350 Train loss: 0.517\n",
      "Val acc: 0.772\n",
      "Epoch: 17/40 Iteration: 355 Train loss: 0.521\n",
      "Epoch: 17/40 Iteration: 360 Train loss: 0.509\n",
      "Epoch: 18/40 Iteration: 365 Train loss: 0.505\n",
      "Epoch: 18/40 Iteration: 370 Train loss: 0.515\n",
      "Epoch: 18/40 Iteration: 375 Train loss: 0.520\n",
      "Val acc: 0.780\n",
      "Epoch: 18/40 Iteration: 380 Train loss: 0.507\n",
      "Epoch: 19/40 Iteration: 385 Train loss: 0.500\n",
      "Epoch: 19/40 Iteration: 390 Train loss: 0.515\n",
      "Epoch: 19/40 Iteration: 395 Train loss: 0.519\n",
      "Epoch: 19/40 Iteration: 400 Train loss: 0.505\n",
      "Val acc: 0.762\n",
      "Epoch: 20/40 Iteration: 405 Train loss: 0.500\n",
      "Epoch: 20/40 Iteration: 410 Train loss: 0.515\n",
      "Epoch: 20/40 Iteration: 415 Train loss: 0.519\n",
      "Epoch: 20/40 Iteration: 420 Train loss: 0.505\n",
      "Epoch: 21/40 Iteration: 425 Train loss: 0.499\n",
      "Val acc: 0.798\n",
      "Epoch: 21/40 Iteration: 430 Train loss: 0.515\n",
      "Epoch: 21/40 Iteration: 435 Train loss: 0.518\n",
      "Epoch: 21/40 Iteration: 440 Train loss: 0.505\n",
      "Epoch: 22/40 Iteration: 445 Train loss: 0.499\n",
      "Epoch: 22/40 Iteration: 450 Train loss: 0.514\n",
      "Val acc: 0.775\n",
      "Epoch: 22/40 Iteration: 455 Train loss: 0.517\n",
      "Epoch: 22/40 Iteration: 460 Train loss: 0.505\n",
      "Epoch: 23/40 Iteration: 465 Train loss: 0.499\n",
      "Epoch: 23/40 Iteration: 470 Train loss: 0.514\n",
      "Epoch: 23/40 Iteration: 475 Train loss: 0.518\n",
      "Val acc: 0.775\n",
      "Epoch: 23/40 Iteration: 480 Train loss: 0.505\n",
      "Epoch: 24/40 Iteration: 485 Train loss: 0.498\n",
      "Epoch: 24/40 Iteration: 490 Train loss: 0.514\n",
      "Epoch: 24/40 Iteration: 495 Train loss: 0.517\n",
      "Epoch: 24/40 Iteration: 500 Train loss: 0.505\n",
      "Val acc: 0.777\n",
      "Epoch: 25/40 Iteration: 505 Train loss: 0.498\n",
      "Epoch: 25/40 Iteration: 510 Train loss: 0.514\n",
      "Epoch: 25/40 Iteration: 515 Train loss: 0.517\n",
      "Epoch: 25/40 Iteration: 520 Train loss: 0.505\n",
      "Epoch: 26/40 Iteration: 525 Train loss: 0.498\n",
      "Val acc: 0.767\n",
      "Epoch: 26/40 Iteration: 530 Train loss: 0.514\n",
      "Epoch: 26/40 Iteration: 535 Train loss: 0.517\n",
      "Epoch: 26/40 Iteration: 540 Train loss: 0.505\n",
      "Epoch: 27/40 Iteration: 545 Train loss: 0.498\n",
      "Epoch: 27/40 Iteration: 550 Train loss: 0.514\n",
      "Val acc: 0.780\n",
      "Epoch: 27/40 Iteration: 555 Train loss: 0.517\n",
      "Epoch: 27/40 Iteration: 560 Train loss: 0.505\n",
      "Epoch: 28/40 Iteration: 565 Train loss: 0.498\n",
      "Epoch: 28/40 Iteration: 570 Train loss: 0.514\n",
      "Epoch: 28/40 Iteration: 575 Train loss: 0.517\n",
      "Val acc: 0.783\n",
      "Epoch: 28/40 Iteration: 580 Train loss: 0.505\n",
      "Epoch: 29/40 Iteration: 585 Train loss: 0.498\n",
      "Epoch: 29/40 Iteration: 590 Train loss: 0.514\n",
      "Epoch: 29/40 Iteration: 595 Train loss: 0.517\n",
      "Epoch: 29/40 Iteration: 600 Train loss: 0.505\n",
      "Val acc: 0.783\n",
      "Epoch: 30/40 Iteration: 605 Train loss: 0.498\n",
      "Epoch: 30/40 Iteration: 610 Train loss: 0.514\n",
      "Epoch: 30/40 Iteration: 615 Train loss: 0.517\n",
      "Epoch: 30/40 Iteration: 620 Train loss: 0.505\n",
      "Epoch: 31/40 Iteration: 625 Train loss: 0.498\n",
      "Val acc: 0.783\n",
      "Epoch: 31/40 Iteration: 630 Train loss: 0.514\n",
      "Epoch: 31/40 Iteration: 635 Train loss: 0.517\n",
      "Epoch: 31/40 Iteration: 640 Train loss: 0.505\n",
      "Epoch: 32/40 Iteration: 645 Train loss: 0.498\n",
      "Epoch: 32/40 Iteration: 650 Train loss: 0.514\n",
      "Val acc: 0.783\n",
      "Epoch: 32/40 Iteration: 655 Train loss: 0.517\n",
      "Epoch: 32/40 Iteration: 660 Train loss: 0.505\n",
      "Epoch: 33/40 Iteration: 665 Train loss: 0.498\n",
      "Epoch: 33/40 Iteration: 670 Train loss: 0.514\n",
      "Epoch: 33/40 Iteration: 675 Train loss: 0.517\n",
      "Val acc: 0.783\n",
      "Epoch: 33/40 Iteration: 680 Train loss: 0.505\n",
      "Epoch: 34/40 Iteration: 685 Train loss: 0.498\n",
      "Epoch: 34/40 Iteration: 690 Train loss: 0.514\n",
      "Epoch: 34/40 Iteration: 695 Train loss: 0.517\n",
      "Epoch: 34/40 Iteration: 700 Train loss: 0.505\n",
      "Val acc: 0.767\n",
      "Epoch: 35/40 Iteration: 705 Train loss: 0.497\n",
      "Epoch: 35/40 Iteration: 710 Train loss: 0.514\n",
      "Epoch: 35/40 Iteration: 715 Train loss: 0.517\n",
      "Epoch: 35/40 Iteration: 720 Train loss: 0.505\n",
      "Epoch: 36/40 Iteration: 725 Train loss: 0.497\n",
      "Val acc: 0.770\n",
      "Epoch: 36/40 Iteration: 730 Train loss: 0.514\n",
      "Epoch: 36/40 Iteration: 735 Train loss: 0.517\n",
      "Epoch: 36/40 Iteration: 740 Train loss: 0.505\n",
      "Epoch: 37/40 Iteration: 745 Train loss: 0.497\n",
      "Epoch: 37/40 Iteration: 750 Train loss: 0.514\n",
      "Val acc: 0.775\n",
      "Epoch: 37/40 Iteration: 755 Train loss: 0.517\n",
      "Epoch: 37/40 Iteration: 760 Train loss: 0.505\n",
      "Epoch: 38/40 Iteration: 765 Train loss: 0.497\n",
      "Epoch: 38/40 Iteration: 770 Train loss: 0.514\n",
      "Epoch: 38/40 Iteration: 775 Train loss: 0.517\n",
      "Val acc: 0.775\n",
      "Epoch: 38/40 Iteration: 780 Train loss: 0.505\n",
      "Epoch: 39/40 Iteration: 785 Train loss: 0.497\n",
      "Epoch: 39/40 Iteration: 790 Train loss: 0.514\n",
      "Epoch: 39/40 Iteration: 795 Train loss: 0.517\n",
      "Epoch: 39/40 Iteration: 800 Train loss: 0.505\n",
      "Val acc: 0.770\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "num_lstm_layer = 1\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "batch_sizes = [400]\n",
    "\n",
    "embedding_dims = [256]\n",
    "lstm_sizes = [128, 256]\n",
    "learning_rates = [0.001, 0.005]\n",
    "keep_probabilities = [0.7, 0.8]\n",
    "\n",
    "# embedding_dims = [200, 300, 400]\n",
    "# lstm_sizes = [128, 256]\n",
    "# learning_rates = [0.001, 0.0005, 0.0001]\n",
    "# keep_probabilities = [0.6, 0.8, 1.0]\n",
    "\n",
    "# Grid Search\n",
    "for batch_size in batch_sizes:\n",
    "    for embedding_dim in embedding_dims:\n",
    "        for lstm_size in lstm_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                for keep_probability in keep_probabilities:\n",
    "                    print(\"---------------------------------------------\")\n",
    "                    print(\"Start with new set of hyperparameters:\")\n",
    "                    print(\"batch_size\", batch_size)\n",
    "                    print(\"embedding_dim\", embedding_dim)\n",
    "                    print(\"lstm_size\", lstm_size)\n",
    "                    print(\"learning_rate\", learning_rate)\n",
    "                    print(\"keep_probability\", keep_probability)\n",
    "                    BiLSTM_RNN_Model(vocab_size, learning_rate, embedding_dim, lstm_size, num_lstm_layer, \n",
    "                       batch_size, keep_probability, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_RNN_Model(vocab_size, learning_rate = 0.001, embedding_dim = 300, \n",
    "                   lstm_size = 256, num_lstm_layer = 1, batch_size = 200, keep_probability=0.8, epochs=400):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # input placeholders\n",
    "        inputs = tf.placeholder(tf.int32, shape=[None, None], name='inputs')\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, None], name='labels')\n",
    "#         seq_len = tf.placeholder(tf.int32, shape=[None], name='seq_len')\n",
    "        keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "        # embedding layer\n",
    "        embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_dim), -1, 1), name = 'embedding')\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "\n",
    "        # LSTM layer\n",
    "        def build_cell(lstm_units, keep_prob):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(num_units=lstm_units)\n",
    "#             cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_units, activation=tf.nn.relu)\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "            return drop\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_lstm_layer)])\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                 embed, \n",
    "#                                                  sequence_length = seq_len,\n",
    "                                                 initial_state = initial_state)\n",
    "\n",
    "\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "    #     cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=predictions)\n",
    "        entroy = tf.reduce_mean(cost)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(entroy)\n",
    "\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), tf.cast(tf.round(labels), tf.int32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        \n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 1\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            state = sess.run(initial_state)\n",
    "\n",
    "            for batch_features, batch_labels, batch_len in get_batches(x_train_, y_train_, x_train_len_, batch_size):\n",
    "                batch_len = np.array(batch_len)\n",
    "\n",
    "                feed = {inputs: batch_features,\n",
    "                        labels: batch_labels[:, None],   # change ths shape of y to [batch_size, 1]\n",
    "#                         seq_len: batch_len,\n",
    "                        keep_prob: keep_probability,\n",
    "                        initial_state: state}\n",
    "                loss, state, _ = sess.run([entroy, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "                if iteration%25==0:\n",
    "    #                   print(loss)\n",
    "                        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                              \"Iteration: {}\".format(iteration),\n",
    "                              \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "                if iteration%25==0:\n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "\n",
    "                    for x, y, batch_len in get_batches(x_test_, y_test_, x_test_len_, batch_size):\n",
    "                        feed = {inputs: x,\n",
    "                                labels: y[:, None],\n",
    "#                                 seq_len: batch_len,\n",
    "                                keep_prob: 1,  # note the keep probablity is 1 here\n",
    "                                initial_state: val_state}\n",
    "                        batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                        val_acc.append(batch_acc)\n",
    "                    print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "                iteration +=1\n",
    "\n",
    "        saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.001\n",
      "keep_probability 0.7\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.673\n",
      "Val acc: 0.630\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.653\n",
      "Val acc: 0.700\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.610\n",
      "Val acc: 0.743\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.561\n",
      "Val acc: 0.772\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.538\n",
      "Val acc: 0.728\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.558\n",
      "Val acc: 0.743\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.541\n",
      "Val acc: 0.752\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.525\n",
      "Val acc: 0.765\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.523\n",
      "Val acc: 0.762\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.528\n",
      "Val acc: 0.780\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.528\n",
      "Val acc: 0.785\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.510\n",
      "Val acc: 0.788\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.503\n",
      "Val acc: 0.790\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.520\n",
      "Val acc: 0.780\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.524\n",
      "Val acc: 0.785\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.508\n",
      "Val acc: 0.777\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.503\n",
      "Val acc: 0.780\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.519\n",
      "Val acc: 0.775\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.523\n",
      "Val acc: 0.775\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.507\n",
      "Val acc: 0.777\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.503\n",
      "Val acc: 0.770\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.519\n",
      "Val acc: 0.770\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.523\n",
      "Val acc: 0.767\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.507\n",
      "Val acc: 0.767\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.001\n",
      "keep_probability 0.8\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.679\n",
      "Val acc: 0.585\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.661\n",
      "Val acc: 0.680\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.606\n",
      "Val acc: 0.738\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.555\n",
      "Val acc: 0.748\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.535\n",
      "Val acc: 0.743\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.553\n",
      "Val acc: 0.745\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.542\n",
      "Val acc: 0.745\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.520\n",
      "Val acc: 0.757\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.515\n",
      "Val acc: 0.767\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.538\n",
      "Val acc: 0.755\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.531\n",
      "Val acc: 0.745\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.511\n",
      "Val acc: 0.752\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.504\n",
      "Val acc: 0.757\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.523\n",
      "Val acc: 0.757\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.519\n",
      "Val acc: 0.757\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.507\n",
      "Val acc: 0.760\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.503\n",
      "Val acc: 0.762\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.522\n",
      "Val acc: 0.755\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.519\n",
      "Val acc: 0.757\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.507\n",
      "Val acc: 0.757\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.501\n",
      "Val acc: 0.752\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.520\n",
      "Val acc: 0.762\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.519\n",
      "Val acc: 0.762\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.507\n",
      "Val acc: 0.762\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.005\n",
      "keep_probability 0.7\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.598\n",
      "Val acc: 0.735\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.554\n",
      "Val acc: 0.777\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.546\n",
      "Val acc: 0.798\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.525\n",
      "Val acc: 0.775\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.508\n",
      "Val acc: 0.777\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.523\n",
      "Val acc: 0.795\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.519\n",
      "Val acc: 0.780\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.506\n",
      "Val acc: 0.800\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.498\n",
      "Val acc: 0.788\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.517\n",
      "Val acc: 0.780\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.516\n",
      "Val acc: 0.788\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.505\n",
      "Val acc: 0.780\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.497\n",
      "Val acc: 0.785\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.515\n",
      "Val acc: 0.780\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.515\n",
      "Val acc: 0.780\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.505\n",
      "Val acc: 0.772\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.497\n",
      "Val acc: 0.770\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.515\n",
      "Val acc: 0.770\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.515\n",
      "Val acc: 0.767\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.505\n",
      "Val acc: 0.765\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.497\n",
      "Val acc: 0.760\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.516\n",
      "Val acc: 0.770\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.514\n",
      "Val acc: 0.762\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.505\n",
      "Val acc: 0.765\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 128\n",
      "learning_rate 0.005\n",
      "keep_probability 0.8\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.598\n",
      "Val acc: 0.743\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.556\n",
      "Val acc: 0.785\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.537\n",
      "Val acc: 0.795\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.538\n",
      "Val acc: 0.808\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.516\n",
      "Val acc: 0.795\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.531\n",
      "Val acc: 0.803\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.522\n",
      "Val acc: 0.815\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.513\n",
      "Val acc: 0.798\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.500\n",
      "Val acc: 0.798\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.519\n",
      "Val acc: 0.793\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.515\n",
      "Val acc: 0.777\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.510\n",
      "Val acc: 0.788\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.500\n",
      "Val acc: 0.783\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.517\n",
      "Val acc: 0.783\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.515\n",
      "Val acc: 0.785\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.509\n",
      "Val acc: 0.783\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.499\n",
      "Val acc: 0.783\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.517\n",
      "Val acc: 0.793\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.515\n",
      "Val acc: 0.783\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.509\n",
      "Val acc: 0.785\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.499\n",
      "Val acc: 0.785\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.516\n",
      "Val acc: 0.785\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.515\n",
      "Val acc: 0.785\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.508\n",
      "Val acc: 0.785\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.001\n",
      "keep_probability 0.7\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.667\n",
      "Val acc: 0.670\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.643\n",
      "Val acc: 0.707\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.597\n",
      "Val acc: 0.735\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.570\n",
      "Val acc: 0.757\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.527\n",
      "Val acc: 0.750\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.542\n",
      "Val acc: 0.733\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.547\n",
      "Val acc: 0.762\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.538\n",
      "Val acc: 0.788\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.516\n",
      "Val acc: 0.783\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.528\n",
      "Val acc: 0.755\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.526\n",
      "Val acc: 0.767\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.515\n",
      "Val acc: 0.765\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.507\n",
      "Val acc: 0.765\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.522\n",
      "Val acc: 0.755\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.525\n",
      "Val acc: 0.752\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.509\n",
      "Val acc: 0.760\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.504\n",
      "Val acc: 0.765\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.522\n",
      "Val acc: 0.755\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.524\n",
      "Val acc: 0.755\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.509\n",
      "Val acc: 0.755\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.504\n",
      "Val acc: 0.755\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.522\n",
      "Val acc: 0.760\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.523\n",
      "Val acc: 0.762\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.760\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.001\n",
      "keep_probability 0.8\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.666\n",
      "Val acc: 0.665\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.630\n",
      "Val acc: 0.702\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.588\n",
      "Val acc: 0.728\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.559\n",
      "Val acc: 0.745\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.532\n",
      "Val acc: 0.743\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.534\n",
      "Val acc: 0.748\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.535\n",
      "Val acc: 0.762\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.526\n",
      "Val acc: 0.762\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.529\n",
      "Val acc: 0.770\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.531\n",
      "Val acc: 0.762\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.525\n",
      "Val acc: 0.740\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.510\n",
      "Val acc: 0.735\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.510\n",
      "Val acc: 0.723\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.519\n",
      "Val acc: 0.725\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.521\n",
      "Val acc: 0.728\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.506\n",
      "Val acc: 0.733\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.501\n",
      "Val acc: 0.735\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.519\n",
      "Val acc: 0.728\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.520\n",
      "Val acc: 0.733\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.506\n",
      "Val acc: 0.733\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.500\n",
      "Val acc: 0.730\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.519\n",
      "Val acc: 0.730\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.520\n",
      "Val acc: 0.730\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.506\n",
      "Val acc: 0.730\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.005\n",
      "keep_probability 0.7\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.693\n",
      "Val acc: 0.553\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.693\n",
      "Val acc: 0.553\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.619\n",
      "Val acc: 0.745\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.561\n",
      "Val acc: 0.760\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.522\n",
      "Val acc: 0.795\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.522\n",
      "Val acc: 0.790\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.523\n",
      "Val acc: 0.777\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.514\n",
      "Val acc: 0.772\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.504\n",
      "Val acc: 0.783\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.514\n",
      "Val acc: 0.805\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.521\n",
      "Val acc: 0.810\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.506\n",
      "Val acc: 0.805\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.498\n",
      "Val acc: 0.790\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.512\n",
      "Val acc: 0.822\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.518\n",
      "Val acc: 0.810\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.505\n",
      "Val acc: 0.808\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.498\n",
      "Val acc: 0.803\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.512\n",
      "Val acc: 0.803\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.517\n",
      "Val acc: 0.803\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.505\n",
      "Val acc: 0.800\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.498\n",
      "Val acc: 0.798\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.512\n",
      "Val acc: 0.795\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.517\n",
      "Val acc: 0.803\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.505\n",
      "Val acc: 0.805\n",
      "-------------------------------------------------------------------\n",
      "Start with new set of hyperparameters:\n",
      "batch_size 400\n",
      "embedding_dim 256\n",
      "lstm_size 256\n",
      "learning_rate 0.005\n",
      "keep_probability 0.8\n",
      "Epoch: 1/30 Iteration: 25 Train loss: 0.688\n",
      "Val acc: 0.555\n",
      "Epoch: 2/30 Iteration: 50 Train loss: 0.593\n",
      "Val acc: 0.760\n",
      "Epoch: 3/30 Iteration: 75 Train loss: 0.543\n",
      "Val acc: 0.770\n",
      "Epoch: 4/30 Iteration: 100 Train loss: 0.527\n",
      "Val acc: 0.767\n",
      "Epoch: 6/30 Iteration: 125 Train loss: 0.512\n",
      "Val acc: 0.790\n",
      "Epoch: 7/30 Iteration: 150 Train loss: 0.524\n",
      "Val acc: 0.795\n",
      "Epoch: 8/30 Iteration: 175 Train loss: 0.522\n",
      "Val acc: 0.770\n",
      "Epoch: 9/30 Iteration: 200 Train loss: 0.508\n",
      "Val acc: 0.780\n",
      "Epoch: 11/30 Iteration: 225 Train loss: 0.502\n",
      "Val acc: 0.795\n",
      "Epoch: 12/30 Iteration: 250 Train loss: 0.518\n",
      "Val acc: 0.757\n",
      "Epoch: 13/30 Iteration: 275 Train loss: 0.521\n",
      "Val acc: 0.785\n",
      "Epoch: 14/30 Iteration: 300 Train loss: 0.507\n",
      "Val acc: 0.772\n",
      "Epoch: 16/30 Iteration: 325 Train loss: 0.502\n",
      "Val acc: 0.770\n",
      "Epoch: 17/30 Iteration: 350 Train loss: 0.518\n",
      "Val acc: 0.765\n",
      "Epoch: 18/30 Iteration: 375 Train loss: 0.519\n",
      "Val acc: 0.788\n",
      "Epoch: 19/30 Iteration: 400 Train loss: 0.506\n",
      "Val acc: 0.780\n",
      "Epoch: 21/30 Iteration: 425 Train loss: 0.501\n",
      "Val acc: 0.772\n",
      "Epoch: 22/30 Iteration: 450 Train loss: 0.516\n",
      "Val acc: 0.780\n",
      "Epoch: 23/30 Iteration: 475 Train loss: 0.518\n",
      "Val acc: 0.775\n",
      "Epoch: 24/30 Iteration: 500 Train loss: 0.506\n",
      "Val acc: 0.765\n",
      "Epoch: 26/30 Iteration: 525 Train loss: 0.500\n",
      "Val acc: 0.755\n",
      "Epoch: 27/30 Iteration: 550 Train loss: 0.516\n",
      "Val acc: 0.767\n",
      "Epoch: 28/30 Iteration: 575 Train loss: 0.518\n",
      "Val acc: 0.770\n",
      "Epoch: 29/30 Iteration: 600 Train loss: 0.506\n",
      "Val acc: 0.767\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "num_lstm_layer = 1\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# batch_size 400\n",
    "# embedding_dim 300\n",
    "# lstm_size 128\n",
    "# learning_rate 0.005\n",
    "# keep_probability 0.8\n",
    "\n",
    "batch_sizes = [400]\n",
    "\n",
    "embedding_dims = [256]\n",
    "lstm_sizes = [128, 256]\n",
    "learning_rates = [0.001, 0.005]\n",
    "keep_probabilities = [0.7, 0.8]\n",
    "\n",
    "# embedding_dims = [200, 300, 400]\n",
    "# lstm_sizes = [128, 256]\n",
    "# learning_rates = [0.001, 0.0005, 0.0001]\n",
    "# keep_probabilities = [0.6, 0.8, 1.0]\n",
    "\n",
    "# Grid Search\n",
    "for batch_size in batch_sizes:\n",
    "    for embedding_dim in embedding_dims:\n",
    "        for lstm_size in lstm_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                for keep_probability in keep_probabilities:\n",
    "                    print(\"-------------------------------------------------------------------\")\n",
    "                    print(\"Start with new set of hyperparameters:\")\n",
    "                    print(\"batch_size\", batch_size)\n",
    "                    print(\"embedding_dim\", embedding_dim)\n",
    "                    print(\"lstm_size\", lstm_size)\n",
    "                    print(\"learning_rate\", learning_rate)\n",
    "                    print(\"keep_probability\", keep_probability)\n",
    "                    LSTM_RNN_Model(vocab_size, learning_rate, embedding_dim, lstm_size, num_lstm_layer, \n",
    "                       batch_size, keep_probability, epochs)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
