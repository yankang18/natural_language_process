{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis for RNN by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers.core import Activation, Dropout \n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file=None):\n",
    "    if data_file == None:\n",
    "        return\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/large_files/stanford_sentiment/parsed_data/'\n",
    "word2idx = load_data(folder + \"sentiment_word2idx.json\")\n",
    "sentiment_binary_train = load_data(folder + \"sentiment_binary_train.json\")\n",
    "sentiment_train = load_data(folder + \"sentiment_train.json\")\n",
    "sentiment_binary_test = load_data(folder + \"sentiment_binary_test.json\")\n",
    "sentiment_test = load_data(folder + \"sentiment_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude neutral samples\n",
    "\n",
    "* The loaded samples has three type of labels -1,0,1, in which -1 indicates neutral sentiment.\n",
    "* We exclude samples with neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: # of training samples and # of test samples\n",
      "# of traing samples:  6920\n",
      "# of test samples:  1821\n"
     ]
    }
   ],
   "source": [
    "# the loaded samples has three type of labels -1,0,1, in which -1 indicates neutral sentiment.\n",
    "# We exclude samples with neutral sentiment.\n",
    "def exclude_neutral_sample(samples:dict):\n",
    "    ssamples = {}\n",
    "    for k, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            ssamples[k] = v\n",
    "    return ssamples\n",
    "        \n",
    "train_b = exclude_neutral_sample(sentiment_binary_train)\n",
    "test_b = exclude_neutral_sample(sentiment_binary_test)\n",
    "\n",
    "print(\"After filtering: # of training samples and # of test samples\")\n",
    "print(\"# of traing samples: \", len(train_b))\n",
    "print(\"# of test samples: \", len(test_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert training/test data to sentences\n",
    "* Currently, the training/test data are in the form of integer sequences, which are directly parsed from Stanford sentimental analysis raw data. We have not done any preprocessing on those data yet. \n",
    "* The purpose of coverting training/test data into sentences is that we will do some preprocessing on these sentences. Then, we will convert sentences back to integer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(wordidx, idx2word:dict):\n",
    "    wordlist = []\n",
    "    for idx in wordidx:\n",
    "        if idx != -1:\n",
    "            token = idx2word[idx]\n",
    "            if token not in string.punctuation:\n",
    "                wordlist.append(token)\n",
    "    return wordlist\n",
    "\n",
    "# Convert training/test examples to the form that each example is list of words (including punctation).\n",
    "# Also construct the targets for each example.\n",
    "def get_comments_samples(samples:dict, idx2word:dict):\n",
    "    comments = []\n",
    "    targets = []\n",
    "    for _, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            comment = \" \".join(get_comment(v[0], idx2word))\n",
    "            label = v[3][-1]\n",
    "            comments.append(comment)\n",
    "            targets.append(label) \n",
    "    return comments, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size 18647\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "train_comments_o, train_targets = get_comments_samples(train_b, idx2word)\n",
    "test_comments_o, test_targets = get_comments_samples(test_b, idx2word)\n",
    "\n",
    "vocabulary_size = len(idx2word)\n",
    "print('vocabulary_size' , vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3310\n",
      "1 3610\n",
      "-1 0\n"
     ]
    }
   ],
   "source": [
    "# Test: just see the split of training data in terms of label\n",
    "\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(len(train_comments_o)):\n",
    "\n",
    "    if train_targets[i] == 0:\n",
    "        count0 += 1\n",
    "    elif train_targets[i] == 1:\n",
    "        count1 += 1\n",
    "    else:\n",
    "        count2 += 1\n",
    "    \n",
    "print(\"0\", count0)\n",
    "print(\"1\", count1)\n",
    "print(\"-1\", count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and analyzing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "* Remove stopwords\n",
    "* Remove punctuation\n",
    "* Tokenization\n",
    "* Change the representation of training/test data to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(samples):\n",
    "    filtered_samples = []\n",
    "    for i in samples:\n",
    "        filtered_samples.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return filtered_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    tokenized_samples = []\n",
    "    for s in samples:\n",
    "        tokens = word_tokenize(s)\n",
    "        tokenized_samples.append(tokens)\n",
    "    return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'but', 'so', \"isn't\", 'doesn', 'between', 'shan', 'other', 'then', 'each', 'aren', 'ma', 'about', 'him', 'won', 'them', 'her', 'did', 'when', \"haven't\", 'we', \"you're\", \"that'll\", 'out', 'over', 'theirs', 'after', 'down', \"needn't\", 'an', 'a', 'this', 'am', \"hasn't\", 'of', 'until', 'to', 'wasn', 'against', 'yourself', 'by', 'above', 'off', 'now', 'while', \"don't\", \"wouldn't\", 've', 'be', 'at', 'itself', \"you'd\", 'll', 'yours', \"you'll\", 'me', 'through', 'for', 'himself', 'couldn', 'ain', 'which', \"couldn't\", \"she's\", 'your', 'hasn', 'and', \"wasn't\", 'because', 'where', 'those', 'whom', 'under', 'he', 'up', 'wouldn', 'doing', 'i', 'you', 'what', 'no', 'does', 'our', 'can', 'had', 'isn', 'didn', 'having', 'it', 't', 'too', 'both', 'during', 'in', 'from', 'myself', 'before', 'than', 'some', 'mightn', 're', \"shan't\", 'ourselves', \"hadn't\", 'his', \"aren't\", \"doesn't\", 'not', \"it's\", 'my', 'these', 'o', 'being', 'shouldn', 'hers', 'the', 'below', 'was', 'here', 'again', 'with', 'own', \"won't\", \"mightn't\", \"weren't\", \"mustn't\", 'should', 'are', 'yourselves', 'will', 'there', 'or', 'on', 'if', 'into', \"didn't\", 'they', 's', 'few', \"shouldn't\", 'needn', 'just', 'further', 'only', 'nor', \"you've\", 'more', 'm', 'were', 'that', 'she', 'don', 'do', 'all', 'how', 'same', 'haven', 'herself', 'as', 'have', 'y', 'been', 'has', 'd', 'their', 'most', 'themselves', 'why', 'who', 'its', 'weren', \"should've\", 'hadn', 'ours', 'very', 'is', 'once', 'any', 'such', 'mustn'}\n"
     ]
    }
   ],
   "source": [
    "# default stopwords set from NLTK\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords version 1: exclude negation words from the stopwords set\n",
    "stopWords_revised = set(('at', 'how', 'each', 's', 'those', 'from','whom', 'if', 're', 'we', 'by','into', 'it', 'ma', 'than', \"you'll\", \n",
    "             'very', 'was', 'is', 'be', 'had', 'you', 'hers', 'off', 'her', 'your', 'other', 'on', 'down', 'its', 'should', \n",
    "             'which', 'now', 'ours', 'in', \"you've\", 'before', 'further', 'below', 'did',  'who', 'once', 'some', 'being', \n",
    "             'does', 'too', 'herself', 'about', 'my', 'are', 'during', 'few', 'an', 'do', 'over',  'themselves', 'the', 'why', 'a', 'same', 'all', \n",
    "             'own', 'with', 'under', 'myself', 'he', 'because', 'again', 'himself', 'these', 'that', 'am', 'through', 'll', 'so', 've', \"you're\", 'doing', 'between', \n",
    "             'when', 'ourselves', 'been', 'of', 'our', 'them', 'their', 'while', 'as', 'can', 'where', 'such', 'yourself', 'haven', 'they', 'theirs', 'm', 'both', \n",
    "                     \"that'll\", 'or', 'were', 'up', 'will', 'me', 'yours', 'itself', 'has', 'more', \n",
    "                'd', 'o', 'what', 'having', 't', 'this', 'after', 'then', 'above', 'out', 'nor', \"should've\", 'his', \n",
    "               \"you'd\", \"she's\", 'and', 'shan', 'until', 'here', 'for', 'just', 'him', 'to', 'have', 'she', 'yourselves', \"it's\", 'y', 'i', 'there'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords version 2: exclude negation words and some comparision words from the stopwords set\n",
    "stopWords_revised_2 = set(('at', 'each', 's', 'those', 'from', 'if', 're', 'we', 'by','into', 'it', 'ma', \"you'll\", \n",
    "             'very', 'was', 'is', 'be', 'had', 'you', 'hers', 'off', 'her', 'your', 'other', 'on', 'down', 'its', 'should', \n",
    "              'now', 'ours', 'in', \"you've\", 'below', 'did', 'being', \n",
    "             'does', 'herself', 'about', 'my', 'are', 'an', 'do', 'themselves', 'the',  'a',  \n",
    "             'own', 'myself', 'he',  'himself', 'these', 'that', 'am', 'll', 'so', 've', \"you're\", 'doing', \n",
    "            'ourselves', 'been', 'of', 'our', 'them', 'their', 'can', 'yourself', 'they', 'theirs', 'm', \n",
    "                     \"that'll\", 'or', 'were', 'up', 'will', 'me', 'yours', 'itself', 'has', \n",
    "                'd', 'o', 'having', 't', 'this', 'after', 'then', 'out', \"should've\", 'his', \n",
    "               \"you'd\", \"she's\", 'shan',  'here', 'for', 'him', 'to', 'have', 'she', 'yourselves', \"it's\", 'y', 'i', 'there'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(samples):\n",
    "    filtered_samples = []\n",
    "    for s in samples:\n",
    "        filstered_tokens = []\n",
    "        for w in s:\n",
    "            if w not in stopWords_revised_2:\n",
    "                filstered_tokens.append(w)\n",
    "        filtered_samples.append(filstered_tokens)\n",
    "    return filtered_samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_o[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments_1 = []\n",
    "for comment in train_comments_o:\n",
    "    new_str = comment.replace(\"n't\", 'not')\n",
    "    train_comments_1.append(new_str)\n",
    "    \n",
    "test_comments_1 = []\n",
    "for comment in test_comments_o:\n",
    "    new_str = comment.replace(\"n't\", 'not')\n",
    "    test_comments_1.append(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation removed\n"
     ]
    }
   ],
   "source": [
    "train_comments_punc = remove_punctuation(train_comments_1)\n",
    "test_comments_punc = remove_punctuation(test_comments_1)\n",
    "print(\"punctuation removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_punc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized\n"
     ]
    }
   ],
   "source": [
    "train_comments_tokenized = tokenize(train_comments_punc)\n",
    "test_comments_tokenized = tokenize(test_comments_punc)\n",
    "print(\"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_tokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords removed\n"
     ]
    }
   ],
   "source": [
    "train_comments_stopwords = remove_stopwords(train_comments_tokenized)\n",
    "test_comments_stopwords = remove_stopwords(test_comments_tokenized)\n",
    "print(\"stopwords removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# for i in range(100):\n",
    "#     print(i, \": \", train_comments_stopwords[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert training/test data in the form of sentences into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the comments to gather all the words for the vocabulary\n",
    "def combine(tokenized_comments):\n",
    "    text = []\n",
    "    for comment in tokenized_comments:\n",
    "        text += comment\n",
    "    return text\n",
    "\n",
    "all_words = combine(train_comments_stopwords) + combine(test_comments_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "# Create your dictionary that maps vocab words to integers here\n",
    "from collections import Counter\n",
    "counts = Counter(all_words)\n",
    "print(type(counts))\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Create words-to-index map. \n",
    "# Note that index start from 1\n",
    "vocab_to_int = {word:i for i, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of vocab <class 'list'>\n",
      "total # of words:  16547\n",
      "first word: and\n",
      "last word: hired\n",
      "first word index: 1\n",
      "last word index 16547\n"
     ]
    }
   ],
   "source": [
    "print(\"type of vocab\", type(vocab))\n",
    "print('total # of words: ', len(vocab_to_int))\n",
    "print(\"first word:\", vocab[0])\n",
    "print(\"last word:\", vocab[-1])\n",
    "print(\"first word index:\", vocab_to_int[vocab[0]])\n",
    "print(\"last word index\", vocab_to_int[vocab[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index-to-words map.\n",
    "index2word = {idx:word for word, idx in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(reviews, vocab_to_int):\n",
    "    # Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "    print('# of reviews before index: ', len(reviews))\n",
    "    reviews_ints = []\n",
    "    for review in reviews:\n",
    "        reviews_ints.append([vocab_to_int[word] for word in review])\n",
    "\n",
    "    print('# of reviews after index: ', len(reviews_ints))\n",
    "    return reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reviews before index:  6920\n",
      "# of reviews after index:  6920\n"
     ]
    }
   ],
   "source": [
    "# convert the representation of training examples from text to integer\n",
    "x_train = convert_to_int(train_comments_stopwords, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[428, 1555, 2686, 612, 5786, 645]\n",
      "visually santa clause 2 is wondrously creative\n",
      "['visually', 'santa', 'clause', '2', 'wondrously', 'creative']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(x_train[7])\n",
    "print(train_comments_o[7])\n",
    "text = [index2word[idx] for idx in x_train[7]]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reviews before index:  1821\n",
      "# of reviews after index:  1821\n"
     ]
    }
   ],
   "source": [
    "# convert the representation of test examples from text to integer\n",
    "x_test = convert_to_int(test_comments_stopwords, vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate/analyze distribution over training/test example lengths\n",
    "* The purpose for this analysis is to find the best `maximum sequence length` for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 2\n",
      "Maximum train example length: 36\n",
      "Maximum train example length: 0\n",
      "Counter({10: 465, 9: 454, 6: 436, 11: 434, 12: 432, 13: 422, 8: 414, 7: 398, 14: 368, 15: 338, 5: 335, 4: 331, 16: 286, 17: 259, 3: 252, 18: 238, 19: 189, 2: 187, 20: 140, 21: 124, 22: 86, 23: 83, 1: 54, 24: 53, 25: 53, 26: 29, 27: 23, 28: 11, 30: 9, 29: 8, 31: 3, 0: 2, 32: 2, 33: 1, 36: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create length to frequency map\n",
    "x_train_lens_map = Counter([len(x) for x in x_train])\n",
    "x_train_lens = [len(x) for x in x_train]\n",
    "print(\"Zero-length reviews: {}\".format(x_train_lens_map[0]))\n",
    "print(\"Maximum train example length: {}\".format(max(x_train_lens_map)))\n",
    "print(\"Maximum train example length: {}\".format(x_train_lens_map[-1]))\n",
    "print(x_train_lens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum test example length: 31\n",
      "11.324566473988439\n",
      "11.314662273476111\n"
     ]
    }
   ],
   "source": [
    "# Create length to frequency map\n",
    "x_test_lens_map = Counter([len(x) for x in x_test])\n",
    "x_test_lens = [len(x) for x in x_test]\n",
    "print(\"Zero-length reviews: {}\".format(x_test_lens_map[0]))\n",
    "print(\"Maximum test example length: {}\".format(max(x_test_lens_map)))\n",
    "\n",
    "ave_len = 0\n",
    "for i in x_train:\n",
    "    ave_len += len(i) \n",
    "ave_len = ave_len / len(x_train)\n",
    "print(ave_len)\n",
    "\n",
    "ave_len = 0\n",
    "for i in x_test:\n",
    "    ave_len += len(i)  \n",
    "ave_len = ave_len / len(x_test)\n",
    "print(ave_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f5dff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nToNeKaX8nAa9Ukr5uf8PX//QcGM9dL8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f561d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(x_train_lens_map.keys()), list(x_train_lens_map.values()))\n",
    "# plt.axis([1000, 1500, 0, 55])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(x_test_lens_map.keys()), list(x_test_lens_map.values()))\n",
    "# plt.axis([1000, 1500, 0, 55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried different lengths as the maximum sequence length, 36 gives us best result for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_padding(reviews_ints, seq_len):\n",
    "    \n",
    "    # The features created here are the data that we are going to train and test the network\n",
    "\n",
    "    # Create features with shape (len(reviews_ints), seq_len) and initialized with zeros\n",
    "    features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "\n",
    "    print(features.shape)\n",
    "    # Create list holding the length for each review\n",
    "    lengths = []\n",
    "\n",
    "    # row is the review in forms of a list of integers\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "\n",
    "        # left padding\n",
    "        features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "        \n",
    "        # record the length of each review. This might be useful when we want to use sequence_length argument\n",
    "        # of tf.nn.dynamic_rnn(...)\n",
    "        lengths.append(len(row) if len(row) < seq_len else seq_len)\n",
    "        \n",
    "    return features, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# sample1 = [4,6,7,2,3,5,7,8,1]\n",
    "# sample2 = [1,2,3,4,6,7,2,3,5,7]\n",
    "# sample3 = [1,2,3,4,6,7,2,3,26,1, 11, 12]\n",
    "# sample4 = [8,7,3]\n",
    "# samples = []\n",
    "# samples.append(sample1)\n",
    "# samples.append(sample2)\n",
    "# samples.append(sample3)\n",
    "# samples.append(sample4)\n",
    "# features_, lengths_ = left_padding(samples, 12)\n",
    "\n",
    "# print(features_)\n",
    "# print(lengths_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude training/test examples with 0 length. It is possible that some examples may have 0 length since we did \n",
    "# punctuation and stopword removing. \n",
    "x_train_f = []\n",
    "y_train_f = []\n",
    "for idx in range(len(x_train)):\n",
    "    if len(x_train[idx]) != 0:\n",
    "        x_train_f.append(x_train[idx])\n",
    "        y_train_f.append(train_targets[idx])\n",
    "\n",
    "x_test_f = []\n",
    "y_test_f = []\n",
    "for idx in range(len(x_test)):\n",
    "    if len(x_test[idx]) != 0:\n",
    "        x_test_f.append(x_test[idx])\n",
    "        y_test_f.append(test_targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6918, 36)\n",
      "(1821, 36)\n"
     ]
    }
   ],
   "source": [
    "# perform padding\n",
    "train_seq_len = 36\n",
    "test_seq_len = 31\n",
    "x_train_p, x_train_len = left_padding(x_train_f, train_seq_len)\n",
    "x_test_p, x_test_len = left_padding(x_test_f, train_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        10263,   449,   961,   380,    41,   546,     1,  5586,   690],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0, 11967,   365,\n",
       "         1896,  2664,  7113,  1349,  3363,  8490,     1, 10225,   146],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,  2383,   285,  7139],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,    88,   480,\n",
       "            8,     4,   109,   144,  1386,  2501,   372,  1176,   151],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,   694,   382,\n",
       "          307,     6,    68,   533,     4,     5,  2175,     1,     8]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "x_train_p[:5,:train_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004\n",
      "735\n",
      "8004\n",
      "735\n",
      "8004\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "# Add more data for training, since RNN consumes lots of data\n",
    "x_train_ = np.append(x_train_p, x_test_p[:1086], axis=0)\n",
    "x_test_ = x_test_p[1086:]\n",
    "\n",
    "y_train_ = np.array(np.append(y_train_f, y_test_f[:1086], axis=0))\n",
    "y_test_ = np.array(y_test_f[1086:])\n",
    "\n",
    "x_train_len_ = np.append(x_train_len, x_test_len[:1086], axis=0)\n",
    "x_test_len_ = x_test_len[1086:]\n",
    "\n",
    "print(len(x_train_))\n",
    "print(len(x_test_))\n",
    "print(len(y_train_))\n",
    "print(len(y_test_))\n",
    "print(len(x_train_len_))\n",
    "print(len(x_test_len_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_.shape)\n",
    "# print(x_test_.shape)\n",
    "# print(y_train_.shape)\n",
    "# print(y_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(input_shape, vocab_size, embedding_dim=256, rnn_units=128, dropout_prob=0.8):\n",
    "    \"\"\"\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param vocab_size: Number of unique English words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_shape=input_shape[1:]))\n",
    "    # Encoder with bidirectional RNN\n",
    "    model.add(LSTM(rnn_units, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(dropout_prob))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # checkpoint\n",
    "    filepath=\"weights.best.hdf5\"\n",
    "#     filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM_model(input_shape, vocab_size, embedding_dim=256, rnn_units=128, dropout_prob=0.8):\n",
    "    \"\"\"\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param vocab_size: Number of unique English words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_shape=input_shape[1:]))\n",
    "    # Encoder with bidirectional RNN\n",
    "    model.add(Bidirectional(LSTM(rnn_units, activation='relu', return_sequences=False)))\n",
    "    model.add(Dropout(dropout_prob))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # checkpoint\n",
    "    filepath=\"weights.best.hdf5\"\n",
    "#     filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "e 256\n",
      "r 128\n",
      "d 0.7\n",
      "Using LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 36, 256)           4236288   \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,433,537\n",
      "Trainable params: 4,433,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7203 samples, validate on 801 samples\n",
      "Epoch 1/10\n",
      "7203/7203 [==============================] - 22s 3ms/step - loss: 0.6895 - acc: 0.5227 - val_loss: 0.6861 - val_acc: 0.4931\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49313, saving model to weights.best.hdf5\n",
      "Epoch 2/10\n",
      "7203/7203 [==============================] - 19s 3ms/step - loss: 0.6405 - acc: 0.6047 - val_loss: 0.5934 - val_acc: 0.7503\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49313 to 0.75031, saving model to weights.best.hdf5\n",
      "Epoch 3/10\n",
      "7203/7203 [==============================] - 17s 2ms/step - loss: 0.4865 - acc: 0.8478 - val_loss: 0.6143 - val_acc: 0.7828\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75031 to 0.78277, saving model to weights.best.hdf5\n",
      "Epoch 4/10\n",
      "7203/7203 [==============================] - 14s 2ms/step - loss: 0.3228 - acc: 0.8971 - val_loss: 0.6028 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.78277\n",
      "Epoch 5/10\n",
      "7203/7203 [==============================] - 13s 2ms/step - loss: 0.2613 - acc: 0.9349 - val_loss: 0.6321 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78277 to 0.81273, saving model to weights.best.hdf5\n",
      "Epoch 6/10\n",
      "7203/7203 [==============================] - 13s 2ms/step - loss: 0.1590 - acc: 0.9588 - val_loss: 0.9075 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81273\n",
      "Epoch 7/10\n",
      "7203/7203 [==============================] - 17s 2ms/step - loss: 0.1185 - acc: 0.9753 - val_loss: 0.7688 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81273\n",
      "Epoch 8/10\n",
      "7203/7203 [==============================] - 23s 3ms/step - loss: 0.1118 - acc: 0.9758 - val_loss: 0.8829 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81273\n",
      "Epoch 9/10\n",
      "7203/7203 [==============================] - 18s 3ms/step - loss: 0.1063 - acc: 0.9776 - val_loss: 0.6486 - val_acc: 0.7815\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81273\n",
      "Epoch 10/10\n",
      "7203/7203 [==============================] - 17s 2ms/step - loss: 0.0707 - acc: 0.9861 - val_loss: 0.7621 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81273\n",
      "['loss', 'acc']\n",
      "735/735 [==============================] - 6s 8ms/step\n",
      "[0.5689275536407419, 0.8081632730912189]\n",
      "---------------------------------------------------------------------\n",
      "e 256\n",
      "r 128\n",
      "d 0.8\n",
      "Using LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 36, 256)           4236288   \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,433,537\n",
      "Trainable params: 4,433,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7203 samples, validate on 801 samples\n",
      "Epoch 1/10\n",
      "7203/7203 [==============================] - 23s 3ms/step - loss: 0.6907 - acc: 0.5209 - val_loss: 0.6883 - val_acc: 0.4969\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49688, saving model to weights.best.hdf5\n",
      "Epoch 2/10\n",
      "7203/7203 [==============================] - 14s 2ms/step - loss: 0.6672 - acc: 0.5592 - val_loss: 0.6409 - val_acc: 0.6055\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49688 to 0.60549, saving model to weights.best.hdf5\n",
      "Epoch 3/10\n",
      "7203/7203 [==============================] - 20s 3ms/step - loss: 0.5412 - acc: 0.7729 - val_loss: 0.5065 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60549 to 0.80649, saving model to weights.best.hdf5\n",
      "Epoch 4/10\n",
      "7203/7203 [==============================] - 21s 3ms/step - loss: 0.3314 - acc: 0.9080 - val_loss: 0.6296 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80649 to 0.81773, saving model to weights.best.hdf5\n",
      "Epoch 5/10\n",
      "7203/7203 [==============================] - 14s 2ms/step - loss: 0.2130 - acc: 0.9450 - val_loss: 0.9562 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81773\n",
      "Epoch 6/10\n",
      "7203/7203 [==============================] - 13s 2ms/step - loss: 0.1429 - acc: 0.9653 - val_loss: 0.8515 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81773\n",
      "Epoch 7/10\n",
      "7203/7203 [==============================] - 14s 2ms/step - loss: 0.1050 - acc: 0.9754 - val_loss: 0.5612 - val_acc: 0.7903\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81773\n",
      "Epoch 8/10\n",
      "7203/7203 [==============================] - 13s 2ms/step - loss: 0.0899 - acc: 0.9818 - val_loss: 1.2268 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81773\n",
      "Epoch 9/10\n",
      "7203/7203 [==============================] - 14s 2ms/step - loss: 0.0685 - acc: 0.9857 - val_loss: 1.4735 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81773\n",
      "Epoch 10/10\n",
      "7203/7203 [==============================] - 14s 2ms/step - loss: 0.0546 - acc: 0.9883 - val_loss: 1.5825 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81773\n",
      "['loss', 'acc']\n",
      "735/735 [==============================] - 4s 5ms/step\n",
      "[0.6176379237856183, 0.8149659962070231]\n",
      "---------------------------------------------------------------------\n",
      "e 256\n",
      "r 256\n",
      "d 0.7\n",
      "Using LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 36, 256)           4236288   \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,761,857\n",
      "Trainable params: 4,761,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7203 samples, validate on 801 samples\n",
      "Epoch 1/10\n",
      "7203/7203 [==============================] - 28s 4ms/step - loss: 0.6900 - acc: 0.5351 - val_loss: 0.6850 - val_acc: 0.4981\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49813, saving model to weights.best.hdf5\n",
      "Epoch 2/10\n",
      "7203/7203 [==============================] - 23s 3ms/step - loss: 0.6316 - acc: 0.6511 - val_loss: 0.5460 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49813 to 0.77528, saving model to weights.best.hdf5\n",
      "Epoch 3/10\n",
      "7203/7203 [==============================] - 24s 3ms/step - loss: 0.4147 - acc: 0.8634 - val_loss: 0.6292 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77528 to 0.80524, saving model to weights.best.hdf5\n",
      "Epoch 4/10\n",
      "7203/7203 [==============================] - 23s 3ms/step - loss: 0.2181 - acc: 0.9360 - val_loss: 0.6918 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80524 to 0.82272, saving model to weights.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "7203/7203 [==============================] - 25s 3ms/step - loss: 0.2024 - acc: 0.9500 - val_loss: 0.4522 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82272\n",
      "Epoch 6/10\n",
      "7203/7203 [==============================] - 29s 4ms/step - loss: 0.1489 - acc: 0.9731 - val_loss: 0.8003 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82272\n",
      "Epoch 7/10\n",
      "7203/7203 [==============================] - 27s 4ms/step - loss: 0.0765 - acc: 0.9833 - val_loss: 0.5427 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82272\n",
      "Epoch 8/10\n",
      "7203/7203 [==============================] - 24s 3ms/step - loss: 0.0518 - acc: 0.9875 - val_loss: 0.8939 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.82272\n",
      "Epoch 9/10\n",
      "7203/7203 [==============================] - 26s 4ms/step - loss: 0.0313 - acc: 0.9908 - val_loss: 0.9980 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82272\n",
      "Epoch 10/10\n",
      "7203/7203 [==============================] - 24s 3ms/step - loss: 0.0205 - acc: 0.9944 - val_loss: 1.2998 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82272\n",
      "['loss', 'acc']\n",
      "735/735 [==============================] - 4s 6ms/step\n",
      "[0.7368396480067246, 0.8108843674465102]\n",
      "---------------------------------------------------------------------\n",
      "e 256\n",
      "r 256\n",
      "d 0.8\n",
      "Using LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 36, 256)           4236288   \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,761,857\n",
      "Trainable params: 4,761,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7203 samples, validate on 801 samples\n",
      "Epoch 1/10\n",
      "7203/7203 [==============================] - 45s 6ms/step - loss: 0.6902 - acc: 0.5295 - val_loss: 0.6853 - val_acc: 0.4956\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49563, saving model to weights.best.hdf5\n",
      "Epoch 2/10\n",
      "7203/7203 [==============================] - 32s 4ms/step - loss: 0.6371 - acc: 0.6356 - val_loss: 0.5949 - val_acc: 0.7603\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49563 to 0.76030, saving model to weights.best.hdf5\n",
      "Epoch 3/10\n",
      "7203/7203 [==============================] - 26s 4ms/step - loss: 0.4583 - acc: 0.8423 - val_loss: 0.5675 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76030\n",
      "Epoch 4/10\n",
      "7203/7203 [==============================] - 24s 3ms/step - loss: 0.3484 - acc: 0.9120 - val_loss: 0.5352 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76030 to 0.80649, saving model to weights.best.hdf5\n",
      "Epoch 5/10\n",
      "7203/7203 [==============================] - 29s 4ms/step - loss: 0.1811 - acc: 0.9470 - val_loss: 0.5983 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.80649\n",
      "Epoch 6/10\n",
      "7203/7203 [==============================] - 31s 4ms/step - loss: 0.1683 - acc: 0.9617 - val_loss: 0.7462 - val_acc: 0.7903\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.80649\n",
      "Epoch 7/10\n",
      "7203/7203 [==============================] - 33s 5ms/step - loss: 0.1220 - acc: 0.9731 - val_loss: 0.9501 - val_acc: 0.7890\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80649\n",
      "Epoch 8/10\n",
      "7203/7203 [==============================] - 33s 5ms/step - loss: 0.0885 - acc: 0.9799 - val_loss: 1.1210 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80649\n",
      "Epoch 9/10\n",
      "7203/7203 [==============================] - 27s 4ms/step - loss: 0.0818 - acc: 0.9804 - val_loss: 0.9051 - val_acc: 0.7878\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80649\n",
      "Epoch 10/10\n",
      "7203/7203 [==============================] - 28s 4ms/step - loss: 0.0677 - acc: 0.9863 - val_loss: 1.0440 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80649\n",
      "['loss', 'acc']\n",
      "735/735 [==============================] - 4s 6ms/step\n",
      "[0.5063963309437239, 0.8217687124297732]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "\n",
    "# Grid Search for \"best\" model\n",
    "# embedding_dims = [256, 300]\n",
    "embedding_dims = [256]\n",
    "rnn_units = [128, 256]\n",
    "dropout_probs = [0.7, 0.8]\n",
    "using_biLSTM = False\n",
    "\n",
    "for e in embedding_dims:\n",
    "    for r in rnn_units:\n",
    "        for d in dropout_probs:\n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            print(\"e\", e)\n",
    "            print(\"r\", r)\n",
    "            print(\"d\", d)\n",
    "            \n",
    "            # test LSTM, BiLSTM\n",
    "            if using_biLSTM:\n",
    "                print('Using BiLSTM')\n",
    "                custom_rnn_model, callbacks_list = BiLSTM_model(x_train_.shape, vocab_size, e, r, d)\n",
    "            else:\n",
    "                print('Using LSTM')\n",
    "                custom_rnn_model, callbacks_list = LSTM_model(x_train_.shape, vocab_size, e, r, d)\n",
    "            print(custom_rnn_model.summary())\n",
    "            custom_rnn_model.fit(x_train_, y_train_, batch_size=400, epochs=10, validation_split=0.1, callbacks=callbacks_list,)\n",
    "            print(custom_rnn_model.metrics_names)\n",
    "            \n",
    "            custom_rnn_model.load_weights(\"weights.best.hdf5\")\n",
    "            custom_rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            result = custom_rnn_model.evaluate(x=x_test_, y=y_test_, batch_size=200)\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
