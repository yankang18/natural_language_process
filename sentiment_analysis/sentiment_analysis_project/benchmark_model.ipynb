{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train benchmark models\n",
    "* We choose Naive Bayes as the benchmark model\n",
    "* For comparison purpose, we will also train a SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file=None):\n",
    "    if data_file == None:\n",
    "        return\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/large_files/stanford_sentiment/parsed_data/'\n",
    "word2idx = load_data(folder + \"sentiment_word2idx.json\")\n",
    "sentiment_binary_train = load_data(folder + \"sentiment_binary_train.json\")\n",
    "sentiment_train = load_data(folder + \"sentiment_train.json\")\n",
    "sentiment_binary_test = load_data(folder + \"sentiment_binary_test.json\")\n",
    "sentiment_test = load_data(folder + \"sentiment_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude neutral samples\n",
    "\n",
    "* The loaded samples has three type of labels -1,0,1, in which -1 indicates neutral sentiment.\n",
    "* We exclude samples with neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: # of training samples and # of test samples\n",
      "# of traing samples:  6920\n",
      "# of test samples:  1821\n"
     ]
    }
   ],
   "source": [
    "def exclude_neutral_sample(samples:dict):\n",
    "    ssamples = {}\n",
    "    for k, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            ssamples[k] = v\n",
    "    return ssamples\n",
    "        \n",
    "train_b = exclude_neutral_sample(sentiment_binary_train)\n",
    "test_b = exclude_neutral_sample(sentiment_binary_test)\n",
    "\n",
    "print(\"After filtering: # of training samples and # of test samples\")\n",
    "print(\"# of traing samples: \", len(train_b))\n",
    "print(\"# of test samples: \", len(test_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert training/test data to sentences\n",
    "* Currently, the training/test data are in the form of integer sequences, which are directly parsed from Stanford sentimental analysis raw data. We have not done any preprocessing nor feature extracting on those data yet. \n",
    "* The purpose of coverting training/test data into sentences is that we will do some preprocessing and feature extracting on these sentences. Then, we will convert sentences back to integer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert review comment in form of integers to the form of words\n",
    "def get_comment(wordidx, idx2word:dict):\n",
    "    wordlist = []\n",
    "    for idx in wordidx:\n",
    "        if idx != -1:\n",
    "            token = idx2word[idx]\n",
    "            # remove punctuation\n",
    "            if token not in string.punctuation:\n",
    "                wordlist.append(token)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_samples(samples:dict, idx2word:dict):\n",
    "    comments = []\n",
    "    targets = []\n",
    "    for _, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            # concatenate word list to a string\n",
    "            comment = \" \".join(get_comment(v[0], idx2word))\n",
    "            label = v[3][-1]\n",
    "            comments.append(comment)\n",
    "            targets.append(label) \n",
    "    return comments, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "train_comments_o, train_targets = get_comments_samples(train_b, idx2word)\n",
    "test_comments_o, test_targets = get_comments_samples(test_b, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n",
      "1821\n",
      "6920\n",
      "1821\n"
     ]
    }
   ],
   "source": [
    "print(len(train_comments_o))\n",
    "print(len(test_comments_o))\n",
    "print(len(train_targets))\n",
    "print(len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resplit data\n",
    "* Since we will add more data to the training set for training Recurrent Neural Network and Recursive Neural Network, we want to do the same thing for training Naive Bayes. \n",
    "* However, After experiments, we found that with more training data, the accuray of Naive Bayes model on the test data set was decreased dramatically, from 0.811 to 0.689. Even the accuracy on training data set was decreased from 0.911 to 0.897 (need to further investigate the reason).\n",
    "* Therefore, we will just use 6920 examples for training and 1821 examples for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_comments_o  = train_comments_o + test_comments_o[:1000]\n",
    "# test_comments_o = test_comments_o[1000:]\n",
    "\n",
    "# train_targets = train_targets + test_targets[:1000]\n",
    "# test_targets = test_targets[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n",
      "1821\n",
      "6920\n",
      "1821\n"
     ]
    }
   ],
   "source": [
    "print(len(train_comments_o))\n",
    "print(len(test_comments_o))\n",
    "print(len(train_targets))\n",
    "print(len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train comments: 6920\n",
      "# of test comments: 6920\n",
      "total # of comments: 8741\n"
     ]
    }
   ],
   "source": [
    "all_comments = train_comments_o + test_comments_o\n",
    "\n",
    "print('# of train comments:', len(train_comments_o))\n",
    "print('# of test comments:', len(train_comments_o))\n",
    "print(\"total # of comments:\", len(all_comments))\n",
    "\n",
    "# Initialize a CoutVectorizer to use NLTK's tokenizer instead of its \n",
    "# default one (which ignores punctuation and stopwords). \n",
    "# Minimum document frequency set to 1. \n",
    "foovec = CountVectorizer(max_features=6000)\n",
    "# sentences turned into sparse vector of word frequency counts\n",
    "foovec = foovec.fit(train_comments_o)\n",
    "train_comments = foovec.transform(train_comments_o)\n",
    "test_comments = foovec.transform(test_comments_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 6000)\n",
      "(1821, 6000)\n"
     ]
    }
   ],
   "source": [
    "print(train_comments.shape)\n",
    "print(test_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 6000)\n",
      "(1821, 6000)\n",
      "(6920, 6000)\n",
      "(1821, 6000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(train_comments)\n",
    "X_test = tfidf_transformer.transform(test_comments)\n",
    "\n",
    "print(train_comments.shape)\n",
    "print(test_comments.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Naive Bayes and SVM \n",
    "\n",
    "### Train and validate Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test data set: 0.8116419549697969\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results, find accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "print('accuracy on test data set:', accuracy_score(test_targets, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data set: 0.911271676300578\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = clf.predict(X_train)\n",
    "print('accuracy on training data set:',accuracy_score(train_targets, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[699, 213],\n",
       "       [130, 779]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(test_targets, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test data set: 0.8088962108731467\n"
     ]
    }
   ],
   "source": [
    "classifier_rbf = SVC(kernel='linear').fit(X_train, train_targets)\n",
    "y_pred = classifier_rbf.predict(X_test)\n",
    "print('accuracy on test data set:', accuracy_score(test_targets, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[726, 186],\n",
       "       [162, 747]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_targets, y_pred)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
