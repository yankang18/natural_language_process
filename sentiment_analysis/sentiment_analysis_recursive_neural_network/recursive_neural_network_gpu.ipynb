{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import string\n",
    "# import matplotlib.pyplot as plt\n",
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveNN(object):\n",
    "    def __init__(self, V, D, K):\n",
    "        self.V = V\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        \n",
    "    def fit(self, trees, learning_rate=1e-3, reg=1e-2, mu=0.99, eps=1e-2, decay_rate=0.999, epochs=20, \n",
    "            activation=T.nnet.relu, train_inner_nodes=True):\n",
    "        \n",
    "        print(\"learning rate:\", learning_rate)\n",
    "        print(\"regularization:\", reg)\n",
    "        print(\"mu:\", mu)\n",
    "        print(\"eps:\", eps)\n",
    "        print(\"epochs:\", epochs)\n",
    "        print(\"decay_rate:\", decay_rate)\n",
    "        print(\"dim:\", self.D)\n",
    "        print(\"activation: \", type(activation))\n",
    "        print(\"train_inner_nodes: \", train_inner_nodes)\n",
    "        \n",
    "        V = self.V\n",
    "        D = self.D\n",
    "        K = self.K\n",
    "        R = 2\n",
    "        self.f = activation\n",
    "        N = len(trees)\n",
    "        \n",
    "        ### initialize weights\n",
    "        \n",
    "        We = init_weights(V, D)\n",
    "        \n",
    "        Wh = np.random.randn(R, D, D) / np.sqrt(R + D + D)\n",
    "        bh = np.zeros(D)\n",
    "        \n",
    "        Wo = init_weights(D, K)\n",
    "        bo = np.zeros(K)\n",
    "        \n",
    "        self.We = theano.shared(We.astype(np.float32), 'We')\n",
    "        self.Wh = theano.shared(Wh.astype(np.float32), 'Wh')\n",
    "        self.bh = theano.shared(bh.astype(np.float32), 'bh')\n",
    "        self.Wo = theano.shared(Wo.astype(np.float32), 'Wo')\n",
    "        self.bo = theano.shared(bo.astype(np.float32), 'bo')\n",
    "        self.params = [self.We, self.Wh, self.bh, self.Wo, self.bo]\n",
    "        \n",
    "        \n",
    "        ### symbolic expression for forward propagation\n",
    "            \n",
    "        # create input training vectors\n",
    "#         words = T.ivector('words')\n",
    "#         parents = T.ivector('parents')\n",
    "#         relations = T.ivector('relations')\n",
    "#         labels = T.ivector('labels')\n",
    "\n",
    "        words = T.fvector('words')\n",
    "        parents = T.fvector('parents')\n",
    "        relations = T.fvector('relations')\n",
    "        labels = T.fvector('labels')\n",
    "        \n",
    "        def recurrence(n, hiddens, words, parents, relations):\n",
    "          \n",
    "            w = words[n]\n",
    "        \n",
    "            # update hidden matrix for current node\n",
    "            hiddens = T.switch(\n",
    "                T.ge(w, 0),\n",
    "                T.set_subtensor(hiddens[n], self.We[w]),\n",
    "                T.set_subtensor(hiddens[n], self.f(hiddens[n] + self.bh)),\n",
    "            )\n",
    "            \n",
    "            # update hidden matrix for parent node\n",
    "            p = parents[n]\n",
    "            r = relations[n]\n",
    "            hiddens = T.switch(\n",
    "                T.ge(p, 0),\n",
    "                T.set_subtensor(hiddens[p], hiddens[p] + hiddens[n].dot(self.Wh[r])),\n",
    "                hiddens,\n",
    "            )\n",
    "            \n",
    "            return hiddens\n",
    "        \n",
    "        # initialize hidden matrix\n",
    "        # note that each row of the hidden matrix represents a node in the original parse tree\n",
    "        # it can be leave node containing word or inner node\n",
    "        hiddens = T.zeros((words.shape[0], D))\n",
    "        \n",
    "        h, _ = theano.scan(\n",
    "            fn=recurrence,\n",
    "            sequences=T.arange(words.shape[0]),\n",
    "            n_steps=words.shape[0],\n",
    "            outputs_info=[hiddens],\n",
    "            non_sequences=[words, parents, relations],\n",
    "        )\n",
    "        \n",
    "        # note we use T.arange not python's range and use T.zeros not np.zeros below\n",
    "        \n",
    "        # symbolic expression of the output probablility distribution\n",
    "        py_x = T.nnet.softmax(h[-1].dot(self.Wo) + self.bo)\n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "        \n",
    "        ### symbolic expression for back propagation \n",
    "        \n",
    "        # regularization cost\n",
    "        rcost = reg*T.mean([(p*p).sum() for p in self.params])\n",
    "        \n",
    "        if train_inner_nodes:\n",
    "            # won't work for binary classification\n",
    "            xentropy = -T.mean(T.log(py_x[T.arange(labels.shape[0]), labels]))\n",
    "            acost = xentropy + rcost\n",
    "        else:\n",
    "            xentropy = -T.mean(T.log(py_x[-1, labels[-1]]))\n",
    "            acost = xentropy + rcost\n",
    "        \n",
    "        grads = T.grad(acost, self.params)\n",
    "        \n",
    "        # momentum\n",
    "#         dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "#         updates = [\n",
    "#             (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "#         ] + [\n",
    "#             (dp, mu*dp - learning_rate*g) for dp, g in zip(dparams, grads)\n",
    "#         ]\n",
    "        \n",
    "        # AdaGrad\n",
    "        cache = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        updates = [\n",
    "            (c, c + g*g) for c, g in zip(cache, grads)\n",
    "        ] + [\n",
    "            (p, p - learning_rate*g / (T.sqrt(c) + eps)) for p, c, g in zip(self.params, cache, grads)\n",
    "        ]\n",
    "        \n",
    "        # RMSprop (does not work! why???)\n",
    "#         cache = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "#         updates = [\n",
    "#             (c, decay_rate * c + (1 - decay_rate)*g*g) for c, g in zip(cache, grads)\n",
    "#         ] + [\n",
    "#             (p, p - learning_rate*g / (T.sqrt(c) + eps)) for p, c, g in zip(self.params, cache, grads)\n",
    "#         ]\n",
    "\n",
    "\n",
    "        # RMSprop + momentum\n",
    "#         dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "#         cache = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "#         updates = [\n",
    "#             (c, decay_rate * c + (1 - decay_rate)*g*g) for c, g in zip(cache, grads)\n",
    "#         ] + [\n",
    "#             (dp, mu*dp - learning_rate*g / (T.sqrt(c) + eps)) for dp, c, g in zip(dparams, cache, grads)\n",
    "#         ] + [\n",
    "#             (p, p + dp) for p, dp in zip(self.params, dparams)\n",
    "#         ]\n",
    "        \n",
    "        \n",
    "        self.cost_predict_op = theano.function(\n",
    "            inputs = [words, parents, relations, labels],\n",
    "            outputs = [acost, prediction],\n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "        \n",
    "        self.train_op = theano.function(\n",
    "            inputs = [words, parents, relations, labels],\n",
    "            outputs = [xentropy, acost, prediction],\n",
    "            updates=updates,\n",
    "            allow_input_downcast=True,\n",
    "        ) \n",
    "        \n",
    "        ### start training\n",
    "        print(\"start training...\")\n",
    "        \n",
    "        costs = []\n",
    "        xents = []\n",
    "        sequence_indexes = range(N)\n",
    "        if train_inner_nodes:\n",
    "            n_total = sum(len(words) for words, _, _, _ in trees)\n",
    "        else:\n",
    "            n_total = N\n",
    "            \n",
    "        for i in range(epochs):\n",
    "            t0 = datetime.now()\n",
    "            sequence_indexes = shuffle(sequence_indexes)\n",
    "            n_correct = 0\n",
    "            cost = 0\n",
    "            xent = 0\n",
    "            it = 0 # iteration count\n",
    "            for j in sequence_indexes:\n",
    "                words, parents, relations, labels = trees[j]\n",
    "                xe, c, p = self.train_op(words, parents, relations, labels)\n",
    "                \n",
    "                if np.isnan(c):\n",
    "                    print(\"Cost is nan! Let's stop here. Why don't you try decreasing the learning rate?\")\n",
    "                    exit()\n",
    "                    \n",
    "                cost += c\n",
    "                xent += xe\n",
    "                if train_inner_nodes:\n",
    "                    n_correct += np.sum(p == labels)\n",
    "                else:\n",
    "                    n_correct += (p[-1] == labels[-1])\n",
    "                it+=1\n",
    "                if it % 1 == 0:\n",
    "                    sys.stdout.write(\"epoch: %d, j/N: %d/%d correct rate so far: %f, cost so far: %f, xent so far: %f\\r\" % (i, it, N, float(n_correct)/n_total, cost, xent))\n",
    "                    sys.stdout.flush()\n",
    "            \n",
    "            print(\"i:\", i, \"cost:\", cost, \"xent\", xent, \"correct rate:\", (float(n_correct)/n_total), \"time for epoch:\", (datetime.now() - t0))\n",
    "            costs.append(cost)\n",
    "            xents.append(xent)\n",
    "            \n",
    "#         plt.plot(costs)\n",
    "#         plt.title(\"cost\")\n",
    "#         plt.show()\n",
    "        \n",
    "#         plt.plot(xents)\n",
    "#         plt.title(\"cross entropy\")\n",
    "#         plt.show()\n",
    "        \n",
    "    def score(self, trees):\n",
    "        n_total = len(trees)\n",
    "        n_correct = 0\n",
    "        for words, parents, relations, labels in trees:\n",
    "            _, p = self.cost_predict_op(words, parents, relations, labels)\n",
    "            n_correct += (p[-1] == labels[-1])\n",
    "        print(\"n_correct:\", n_correct, \"n_total:\", n_total)\n",
    "        return float(n_correct) / n_total\n",
    "    \n",
    "    def f1_score(self, trees):\n",
    "        Y = []\n",
    "        P = []\n",
    "        for words, left, right, lab in trees:\n",
    "            _, p = self.cost_predict_op(words, left, right, lab)\n",
    "            Y.append(lab[-1])\n",
    "            P.append(p[-1])\n",
    "        return f1_score(Y, P, average=None).mean()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file=None):\n",
    "    if data_file == None:\n",
    "        return\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n",
      "2210\n",
      "Load data finished\n"
     ]
    }
   ],
   "source": [
    "folder = './data/large_files/stanford_sentiment/parsed_data/'\n",
    "word2idx = load_data(folder + \"sentiment_word2idx.json\")\n",
    "sentiment_binary_train = load_data(folder + \"sentiment_binary_train.json\")\n",
    "sentiment_train = load_data(folder + \"sentiment_train.json\")\n",
    "sentiment_binary_test = load_data(folder + \"sentiment_binary_test.json\")\n",
    "sentiment_test = load_data(folder + \"sentiment_test.json\")\n",
    "\n",
    "print(len(sentiment_binary_train))\n",
    "print(len(sentiment_binary_test))\n",
    "print(\"Load data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train, test, word2idx, dim=10, is_binary=True, learning_rate=1e-2, reg=1e-2, \n",
    "         mu=0, eps=1e-2, activation=T.tanh, epochs=30, train_inner_nodes=False):\n",
    "    \n",
    "    print(\"total train size before filtering:\", len(train))\n",
    "        \n",
    "    if is_binary:\n",
    "        train = [t for t in train if t[3][-1] >=0]\n",
    "        test = [t for t in test if t[3][-1] >=0]\n",
    "    \n",
    "    print(\"total train size after filtering:\", len(train))\n",
    "    \n",
    "    train = shuffle(train)\n",
    "#     train = train[:5000]\n",
    "    \n",
    "    test = shuffle(test)\n",
    "#     test = test[:1000]\n",
    "    \n",
    "    print(\"train size:\", len(train))\n",
    "    print(\"test size:\", len(test))   \n",
    "    \n",
    "    V = len(word2idx)\n",
    "    print(\"vocab size:\", V)\n",
    "    D = dim\n",
    "    K = 2 if is_binary else 5\n",
    "    \n",
    "    model = RecursiveNN(V, D, K)\n",
    "    model.fit(train, learning_rate=learning_rate, reg=reg, mu=mu, eps=eps, epochs=epochs, activation=activation, train_inner_nodes=train_inner_nodes)\n",
    "    print (\"train accuracy:\", model.score(train))\n",
    "    print (\"test accuracy:\", model.score(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n",
      "2210\n",
      "preprocess data finished\n"
     ]
    }
   ],
   "source": [
    "train = list(sentiment_binary_train.values()) \n",
    "test = list(sentiment_binary_test.values()) \n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(\"preprocess data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
