{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Tree for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two ways so far of representing sentences:\n",
    "    1. bag of words\n",
    "    2. sequence of words\n",
    "\n",
    "\n",
    "* The third way would be: parse trees (with more structural information)\n",
    "\n",
    "[example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Parse Trees\n",
    "    * Relations\n",
    "        * e.g., left child and right child\n",
    "    * A node can have any number of children\n",
    "        * We will use binary tree such that a node either has no child or it has two children\n",
    "    * With recurrent NNs, we did next word prediction $P(x(t)| x(t-1),...,x(0))$. In parse trees, there is no such thing as \"next\" since it is a hierarchical structure and not a sequence\n",
    "        * We will transform parse trees to sequences such that we can use recurrent neural network to train these parse trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentiment analysis\n",
    "    * bag-of-words representing a sentence can not handle negation.\n",
    "    * Recursive neural nets solve this problem\n",
    "        * Best prior to recursive neural nets: ~80%\n",
    "        * Recursive neural nets can achieve 85% - 90%\n",
    "    * Why recursive neural nets work on sentiment analysis\n",
    "        * Parse tree is structured to identify dependencies between words. \n",
    "            * If you have negation, then it is very easy to reverse whatever is the phrase that it is negating\n",
    "        * The neural network can use them to do negation\n",
    "        * We need to label every node in the tree\n",
    "            * labeling sentences with parse tree structure is labor-tensive. This is one of the reasons why progress of adopting recurive neural network is hard compared to other neural network architecture. \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parse Tree Structure in Sentiment analysis\n",
    "    * It has basic structure of a parse tree as we introduced above\n",
    "    * only leaves of the tree represent words\n",
    "    * Any inner nodes represents phrase made up of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data \n",
    "    * Get data from [Stanford Sentiment Analysis](http://nlp.stanford.edu/sentiment)\n",
    "    * Look for file: <b>trainDevTestTrees_PTB.zip</b>\n",
    "    * Each node is labeled using Amazon Mechanicl Turk, and have rating: 1-5, in which 3 is neural, 1 is the most negative and 5 is the most positive\n",
    "    * A sentence has the format of: (5 (5 great) (3 movie))\n",
    "    * Represent it in tree structure:\n",
    "    \n",
    "    [example]\n",
    "    \n",
    "    * We need to transform each sentence into a sequence that embodies the parse tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Neural Network for Parse Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recursiveness: A node's value is defined by its children, and in turn children's values are defined by their children respectively, and so on so forth...\n",
    "* We need weights (i.e., weight matrix) one each edge between a parent node and one of its child to tell how this child connects to the parent.\n",
    "    * Since we use binary tree, we only neet two weights for each node (i.e., inner node): one for left child and one for right child\n",
    "    > Note that although we may have multiple nodes that each has its own children, we use the same left weight matrix for all nodes to connect their left child, and the same right matrix for all nodes to connect their right child\n",
    "    \n",
    "    [picture]\n",
    "    \n",
    "    $$ x_1 = We_{1} = \\text{word embeddings for word 1} $$\n",
    "    \n",
    "    $$ x_2 = We_{2} = \\text{word embeddings for word 2} $$\n",
    "    \n",
    "    $$ h_1 = f(W_{left} x_1 + W_{right} x_2 + b) $$\n",
    "    \n",
    "    $$ h_{root} = f(W_{left} h_1 + W_{right} x_3 + b) $$\n",
    "    \n",
    "where $f()$ can be any of the activation functions: relu, tanh and sigmoid.\n",
    "\n",
    "* What is the dimensionality of the weights?\n",
    "    * Since we use the same set of left and right weight matrix for all inner nodes, the shape of the weigth matrix shoule be $(D, D)$, where $D$ is the pre-defined dimension.\n",
    "    * The bias $b$ should be a $D$ dimention vector.\n",
    "  \n",
    "* How to get the output?\n",
    "* Any node can be labeled (we can choose whether or not to use inner nodes for training)\n",
    "\n",
    "$$ p(y | h) = softmax(W_o h + b_o) $$\n",
    "\n",
    "where $h$ can be root, inner node or leave node (i.e., word); $W_o$ is the weight matrix for the output and $b_o$ is the bias for the output.\n",
    "\n",
    "[picture]\n",
    "\n",
    "**N-ary Trees**\n",
    "\n",
    "* Let us first define value for a node of three children:\n",
    "\n",
    "$$ h = f(W_{1} x_1 + W_{2} x_2 + W_{3} x_2 + b) $$\n",
    "\n",
    "* We can extend this to general case:\n",
    "\n",
    "$$ h = f(\\sum_{i} W_{rel(h, i)} x_{i} + b) $$\n",
    "\n",
    "* We can define the weight matrix W as a matrix with shape $(R, D, D)$, where $R$ is the dimension for all possible parent-child relationships.\n",
    "* function $rel(h, i)$ returns a number form 1 to R telling us the type of relationship between parent $h$ and child $i$.\n",
    "* You can think of binary tree as $R=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive NNs to Recurrent NNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We know that Recursive NNs takes trees as input, while Recurrent NNs takes sequences as input\n",
    "    * We convert trees to sequences\n",
    "* How to do the conversion?\n",
    "* Arrange the parse tree in a list such that children always come before parents\n",
    "    * We can do this using post-order traversal algorithm\n",
    "    * While we are traversing the parse tree in post order, we create three arrays/lists: parent array, relation array and word array.\n",
    "        * parent array: index represents nodes' ID, value represents index of parent. If a node has no parent, the value would be -1\n",
    "        * relation array: index represents nodes' ID, value represents index of relation to parent. If a node has no parent, the value would be -1\n",
    "        * word array:  index represents nodes' ID, value represents index of word. If a node has no word, the value would be -1\n",
    "\n",
    "[three or more pictures]\n",
    "\n",
    "* Summary\n",
    "    * Three separate arrays to store a tree\n",
    "    * parents/relations/words\n",
    "    * Using post-order traversal to create the three arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to write code in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theano scan**\n",
    "\n",
    "```python\n",
    "# initialize the hidden matrix that each row represents a node in the original parse tree\n",
    "hiddens = T.zeros((words.shape[0], D)) \n",
    "h, _ = theano.scan(\n",
    "    fn=recurrence,\n",
    "    squences=range(n)\n",
    "    outputs_info=[hiddens],\n",
    "    non_sequences=[parents, relations, words]\n",
    ")\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrence**\n",
    "\n",
    "* Two things we want to do\n",
    "\n",
    "(1) calculate value at hidden node, n\n",
    "\n",
    "```python\n",
    "if words[n] >= 0:\n",
    "    hiddens[n] = We[words[n]]\n",
    "else:\n",
    "    hiddens = T.set_subtensor(hiddens[n], f(hiddens[n] + bh))\n",
    "```\n",
    "\n",
    "> Note that hiddens[n] already contains values from its children: $W_{left} x_1 + W_{right} x_2$. This is because we add $W_{left} x_1$ to hidden[n] when we are processing left child of node n and add $ W_{right} x_2$ to hidden[n] when we are processing right child of node n, and both left and right children are processd before the processing of node n.\n",
    "\n",
    "(2) update parent if exists\n",
    "\n",
    "```python\n",
    "p = parents[n]\n",
    "r = relation[n]\n",
    "if p > 0:\n",
    "    hiddens = T.set_subtensor(hiddens[p], hiddens[p] + hiddens[n].dot(Wh[r]))\n",
    "```\n",
    "\n",
    "> If current node n has a parent, this node can be either left child or right child, and the relation is defined by $r = relation[n]$. \n",
    "\n",
    "> $Wh$ is a $(R, D, D)$ matrix, of which the first dimension is indexes of relation. For binary three, 0 represents left child while 1 represents right child.\n",
    "\n",
    "\n",
    "** Use if statement in Theano **\n",
    "\n",
    "* Theano must create a graph and thus it can not contains arbitrary Python code.\n",
    "    * e.g., we use theano.scan() to perform the functionality of for loop\n",
    "* In theano we use <b style=\"color:red\">T.switch</b> to perform functionality of if statement\n",
    "* The API for T.swich:\n",
    "\n",
    "```\n",
    "new_value = T.switch(condition, value if condition is true, value if condition is false)\n",
    "```\n",
    "\n",
    "* Use T.switch for (1):\n",
    "\n",
    "```python\n",
    "hiddens = T.switch(\n",
    "    T.ge(words[n], 0),\n",
    "    We[words[n]],\n",
    "    T.set_subtensor(hiddens[n], f(hiddens[n] + hb),\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* [Stanford Sentiment Analysis](https://nlp.stanford.edu/sentiment/)\n",
    "* [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "* [Parsing With Compositional Vector Grammars](https://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
