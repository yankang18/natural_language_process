{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN for sentiment analysis classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Activation\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate, BatchNormalization\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/tweets.csv', encoding='latin1', usecols=['Sentiment', 'SentimentText'])\n",
    "# data.columns = ['sentiment', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['sentiment', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578614, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r\"#(\\w+)\",'', tweet)\n",
    "    tweet = re.sub(r\"@(\\w+)\", '', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = tweet.strip().lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 1578614/1578614 [05:00<00:00, 5247.16it/s]\n"
     ]
    }
   ],
   "source": [
    "data['tokens'] = data['text'].progress_map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[is, so, sad, for, my, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[i, missed, the, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, its, already, 730, o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, been, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me, t_t]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                              tokens  \n",
       "0                [is, so, sad, for, my, apl, friend]  \n",
       "1               [i, missed, the, new, moon, trailer]  \n",
       "2                        [omg, its, already, 730, o]  \n",
       "3  [omgaga, im, sooo, im, gunna, cry, ive, been, ...  \n",
       "4      [i, think, mi, bf, is, cheating, on, me, t_t]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 1578614/1578614 [00:05<00:00, 314056.67it/s]\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['tokens'].progress_map(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[is, so, sad, for, my, apl, friend]</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[i, missed, the, new, moon, trailer]</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, its, already, 730, o]</td>\n",
       "      <td>omg its already 730 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, been, ...</td>\n",
       "      <td>omgaga im sooo im gunna cry ive been at this d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me, t_t]</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                [is, so, sad, for, my, apl, friend]   \n",
       "1               [i, missed, the, new, moon, trailer]   \n",
       "2                        [omg, its, already, 730, o]   \n",
       "3  [omgaga, im, sooo, im, gunna, cry, ive, been, ...   \n",
       "4      [i, think, mi, bf, is, cheating, on, me, t_t]   \n",
       "\n",
       "                                          clean_text  \n",
       "0                        is so sad for my apl friend  \n",
       "1                      i missed the new moon trailer  \n",
       "2                              omg its already 730 o  \n",
       "3  omgaga im sooo im gunna cry ive been at this d...  \n",
       "4                i think mi bf is cheating on me t_t  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['sentiment', 'clean_text']].to_csv('./data/cleaned_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578614, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/cleaned_text.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 730 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry ive been at this d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentiment                                         clean_text\n",
       "0           0          0                        is so sad for my apl friend\n",
       "1           1          0                      i missed the new moon trailer\n",
       "2           2          1                              omg its already 730 o\n",
       "3           3          0  omgaga im sooo im gunna cry ive been at this d...\n",
       "4           4          0                i think mi bf is cheating on me t_t"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 730 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry ive been at this d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>juuuuuuuuuuuuuuuuussssst chillin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>sunny again work tomorrow tv tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today i miss you already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm i wonder how she my number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>i must think about positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>thanks to all the haters up in my face all day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>jb isnt showing in australia any more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ok thats it you win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>lt this is the way i feel right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>awhhe man im completely useless rt now funny a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>feeling strangely fine now im gon na go listen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>huge roll of thunder just nowso scary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>i just cut my beard off its only been growing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad about iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>wompppp wompp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>youre the only one who can see this cause no o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>ltsad level is 3 i was writing a massive blog ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>headed to hospitol had to pull out of the golf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>boring whats wrong with him please tell me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>cant be bothered i wish i could spend the rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>feeeling like shit right now i really want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>goodbye exams hello alcohol tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>i didnt realize it was that deep geez give a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578584</th>\n",
       "      <td>1</td>\n",
       "      <td>zoo was rad today feeling tired and not motiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578585</th>\n",
       "      <td>1</td>\n",
       "      <td>zoo with the woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578586</th>\n",
       "      <td>0</td>\n",
       "      <td>zoolander and alice in wonderland i have a kil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578587</th>\n",
       "      <td>1</td>\n",
       "      <td>zoom zoom back to bristol today i have my bear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578588</th>\n",
       "      <td>0</td>\n",
       "      <td>zootm can not survive without crlf support wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578589</th>\n",
       "      <td>0</td>\n",
       "      <td>zoran lost croatian idol the difference was le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578590</th>\n",
       "      <td>0</td>\n",
       "      <td>zork buggy beta version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578591</th>\n",
       "      <td>1</td>\n",
       "      <td>zow finished uploading pictures on flickr and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578592</th>\n",
       "      <td>1</td>\n",
       "      <td>zrock was awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578593</th>\n",
       "      <td>1</td>\n",
       "      <td>ztecwiz bought mirc for 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578594</th>\n",
       "      <td>1</td>\n",
       "      <td>zu spãât by die ãârzte one of the best bands ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578595</th>\n",
       "      <td>1</td>\n",
       "      <td>zuma bitch tomorrow have a wonderful night eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578596</th>\n",
       "      <td>0</td>\n",
       "      <td>zummies couch tour was amazingto bad i had to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578597</th>\n",
       "      <td>0</td>\n",
       "      <td>zunehd looks great oled screen hdmi only issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578598</th>\n",
       "      <td>1</td>\n",
       "      <td>zup there learning a new magic trick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578599</th>\n",
       "      <td>1</td>\n",
       "      <td>zyklonic showers evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578600</th>\n",
       "      <td>1</td>\n",
       "      <td>zz top ãââ i thank you thanks for your music a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578601</th>\n",
       "      <td>0</td>\n",
       "      <td>zzz time just wish my love could b nxt 2 me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578602</th>\n",
       "      <td>1</td>\n",
       "      <td>zzz twitter good day today got a lot accomplis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578603</th>\n",
       "      <td>1</td>\n",
       "      <td>zzzs time goodnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578604</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzz lying in bed watching the countryside thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578605</th>\n",
       "      <td>1</td>\n",
       "      <td>zzzz fuck ãâ¼ zzzz fuck ãâ¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578606</th>\n",
       "      <td>1</td>\n",
       "      <td>zzzzno work tomorrowyayyy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578607</th>\n",
       "      <td>1</td>\n",
       "      <td>zzzzz time tomorrow will be a busy day for ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578608</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzz want to sleep but at sisters inlawss house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578609</th>\n",
       "      <td>1</td>\n",
       "      <td>zzzzzz finally night tweeters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578610</th>\n",
       "      <td>1</td>\n",
       "      <td>zzzzzzz sleep well people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578611</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzzzz wait no i have homework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578612</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzzzzzzz meh what am i doing up again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578613</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzz i wish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1578614 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                         clean_text\n",
       "0                0                        is so sad for my apl friend\n",
       "1                0                      i missed the new moon trailer\n",
       "2                1                              omg its already 730 o\n",
       "3                0  omgaga im sooo im gunna cry ive been at this d...\n",
       "4                0                i think mi bf is cheating on me t_t\n",
       "5                0                           or i just worry too much\n",
       "6                1                   juuuuuuuuuuuuuuuuussssst chillin\n",
       "7                0               sunny again work tomorrow tv tonight\n",
       "8                1      handed in my uniform today i miss you already\n",
       "9                1                   hmmmm i wonder how she my number\n",
       "10               0                        i must think about positive\n",
       "11               1  thanks to all the haters up in my face all day...\n",
       "12               0                     this weekend has sucked so far\n",
       "13               0              jb isnt showing in australia any more\n",
       "14               0                                ok thats it you win\n",
       "15               0                lt this is the way i feel right now\n",
       "16               0  awhhe man im completely useless rt now funny a...\n",
       "17               1  feeling strangely fine now im gon na go listen...\n",
       "18               0              huge roll of thunder just nowso scary\n",
       "19               0  i just cut my beard off its only been growing ...\n",
       "20               0                                very sad about iran\n",
       "21               0                                      wompppp wompp\n",
       "22               1  youre the only one who can see this cause no o...\n",
       "23               0  ltsad level is 3 i was writing a massive blog ...\n",
       "24               0  headed to hospitol had to pull out of the golf...\n",
       "25               0         boring whats wrong with him please tell me\n",
       "26               0  cant be bothered i wish i could spend the rest...\n",
       "27               0  feeeling like shit right now i really want to ...\n",
       "28               1                goodbye exams hello alcohol tonight\n",
       "29               0  i didnt realize it was that deep geez give a g...\n",
       "...            ...                                                ...\n",
       "1578584          1  zoo was rad today feeling tired and not motiva...\n",
       "1578585          1                                 zoo with the woman\n",
       "1578586          0  zoolander and alice in wonderland i have a kil...\n",
       "1578587          1  zoom zoom back to bristol today i have my bear...\n",
       "1578588          0    zootm can not survive without crlf support wait\n",
       "1578589          0  zoran lost croatian idol the difference was le...\n",
       "1578590          0                            zork buggy beta version\n",
       "1578591          1  zow finished uploading pictures on flickr and ...\n",
       "1578592          1                                  zrock was awesome\n",
       "1578593          1                         ztecwiz bought mirc for 10\n",
       "1578594          1  zu spãât by die ãârzte one of the best bands ever\n",
       "1578595          1  zuma bitch tomorrow have a wonderful night eve...\n",
       "1578596          0  zummies couch tour was amazingto bad i had to ...\n",
       "1578597          0  zunehd looks great oled screen hdmi only issue...\n",
       "1578598          1               zup there learning a new magic trick\n",
       "1578599          1                              zyklonic showers evil\n",
       "1578600          1  zz top ãââ i thank you thanks for your music a...\n",
       "1578601          0        zzz time just wish my love could b nxt 2 me\n",
       "1578602          1  zzz twitter good day today got a lot accomplis...\n",
       "1578603          1                                zzzs time goodnight\n",
       "1578604          0  zzzz lying in bed watching the countryside thr...\n",
       "1578605          1                        zzzz fuck ãâ¼ zzzz fuck ãâ¼\n",
       "1578606          1                          zzzzno work tomorrowyayyy\n",
       "1578607          1  zzzzz time tomorrow will be a busy day for ser...\n",
       "1578608          0   zzzzz want to sleep but at sisters inlawss house\n",
       "1578609          1                      zzzzzz finally night tweeters\n",
       "1578610          1                          zzzzzzz sleep well people\n",
       "1578611          0                 zzzzzzzzzz wait no i have homework\n",
       "1578612          0         zzzzzzzzzzzzz meh what am i doing up again\n",
       "1578613          0                         zzzzzzzzzzzzzzzzzzz i wish\n",
       "\n",
       "[1578614 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['clean_text'].map(str).values\n",
    "labels = data['sentiment'].map(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['is so sad for my apl friend', 'i missed the new moon trailer',\n",
       "       'omg its already 730 o', ..., 'zzzzzzzzzz wait no i have homework',\n",
       "       'zzzzzzzzzzzzz meh what am i doing up again',\n",
       "       'zzzzzzzzzzzzzzzzzzz i wish'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the index starts from 1 instead of 0\n",
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {index:word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444270\n",
      "444270\n"
     ]
    }
   ],
   "source": [
    "print(len(word2index))\n",
    "print(len(index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578614\n",
      "1578614\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = sequences[:1400000]\n",
    "train_labels = labels[:1400000]\n",
    "\n",
    "test_seq = sequences[1400000:]\n",
    "test_labels = labels[1400000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400000\n",
      "1400000\n",
      "178614\n",
      "178614\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seq))\n",
    "print(len(train_labels))\n",
    "print(len(test_seq))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_seq = pad_sequences(train_seq, max_seq_length)\n",
    "padded_test_seq = pad_sequences(test_seq, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loas Pretrained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vector(file_name):\n",
    "    word2vector = {}\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype=np.float32)\n",
    "            word2vector[word] = vec\n",
    "    return word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 50\n",
    "# word2vec_file_path = 'data/glove.6B.50d.txt'\n",
    "word2vec_file_path = \"data/glove.6B.{}d.txt\".format(emb_dim)\n",
    "word2vec = get_word2vector(word2vec_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Word Embedding Matrix\n",
    "\n",
    "* We first calculate mean and standard deviation for each dimension of the pretrained word embedding.\n",
    "* We then initialize the word embedding matrix with values from normal distribution with the calculated means and standard deviations\n",
    "* Finally, we create the word embedding matrix based on the pretrained word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "values = list(word2vec.values())\n",
    "all_embs = np.stack(values)\n",
    "\n",
    "print(all_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02094049\n",
      "0.6441044\n"
     ]
    }
   ],
   "source": [
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "print(emb_mean)\n",
    "print(emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30001, 50)\n"
     ]
    }
   ],
   "source": [
    "max_word = min(len(word2index) + 1, max_vocab_size + 1)\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_word, emb_dim))\n",
    "for word, idx in word2index.items():\n",
    "    if idx <= max_vocab_size:\n",
    "        vec = word2vec.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Construct Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "        input_dim = embedding_matrix.shape[0],\n",
    "        output_dim = embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_seq_length,\n",
    "        trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Construct RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_MODEL(hidden_state_dim, max_seq_length):\n",
    "    \n",
    "    input_ = Input(shape=(max_seq_length,))\n",
    "    x = embedding_layer(input_)\n",
    "#     x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(hidden_state_dim, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "    avg_pooling = GlobalAveragePooling1D()(x)\n",
    "    max_pooling = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pooling, max_pooling])\n",
    "    output = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 35, 50)       1500050     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 35, 256)      137472      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 256)          0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 256)          0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,638,035\n",
      "Trainable params: 137,985\n",
      "Non-trainable params: 1,500,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_MODEL(hidden_state_dim, max_seq_length)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rnn_model, \n",
    "           to_file='./model_images/rnn_model.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./model_images/rnn_model.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_1 =\"./models/rnn_embeddings/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint_1 = ModelCheckpoint(filepath_1, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "history_rnn = rnn_model.fit(x=padded_train_seq, y=train_labels, validation_split=0.1, \n",
    "                        batch_size=batch_size, epochs=epochs, callbacks=[checkpoint_1], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rnn.history['loss'], label='loss')\n",
    "plt.plot(history_rnn.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Construct TextCNN\n",
    "\n",
    "The core idea of TextCNN is using convolutional neural network with different filters to extract features from text and then combine those features to perform the classification task. \n",
    "\n",
    "To perform the convolution task, we can use either 1-dimentional convolution or 2-dimentional convolution. They are conceptually different, but computationally they are the same (We will see later). If you are using Keras to build the model, you can read [Keras中Conv1D和Conv2D的区别](https://blog.csdn.net/hahajinbu/article/details/79535172) for details (Note, this article maybe not accurate, but you can get the idea.). \n",
    "\n",
    "To explain the conceputal difference between Conv-1D and Conv-2D, let's consider a sentence with 7 tokens that each token is embedded as a vector with dimention of 5. That is, the shape of the sentence is $7\\times5$.\n",
    "\n",
    "If we are using Conv-1D, we treat the input sentence with shape (7, 5) as one dimensional data with 5 channels. We will define rank-1 filter with 5 channels to perform the convolution computation over that one dimension (Note, we did not consider the dimenstion for batch size here). Following picture depicts the 1-dimentional computation:\n",
    "\n",
    "<img src='images/1-dim-computation.png' style='height:400px;'>\n",
    "\n",
    "If we are using Conv-2D, we treat the input sentence with shape (7, 5) as two dimensional data with 1 channel. We will define rank-2 filter with 1 channels to perform the convolution computation over that two dimension (Note, we did not consider the dimenstion for batch size here). Following picture depicts the 2-dimentional computation:\n",
    "\n",
    "<img src='images/2-dim-computation.png' style='height:400px;'>\n",
    "\n",
    "In this section, we will build two TextCNN models, one for 1-dimentional convolution and the other for 2-dimentional convolution.\n",
    "\n",
    "\n",
    "### 9.1 TextCNN version 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEXT_CNN_MODEL_v1(filter_sizes, max_seq_length, num_out_filters, drop_rate):\n",
    "    \n",
    "    input_ = Input(shape=(max_seq_length,))\n",
    "    x = embedding_layer(input_)\n",
    "\n",
    "    pools = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        conv = Conv1D(filters=num_out_filters, kernel_size=filter_size, strides=1, padding='valid')(x)\n",
    "        conv = BatchNormalization(axis=-1)(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        pool = MaxPooling1D(pool_size=max_seq_length-filter_size+1, strides=1, padding='valid')(conv)\n",
    "        pools.append(pool)\n",
    "    concatenated_tensor = Concatenate(axis=1)(pools)\n",
    "    \n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop_rate)(flatten)\n",
    "    output = Dense(units=1, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 35, 50)       1500050     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 34, 256)      25856       embedding_3[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 33, 256)      38656       embedding_3[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 32, 256)      51456       embedding_3[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 34, 256)      1024        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 33, 256)      1024        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 256)      1024        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 34, 256)      0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 33, 256)      0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 256)      0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 256)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 256)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1, 256)       0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3, 256)       0           max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 768)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 768)          0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            769         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,619,859\n",
      "Trainable params: 118,273\n",
      "Non-trainable params: 1,501,586\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textcnn_model = TEXT_CNN_MODEL_v1([2,3,4], max_seq_length, 256, 0.3)\n",
    "textcnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(textcnn_model, \n",
    "           to_file='./model_images/text_cnn_model.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./model_images/text_cnn_model.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1260000 samples, validate on 140000 samples\n",
      "Epoch 1/2\n",
      "1260000/1260000 [==============================] - 1450s 1ms/step - loss: 0.6095 - acc: 0.7280 - val_loss: 0.4979 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76479, saving model to ./models/textcnn_embeddings/weights-improvement-01-0.7648.hdf5\n",
      "Epoch 2/2\n",
      "1260000/1260000 [==============================] - 1641s 1ms/step - loss: 0.5011 - acc: 0.7585 - val_loss: 0.4835 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76479 to 0.77414, saving model to ./models/textcnn_embeddings/weights-improvement-02-0.7741.hdf5\n"
     ]
    }
   ],
   "source": [
    "filepath_2 =\"./models/textcnn_embeddings/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "history_textcnn_1 = textcnn_model.fit(x=padded_train_seq, y=train_labels, validation_split=0.1, \n",
    "                                batch_size=batch_size, epochs=epochs, callbacks=[checkpoint_2], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 TextRNN - version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEXT_CNN_MODEL_v2(filter_sizes, max_seq_length, dim, num_out_filters, drop_rate):\n",
    "    \n",
    "    input_ = Input(shape=(max_seq_length,))\n",
    "    x = embedding_layer(input_)\n",
    "    \n",
    "    # target_shape: target shape. Tuple of integers. Does not include the batch axis.\n",
    "    x = Reshape(target_shape=(max_seq_length, dim, 1))(x)\n",
    "    pools = []\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv2D(filters=num_out_filters, kernel_size=(filter_size, dim), strides=(1, 1), padding='valid')(x)\n",
    "        conv = BatchNormalization(axis=-1)(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        pool = MaxPooling2D(pool_size=(max_seq_length-filter_size+1, 1), strides=(1,1), padding='valid')(conv)\n",
    "        pools.append(pool)\n",
    "    concatenated_tensor = Concatenate(axis=1)(pools)\n",
    "    \n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop_rate)(flatten)\n",
    "    output = Dense(units=1, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 35, 50)       1500050     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 35, 50, 1)    0           embedding_3[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 34, 1, 256)   25856       reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 33, 1, 256)   38656       reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 1, 256)   51456       reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 34, 1, 256)   1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 33, 1, 256)   1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 1, 256)   1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 34, 1, 256)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 33, 1, 256)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 1, 256)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 1, 1, 256)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 1, 1, 256)    0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 1, 1, 256)    0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 3, 1, 256)    0           max_pooling2d_10[0][0]           \n",
      "                                                                 max_pooling2d_11[0][0]           \n",
      "                                                                 max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 768)          0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 768)          0           flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            769         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,619,859\n",
      "Trainable params: 118,273\n",
      "Non-trainable params: 1,501,586\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textcnn_model_v2 = TEXT_CNN_MODEL_v2([2,3,4], max_seq_length, emb_dim, 256, 0.3)\n",
    "textcnn_model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(textcnn_model_v2, \n",
    "           to_file='./model_images/text_cnn_model_v3.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./model_images/text_cnn_model_v3.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1260000 samples, validate on 140000 samples\n",
      "Epoch 1/2\n",
      "1260000/1260000 [==============================] - 2927s 2ms/step - loss: 0.6612 - acc: 0.7232 - val_loss: 0.4999 - val_acc: 0.7579\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75792, saving model to ./models/textcnn_v2_embeddings/weights-improvement-01-0.7579.hdf5\n",
      "Epoch 2/2\n",
      "1260000/1260000 [==============================] - 2537s 2ms/step - loss: 0.5045 - acc: 0.7562 - val_loss: 0.4851 - val_acc: 0.7699\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75792 to 0.76992, saving model to ./models/textcnn_v2_embeddings/weights-improvement-02-0.7699.hdf5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 2\n",
    "\n",
    "filepath_3 =\"./models/textcnn_v2_embeddings/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint_3 = ModelCheckpoint(filepath_3, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "history_textcnn_2 = textcnn_model_v2.fit(x=padded_train_seq, y=train_labels, validation_split=0.1, \n",
    "                                batch_size=batch_size, epochs=epochs, callbacks=[checkpoint_3], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1]. The paper: [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)\n",
    "\n",
    "[2]. [Overview and benchmark of traditional and deep learning models in text classification](https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html) and its [chinese version](https://mp.weixin.qq.com/s/z2bdlhaala2ko55MYiyXNw)\n",
    "\n",
    "[3]. [Keras中Conv1D和Conv2D的区别](https://blog.csdn.net/hahajinbu/article/details/79535172)\n",
    "\n",
    "[4]. [Sentiment analysis on Twitter using word2vec and keras](https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html)\n",
    "\n",
    "[5]. [用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践](https://zhuanlan.zhihu.com/p/25928551)\n",
    "\n",
    "[6]. [TextCnn原理及实践](https://blog.csdn.net/john_xyz/article/details/79210088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
