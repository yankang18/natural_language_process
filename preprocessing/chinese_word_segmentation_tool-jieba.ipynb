{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理领域中，分词和提取关键词都是对文本处理时通常要进行的步骤。用Python语言对英文文本进行预处理时可选择NLTK库，中文文本预处理可选择jieba库。jieba的详细使用请看[文档](https://github.com/fxsjy/jieba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 采用的算法：\n",
    "\n",
    "1.基于`前缀词典`实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的`有向无环图(DAG)`\n",
    "\n",
    "2.采用了`动态规划`查找`最大概率路径`, 找出基于词频的最大切分组合\n",
    "\n",
    "3.对于未登录词，采用了基于汉字成词能力的`HMM`模型，使用了`Viterbi`算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本分词函数和用法\n",
    "\n",
    "**jieba分词支持三种模式：**\n",
    "\n",
    "* `精确模式`：适合将句子最精确的分开，适合文本分析；这是默认模式.\n",
    "\n",
    "* `全模式`：把句子中所有可以成词的词语都扫描出来，速度快，但是不能解决歧义；\n",
    "\n",
    "* `搜索引擎模式`：在精确模式的基础上，对长词再次进行切分，提高召回率，适用于搜索引擎分词；\n",
    "\n",
    "**jieba 提供以下两种基本方法来进行分词：**\n",
    "\n",
    "`jieba.cut` 方法接受三个输入参数:\n",
    "\n",
    "* 需要分词的字符串\n",
    "* `cut_all` 参数用来控制是否采用全模式. If True, 采用全模式. Otherwise, 采用精确模式.\n",
    "* `HMM` 参数用来控制是否使用 HMM 模型\n",
    "\n",
    "`jieba.cut_for_search` 运用搜索引擎模式进行分词. 该方法接受两个参数：\n",
    "\n",
    "* 需要分词的字符串\n",
    "* `HMM` 参数用来控制是否使用 HMM 模型。\n",
    "\n",
    "`jieba.cut` 和 `jieba.cut_for_search` 方法返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语\n",
    "\n",
    "如果想直接使用list, 可以使用 `jieb.lcut` 以及 `jieba.lcut_for_search` 方法。 这两个方法都是直接返回list\n",
    "\n",
    "还有另外一个方法 `jieba.Tokenizer(dictionary=DEFUALT_DICT)` 用于新建自定义分词器，可用于同时使用不同字典，\n",
    "jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(默认)精确模式: 我/ 爱/ 学习/ 自然语言/ 处理\n"
     ]
    }
   ],
   "source": [
    "# 运用精确模式进行分词 via cut_all=False\n",
    "wordlist = jieba.cut('我爱学习自然语言处理', cut_all=False)\n",
    "print('(默认)精确模式: ' + '/ '.join(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式: 我/ 爱/ 学习/ 自然/ 自然语言/ 语言/ 处理\n"
     ]
    }
   ],
   "source": [
    "# 运用全模式进行分词 via cut_all=True\n",
    "wordlist = jieba.cut('我爱学习自然语言处理', cut_all=True)\n",
    "print('全模式: ' + '/ '.join(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索引擎模式: 小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 哈佛/ 大学/ 哈佛大学/ 深造\n"
     ]
    }
   ],
   "source": [
    "# 运用搜索引擎模式进行分词 via jieba.cut_for_search\n",
    "wordlist = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")  \n",
    "print('搜索引擎模式: ' + '/ '.join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 添加用户自定义字典：\n",
    "\n",
    "很多时候我们需要针对自己的场景进行分词，会有一些特定领域的专有词汇，以便包含词库中没有的词。虽然jieba分词有新词识别能力，但是自行添加新词可以保证更高的正确率。\n",
    "\n",
    "1. 用 `jieba.load_userdict(file_name)` 加载用户自定义词典。 在使用的时候，词典的格式和jieba分词器本身的分词器中的词典格式必须保持一致：\n",
    "    * 一个词占一行，每一行分成三部分，一部分为`词语`，一部分为`词频`，最后为`词性`（可以省略），用空格隔开。\n",
    "\n",
    "\n",
    "2. 少量的词汇可以自己用下面方法手动添加：\n",
    "    * 用 `add_word(word, freq=None, tag=None)` 和 `del_word(word)` 在程序中动态修改词典\n",
    "    * 用 `suggest_freq(segment, tune=True)` 可调节单个词语的词频，使其能（或不能）被分出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = '扫地后基本是找不到充电座的，或者电量不足的时候自动回充也基本上找不到充电座。扫地存在漏扫，优点是声音小'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(默认)精确模式: \n",
      "扫地/ 后/ 基本/ 是/ 找/ 不到/ 充电/ 座/ 的/ ，/ 或者/ 电量/ 不足/ 的/ 时候/ 自动/ 回充/ 也/ 基本上/ 找/ 不到/ 充电/ 座/ 。/ 扫地/ 存在/ 漏扫/ ，/ 优点/ 是/ 声音/ 小\n"
     ]
    }
   ],
   "source": [
    "wordlist = jieba.cut(comment, cut_all=False)\n",
    "print('(默认)精确模式: \\n' + '/ '.join(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "充电座 1\n",
      "回充 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open('userdict.txt')\n",
    "user_dict = file.read()\n",
    "print(user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict('userdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(默认)精确模式: \n",
      "扫地/ 后/ 基本/ 是/ 找/ 不到/ 充电座/ 的/ ，/ 或者/ 电量/ 不足/ 的/ 时候/ 自动/ 回充/ 也/ 基本上/ 找/ 不到/ 充电座/ 。/ 扫地/ 存在/ 漏扫/ ，/ 优点/ 是/ 声音/ 小\n"
     ]
    }
   ],
   "source": [
    "wordlist = jieba.cut(comment, cut_all=False)\n",
    "print('(默认)精确模式: \\n' + '/ '.join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键词提取是文本分析的关键一步. 只有关键词抽取并且进行词向量化之后，才好进行下一步的文本分析，可以说这一步是自然语言处理技术中文本处理最基础的一步。\n",
    "\n",
    "关键词抽取算法在`jieba.analyse`模块中，主要有两种算法：\n",
    "\n",
    "1.基于 TF-IDF 算法的关键词抽取(import jieba.analyse). 可以参考[详解自然语言处理之TF-IDF模型和python实现](http://mp.weixin.qq.com/s?__biz=MzIwNzYzNjA1OQ==&mid=2247484212&idx=1&sn=8a1f402fcdbf5c982c71859ce7e08c25&chksm=970e1000a079991611e03948ba0a74e6e45f6c2d64dcc0d65d9848d266ef751a0cb6a48864d2&scene=21#wechat_redirect)\n",
    "\n",
    "`jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())`\n",
    "* `sentence` 为待提取的文本；\n",
    "* `topK` 为返回几个 TF/IDF 权重最大的关键词，默认值为 20；\n",
    "* `withWeight` 为是否一并返回关键词权重值，默认值为 False；\n",
    "* `allowPOS` 仅包括指定词性的词，默认值为空，即不筛选。\n",
    "\n",
    "关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径：jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径\n",
    "\n",
    "关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径：jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地后基本是找不到充电座的，或者电量不足的时候自动回充也基本上找不到充电座。扫地存在漏扫，优点是声音小\n",
      "充电座 1.3283075003222222\n",
      "扫地 1.0350789081455556\n",
      "回充 0.6641537501611111\n",
      "漏扫 0.6641537501611111\n",
      "不到 0.59291325201\n",
      "电量 0.520448786601111\n",
      "优点 0.413116074905\n",
      "自动 0.3600720593922222\n",
      "基本上 0.35528807651333333\n",
      "不足 0.31737901157111115\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "\n",
    "print(comment)\n",
    "for word, weight in analyse.extract_tags(comment, topK=10, withWeight=True, allowPOS=()):\n",
    "    print(word, weight)\n",
    "# print(\"  \".join(analyse.extract_tags(comment, topK=10, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.基于TextRank算法的关键词抽取\n",
    "\n",
    "`jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’))` \n",
    "\n",
    "* 提取关键词，接口相同，注意默认过滤词性。\n",
    "* 算法基本思路：\n",
    "    * 将待抽取关键词的文本进行分词；\n",
    "    * 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图；\n",
    "    * 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地 1.0\n",
      "不到 0.8302322174672403\n",
      "漏扫 0.811010851047124\n",
      "优点 0.8015471019947196\n",
      "存在 0.7926304152728431\n",
      "自动 0.7448169853914659\n",
      "时候 0.7404494216857908\n",
      "基本上 0.6990560686302163\n",
      "电量 0.5156251901480693\n",
      "基本 0.43221658571106614\n"
     ]
    }
   ],
   "source": [
    "for word, weight in analyse.textrank(comment, topK=10, withWeight=True, allowPOS=('ns', 'n', 'vn','v')):\n",
    "    print(word, weight)\n",
    "# print(\"  \".join(analyse.textrank(comment, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn','v'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `jieba.posseg.POSTokenizer(tokenizer=None)` 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。\n",
    "* 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地 n\n",
      "后 f\n",
      "基本 n\n",
      "是 v\n",
      "找 v\n",
      "不到 v\n",
      "充电座 x\n",
      "的 uj\n",
      "， x\n",
      "或者 c\n",
      "电量 n\n",
      "不足 a\n",
      "的 uj\n",
      "时候 n\n",
      "自动 vn\n",
      "回充 x\n",
      "也 d\n",
      "基本上 n\n",
      "找 v\n",
      "不到 v\n",
      "充电座 x\n",
      "。 x\n",
      "扫地 n\n",
      "存在 v\n",
      "漏扫 v\n",
      "， x\n",
      "优点 n\n",
      "是 v\n",
      "声音 n\n",
      "小 a\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "comment = '扫地后基本是找不到充电座的，或者电量不足的时候自动回充也基本上找不到充电座。扫地存在漏扫，优点是声音小'\n",
    "words = pseg.cut(comment)\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
