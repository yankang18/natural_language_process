{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different ways of approaching \"many-to-one\"\n",
    "\n",
    "<img src='images/last_hidden_state.png' style=\"width:400px;height:230px;\"/>\n",
    "\n",
    "* We know that LSTM or GRU can learn \"long-term\" dependencies. However, If the sentence is too long, it would still forget words that appear earlier in time.  .\n",
    "* By taking the last RNN state, we hope the RNN has both found the relevant feature and remember it all the way to the end, which is surreal.\n",
    "\n",
    "<img src='images/hard_max_hidden_state.png' style=\"width:400px;height:300px;\"/>\n",
    "\n",
    "* Doing a max pool over RNN states is like doing a max pool over CNN features - it is essentially saying \"pick the most important feature\"\n",
    "* Hard max takes the max and forgets everything else.\n",
    "\n",
    "This leads us to a question: \n",
    "\n",
    "> Why not take all hidden states from encoder into account and pay attention to parts of those hidden states that are most relevant?\n",
    "\n",
    "Actually research shows that:\n",
    "\n",
    "> \"One important property of human perception is that one does not tend to process a whole scene in its entirety at once. Instead humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene, guiding future eye moovements and decision making.\"\n",
    ">\n",
    "> -- Recurrent Models of Visual Attention, 2014\n",
    "\n",
    "\n",
    "This is what attention tries to do: \n",
    "\n",
    "> `Attention` produces a probability distribution over each hidden state (from encoder) for \"how much to pay attention to\" by using softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Attention` mechanism is typically applied to decoder, the encoder does not change and it is composed of a embedding layer and a RNN layer. But instead of using one directional RNN, we can use bidirectioal RNN. \n",
    "\n",
    "<img src='images/bidirectional_rnn.png' style=\"width:560px;height:300px;\"/>\n",
    "\n",
    "* The output shape would be $T_x \\times 2M $, where $T_x$ is the the length of sequence or total time steps and $M$ is the number of hidden states (Since we use bidirectional RNN, the output dimention should be $2M$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Regular encoder-decoder vs. Encoder-decoder with attention\n",
    "\n",
    "\n",
    "<img src='images/encoder-decoder_comparison.png' style=\"width:650px;height:280px;\"/>\n",
    "\n",
    "In the regural encoder-decoder architecture, the decoder takes the final state from encoder. While in the encoder-decoder with attention architecture, the `attention` layer takes as input hidden states from all time steps of the RNN layer in the encoder and outputs a `context` vector which is essentially a `weighted sum` of those hidden states. \n",
    "\n",
    "> The key concept of `Attention` is to calculate an **attention weight vector** that is used to amplify the information of most relevant parts of the input sequence (internally represented by hidden states of the RNN cell of encoder) and in the same time, drown out irrelevant parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Attention Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core step of attention is to compute a `context vector` (also called `context`) at each time step $t$ of the decoder RNN cell. Such `context` is a compact representation of the information processed by encoder and it emphasizes more on parts of that information that are more relevant to hidden state being processed at time step $t$ of the decoder RNN cell.\n",
    "\n",
    "Following picture illustrates in high-level view about how we compute the `context vector` at time step $t$, given all the hidden states of the encoder RNN cell (e.g., $[h_{1}, h_{2}, h_{3}]$) and the current hidden state of the decoder RNN cell (e.g., $S_{t}$)\n",
    "\n",
    "<img src='images/compute_attention_weights.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Mathematical Definition for Attention\n",
    "\n",
    "As shown in above picture:\n",
    "1. We first calcualte a `attention score` for each of encoder hidden state (e.g., $[h_{1}, h_{2}, h_{3}]$).\n",
    "2. Then, we calculate softmax of those `attention scores` and get a `attention weight vector`.\n",
    "3. Finally, we compute the `context vector` by calculating the dot-product value between the `attention weight vector` and the encode hidden states.\n",
    "\n",
    "For each of the three steps, we have mathematical definition:\n",
    "\n",
    "**Scoring function for calculating attention scores:**\n",
    "\n",
    "$$ score(S_{t}, \\overline h_{t^{'}}) \\tag{1}$$  \n",
    "\n",
    "where \n",
    "* $S_{t}$ is the decoder hidden state at time step $t$ \n",
    "* $\\overline h_{t^{'}} $ is a vector of all encoder hidden states from all time steps.\n",
    "* Since we have two sequences one for encoder and one for decoder, we use\n",
    "    * $t$ to denote time step for decoder \n",
    "    * $t'$ to denote time step for encoder\n",
    "\n",
    "**Softmax for calculating attention weight vector:**\n",
    "\n",
    "<br/>\n",
    "$$ \\alpha_t = \\frac{\n",
    "exp(score(S_{t}, \\overline h_{t^{'}}))\n",
    "}{\n",
    "\\displaystyle\\sum_{t^{'} = 1}^{T_x} exp(score(S_{t}, h_{t^{'}}))\n",
    "}  \\tag{2}$$ \n",
    "\n",
    "where $ \\alpha_t = (\\alpha_{t,1}, \\alpha_{t,2}, \\alpha_{t,3}, ... , \\alpha_{t,T_x})$ and $T_x$ is the length of input sequence or the number of time steps of the encoder RNN cell.\n",
    "\n",
    "**Dot-product for computing the context vector**\n",
    "\n",
    "$$context_{t} = \\sum_{t' = 1}^{T_x} \\alpha_{t,t'} h_{t'}\\tag{3}$$\n",
    "\n",
    "Following picture illustrates the encoder-decoder with attention architecture in more detail: \n",
    "\n",
    "<img src='images/encoder_decoder_attention_high_level.png' style=\"width:600px;height:640px;\"/>\n",
    "\n",
    "Now, the most mysterious part of the attention is how to compute the `attention weight vector`. Before diving into the details of computing `attention weight vector`, we first introduce types of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Types of Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major types of attention: **additive attention** and **multiplicative attention**. Sometimes they are called `Bahdanau attention` and `Luong attention` respectively referring to the first authors of the papers which describe those attentions. You can check these two papers for details:\n",
    "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the score function can be calculated in three different ways (Of course, there are other variants).\n",
    "\n",
    "$$ score(S_{t}, \\overline h_{t^{'}})= \\begin{cases}\n",
    "    S^{T}_{t} \\overline h_{t^{'}}   & \\quad \\text{dot } \\\\\n",
    "    S^{T}_{t}W_a \\overline h_{t^{'}}   & \\quad \\text{general } \\\\\n",
    "    v^T_a tanh\\left(W_a[S^{T}_{t};\\overline h_{t^{'}}]\\right)  & \\quad \\text{concat}\n",
    " \\end{cases} $$ \n",
    "\n",
    "where\n",
    "* $S_{t}$ is the decoder hidden state at time step $t$ \n",
    "* $\\overline h_{t^{'}} $ is a vector of all encoder hidden states from all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Attention Score Computation\n",
    "\n",
    "In this section, we will illustrate the three ways of computing attention score in intuitive way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplicative Attention - dot product**\n",
    "\n",
    "<img src='images/multiplicative_attention_dot_product.png' style=\"width:700px;height:550px;\"/>\n",
    "\n",
    "Essentially, this scoring method is using dot product between one encoder hidden state vector and on decoder hidden state vector to calculate the attention score for that encoder hidden state. This makes sense because dot product of two vectors in word-embedding space is a measure of similarity between them.\n",
    "\n",
    "With the simplicity of this method, comes the drawback of assuming the encoder and decoder have the same embedding dimensions. The might work for text summarization for example, where the encoder ane decoder use the same language and the same embedding space. For machine translation, however, you might find that each language tends to have its own embedding space. \n",
    "\n",
    "This is the case where we might want to use the second scoring method - general multiplicative attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplicative Attention - general**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is a slight variation on the first method. It simply introduces a weight matrix between the multiplication of the decoder hidden state and the encoder hidden states. \n",
    "\n",
    "<img src='images/multiplicative_attention_general.png' style=\"width:750px;height:330px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additive Attention - concat**\n",
    "\n",
    "The way to do the additive attention is using a small feed forward neural network. To better illustrate this concept, we will calculate attention score for one encoder hidden state (i.e., $h_1$), instead of calculating attention scores for encoder hidden states all at once.\n",
    "\n",
    "<img src='images/additive_attention_concat.png' style=\"width:700px;height:520px;\"/>\n",
    "\n",
    "The concat scoring method is commonly done by concatenating one encoder hidden state vector and one decoder hidden state vector, and making that the input to a feed forward neural network. This network has a single hidden layer and outputs a score. The weights (i.e., $W_a$ and $V_a$) of this network are learned during the training process.\n",
    "\n",
    "In the above example, we only calculate the attention score for one encoder hidden state. The attention scores for other encoder hidden states (e.g., $h_2$, $h_3$) are calculated in exact the same way. The calculation of attention scores for all encoder hidden states is formalized as below:\n",
    "\n",
    "$$ v^T_a tanh\\left(W_a[S^{T}_{t};\\overline h_{t^{'}}]\\right)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concat method is very similar to the scoring method from the Bahdanau paper, which is shown below: \n",
    "    \n",
    "$$ v^T_a tanh\\left(W_aS^{T}_{t-1} + U_a \\overline h_{t^{'}}\\right)  $$\n",
    "\n",
    "In the Bahdanau paper, there are two major differences:\n",
    "1. It uses two weight matrices instead of one, each is applied to the respective vector.\n",
    "2. It uses hidden state from the previous timestep at the decoder instead of hidden state from current timestep.\n",
    "    * Actually, using hidden state from previous timestep and the one from current timestep are both legitimate choices. However, it would be better using hidden state from previous timestep to calculate context vector if you want to feed the context vector into the RNN cell of decoder. While using hidden state from current timestep might be a better choice if you want to feed the context vector into the dense layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply Context Vector\n",
    "\n",
    "After getting the attention scores, we then applies formula (1) and (2) to compute the `context vector`, denoted as $context_{t}$ (at time step $t$). The next question is how should we apply this $context_{t}$. \n",
    "\n",
    "Typically, we can feed this $context_{t}$ into two places:\n",
    "\n",
    "<img src='images/feed_context_vector.png' style=\"width:200px;height:300px;\"/>\n",
    "\n",
    "\n",
    "1. `Decoder RNN cell`. If we are using teacher forcing, we concat $context_{t}$ with the target vector $y_t$ and feed $[context_{t}; y_t]$ into the RNN cell. In this case, we use $S_{t-1}$ to calculate attention scores since at this point we have not calculated $S_t$ yet.\n",
    "2. `Dense layer`. We concat $context_{t}$ with the hidden state $S_t$ and feed $[context_{t}; S_t]$ into the dense layer. In this case, we use $S_{t}$ to calculate attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Encoder-Decoder with Attention in Detailed View\n",
    "\n",
    "<img src='images/encoder_decoder_architecture_detailed_view.png' style=\"width:750px;height:700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Attention Implementation Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss the attention implementation detail based on **Keras framework**. If you are using other framework, the implemetation might be different. But, the main idea should not change much.\n",
    "\n",
    "At a high level, the architecture has three components:\n",
    "* Encoder\n",
    "* Attention\n",
    "* Decoder\n",
    "\n",
    "Because the Attention is tightly coupled to the Decoder, we typically put them together as a single layer. This Decoder with Attention layer is the most complicated part of the whole architecture. We will explain the implementaion detail on this part. Also, we will keep tracking shapes of tensors while we are constructing the model to get a better understanding how the code works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Regular Decoder with no Attention\n",
    "\n",
    "For regular decoder with no attention, we just ask the Keras framwork to do the job for us. The Keras LSTM will automatically runs $T_y$ time steps, where $T_y$ is the length of `target_sequence` (used for teacher forcing).  \n",
    "* We can feed the `target_sequence` all at once into decoder LSTM because each element of the `target_sequence` does not change while the LSTM is doing the computation for each time step.\n",
    "\n",
    "```python\n",
    "x = LSTM(units, return_sequences=True)(target_sequence)\n",
    "y_hat = Dense(dimenstion)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decoder with Attention\n",
    "\n",
    "For decoder with attention, things become more complicated. The `context vector` at time step $t$ is calculated based on the hidden state $S_{t-1}$ that changes at each time step. As a result, the `context vector` changes at each time step. Therefore, we cannot rely on Keras LSTM to compute `context vectors` for us and we have to code the calculation of `context vector` at each time step. \n",
    "\n",
    "We define a `one_time_step_attention` function to calculate `context vector` at each time step. Following picture illustrates the work that the `one_time_step_attention` function does:\n",
    "\n",
    "<img src='images/attention_computation.png' style=\"width:500px;height:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`one_time_step_attention` function takes hidden state $S_{t-1}$ from decoder and hidden states of all time steps $[h_1, h_2, ... h_{T_x}]$ from encoder as input, and outputs the `context vector` at time step $t$.\n",
    "\n",
    "The pseudocode for this function is:\n",
    "\n",
    "```python\n",
    "repeat_layer = RepeatVector(T_x) # Repeats the input T_x times. \n",
    "concat_layer = Concatenate(axis=-1)\n",
    "dense1 = Dense(10, activation='tanh')\n",
    "dense2 = Dense(1, activation=softmax_over_time)\n",
    "dot = Dot(axes=1,name='attn_dot_layer')\n",
    "\n",
    "def one_time_step_attention(h_bar, S_t_1):\n",
    "    S_t_1 = repeat_layer(S_t_1)\n",
    "    x = concat_layer([h_bar, S_t_1])\n",
    "    x = dense1(x)\n",
    "    alphas = dense2(x) \n",
    "    context = dot([alphas, h_bar])\n",
    "    return context\n",
    "```\n",
    "\n",
    "where\n",
    "* `repeat_layer` copies `S_t_1` the $T_x$ amount of times.  \n",
    "* `concat_layer` concates `h_bar` with `S_t_1`. \n",
    "    * `h_bar` is a vector contains hidden states of all time steps $[h_1, h_2, ... h_{T_x}]$ from encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculating shapes of tensors**\n",
    "\n",
    "Make sure that you are able to understand following calculation because it helps you understand how the code works.\n",
    "\n",
    "* Suppose mini-batch size is $N$\n",
    "* Suppose encoder LSTM has hidden units = $M_1$, then,\n",
    "    * the shape of $h_t$ (e.g., $h_1$) is $(N, 2M_1)$ since we are using Bidirectional LSTM in encoder.\n",
    "    * the shape of $\\overline h$ (`h_bar` in code) is $(N, T_x, 2M_1)$, where $T_x$ is the length of input sequence and also is the number of time steps of encoder RNN layer.\n",
    "* Suppose decoder LSTM has hidden units = $M_2$, then,\n",
    "    * the shape of $S_{t-1}$ (`S_t_1` in code) is $(N, M_2)$. \n",
    "\n",
    "Let's walk through the `one_time_step_attention` funtion line by line:\n",
    "\n",
    "* After `repeat_layer`, the shape of `S_t_1` is $(N, T_x, M_2)$\n",
    "* After `concat_layer` that concates `h_bar` and `S_t_1`, the output `x` has shape of $(N, T_x, 2M_1 + M_2)$ because we defined `concat_layer` to concate two input tensors over their last dimension.\n",
    "* After `dense1` that takes tensor with shape of $(N, T_x, 2M_1 + M_2)$ as input, the output `x` has shape of $(N, T_x, 10)$, since the `dense1` has hidden units of 10.\n",
    "* After `dense2` that takes tensor with shape of $(N, T_x, 10)$ as input, the output `alpha` has shape of $(N, T_x, 1)$, since the `dense2` has hidden units of 1. Also,  the second dimension (i.e., the time step dimension) of `alpha` is **softmaxed**. \n",
    "* After `dot` that takes `alpha` with shape of $(N, T_x, 1)$ and `h_bar` with shape of $(N, T_x, 2M_1)$, the output `context` has shape of $(N, 1, 2M_1)$. This is because:\n",
    "    * We defined `dot` to calculate dot-product between two input tensors over their second dimension (axes=1), which is their time step dimension. \n",
    "    * What the `dot` does are that:\n",
    "        1. It first makes `alpha` to be a tensor with shape of $(N, T_x, 2M_1)$ by broadcasting. More specifically, it copies the original `alpha` with shape of $(N, T_x, 1)$ the $2M_1$ times and stacks those copies together to form a $(N, T_x, 2M_1)$ tensor.\n",
    "        2. It performs dot-product between the new `alpha` and `h_bar` over the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know how the attention works at a single time step. We then feed this `one_time_step_attention` into a bigger picture. That is how the attention works with the decoder. \n",
    "\n",
    "Following picture shows the computation of decoder-attention at time step $t$:\n",
    "\n",
    "<img src='images/decoder_attention_one_step.png' style=\"width:300px;height:400px;\"/>\n",
    "\n",
    "We will use a loop to perform the computation at each time step and following picture shows the whole view of the decoder-attention layer:\n",
    "\n",
    "<img src='images/decoder_attention_all_steps.png' style=\"width:600px;height:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudocode for this layer is:\n",
    "\n",
    "```python\n",
    "\n",
    "concat_layer = Concatenate(axis=2)\n",
    "decoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "h_bar = encoder(input_sequence)\n",
    "s = initial_value\n",
    "c = initial_value\n",
    "for t in range(T_y):\n",
    "    context = one_time_step_attention(h_bar, s)\n",
    "    x = concat_layer([context, y_t])\n",
    "    o, s, c = decoder_lstm(x, initial_state=[s, c])\n",
    "    y_hat = dense(o)\n",
    "```\n",
    "\n",
    "where \n",
    "* `y_t` is the target_sequence at time step $t$. \n",
    "* We concatenate the `context` vector with `y_t` and feed the concatenation into the decoder LSTM cell as input at time step $t$.\n",
    "    * For inferencing/predication, we concatenate the `context` vector with `y_hat_t`, which is the previous predicted word index, and feed the concatenation into the decoder LSTM cell as input at time step $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculating shapes of tensors**\n",
    "\n",
    "* `y_t` is the target_sequence at time step $t$. It is a word vector. Let's assume that the word embedding dimention is K. Then, \n",
    "    * 'y_t' has shape of $(N, 1, K)$\n",
    "* We have already known that `context` has shape of $(N, 1, 2M_1)$.\n",
    "* After `concat_layer` that concates `context` and `y_t`, the output `x` has shape of $(N, 1, 2M_1 + K)$ because we defined `concat_layer` to concate two input tensors over their second dimention which is also their last dimension.\n",
    "* After `decoder_lstm`, we have output `o` that has shape of $(N, M2)$\n",
    "* After `dense`, we have output `y_hat` that has shape of $(N, D)$, where D is total number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "* [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
