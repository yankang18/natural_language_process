{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the word distribution\n",
    "* Word embedding model\n",
    "    * calculate negative sampling for each training sample\n",
    "    * calculate cost function\n",
    "    * calculate derivatives of the cost function w.r.t. input-to-hidden weights and hidden-to-output weights respectively.\n",
    "* Calculate word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(3, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = range(3,3)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_dist(X, v_sz):\n",
    "    freq = {}\n",
    "    count = len(sum(x) for x in X)\n",
    "    for x in X:\n",
    "        for xi in x:\n",
    "            if xi not in freq:\n",
    "                freq[xi] = 0\n",
    "            freq[xi] += 1\n",
    "    wd = np.zeros(v_sz)\n",
    "    for j in range(2, v_sz):\n",
    "        wd[j] = (wd[j] / float(count))**0.75\n",
    "    return wd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_samples(word_dist, v_sz, context, num_neg_samples):\n",
    "    saved = {}\n",
    "    for context_idx in context:\n",
    "        saved[context_idx] = word_dist[context_idx]\n",
    "        word_dist[context_idx] = 0\n",
    "    \n",
    "    neg_samples = np.random.choice(\n",
    "        range(v_sz),\n",
    "        size=num_neg_samples,\n",
    "        replace=False,\n",
    "        p=(word_dist/np.sum(word_dist)),\n",
    "    )\n",
    "    \n",
    "    for idx, p in saved.items():\n",
    "        word_dist[idx] = p\n",
    "        \n",
    "    return neg_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(M1, M2):\n",
    "    return np.random.randn(M1, M2) / np.sqrt(M1 + M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(A):\n",
    "    return 1 / (1 + np.exp(-A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    def __init__(self, V, D):\n",
    "        self.V = V\n",
    "        self.D = D\n",
    "\n",
    "    def fit(X, context_sz, num_neg_samples=10, learning_rate=1e-4, mu=0.99, reg=0.1, epochs=100):\n",
    "        \n",
    "        N = len(X)\n",
    "        v_sz = len(self.V)\n",
    "        word_dist = get_word_dist(X, v_sz)\n",
    "        \n",
    "        self.W = init_weight(V, D)\n",
    "        self.V = init_weight(D, V)\n",
    "        \n",
    "        dW = np.zeros(self.W.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        \n",
    "        \n",
    "        # track cost for every single word\n",
    "        costs = []\n",
    "        \n",
    "        # track cost for each epoch\n",
    "        cost_per_epoch = []\n",
    "        \n",
    "        sample_indices = range(N)\n",
    "        for ep in range(epochs):\n",
    "            t0 = datetime.now()\n",
    "            \n",
    "            sample_indices = shuffle(sample_indices)\n",
    "            \n",
    "            # track cost for each sentence in epoch i\n",
    "            cost_per_epoch_i = []\n",
    "            for it in range(N):\n",
    "\n",
    "                j = sample_indices[it]\n",
    "                x = X[j]\n",
    "                \n",
    "                # too short\n",
    "                if len(x) < 2 * self.context_sz + 1:\n",
    "                    continue\n",
    "\n",
    "                # track cost for each word in sentence x\n",
    "                cj = []\n",
    "                n = len(x)\n",
    "                for jj in range(n):\n",
    "                    \n",
    "                    ## x[jj] is the index of word at position jj in x\n",
    "                    Z = self.W[x[jj], :]\n",
    "                    \n",
    "                    start = max(0, jj - context_sz)\n",
    "                    end = min(n, jj + 1 + context_sz)\n",
    "                    \n",
    "                    ctxt = np.concatenate(x[start: jj], x[(jj+1): end])\n",
    "                    ctxt = np.array(list(set(ctxt)), dtype=np.int32)\n",
    "                    neg = get_neg_samples(word_dist, v_sz, ctxt, num_neg_samples)\n",
    "\n",
    "                    posD = Z.dot(self.V[:, ctxt])\n",
    "                    posS = sigmoid(posD)\n",
    "\n",
    "                    negD = z.dot(self.V[:, neg])\n",
    "                    negS = sigmoid(-negD)\n",
    "                    \n",
    "                    cost = -np.log(posS).sum() - np.log(negS).sum()\n",
    "                    cj.append(cost / (num_neg_samples + len(ctxt)))\n",
    "                    \n",
    "                    pos_err = posS - 1\n",
    "                    gV_pos = np.outer(Z, pos_err)\n",
    "                    dV[:, ctxt] = mu*dV[:, ctxt] - learning_rate*(gV_pos + reg * self.V[:, ctxt])\n",
    "                    \n",
    "                    neg_err = 1 - negS \n",
    "                    gV_neg = np.outer(Z, neg_err)\n",
    "                    dV[:, neg] = mu*dV[:, neg] - learning_rate*(gV_neg + reg * self.V[:, neg])\n",
    "                    \n",
    "                    self.V[:, ctxt] += dV[:, ctxt]\n",
    "                    self.V[:, neg] += dV[:, neg]\n",
    "\n",
    "                    gW = pos_err.dot(V[:, ctxt].T) + neg_err.dot(V[:, neg].T)\n",
    "                    dW[x[jj],:] = mu*dW[x[jj],:] - learning_rate*(gW + reg * self.W[x[jj],:])\n",
    "                    self.W[x[jj],:] += dW[x[jj],:]\n",
    "                    \n",
    "                cj = np.mean(cj)\n",
    "                cost_per_epoch_i.append(cj)\n",
    "                cost.append(cj)\n",
    "            \n",
    "            epoch_cost = np.mean(cost_per_epoch_i)\n",
    "            cost_per_epoch.append(epoch_cost)\n",
    "            print(\"time to complete epoch %d:\" % i, (datetime.now - t0), \"cost:\", epoch_cost)\n",
    "        \n",
    "        plt.plot(costs)\n",
    "        plt.title(\"Numpy costs\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(cost_per_epoch)\n",
    "        plt.title(\"Numpy cost at each epoch\")\n",
    "        plt.show()\n",
    "    \n",
    "    def save(self, fn):\n",
    "        arrays = [self.W, self.V]\n",
    "        np.savez(fn, *arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, word2idx = get_wikipedia_date(n_files=50, n_vocab=2000)\n",
    "with open('w2v_word2idx.json', 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "V = len(words2idx)\n",
    "model = Model(80, V)\n",
    "model.fit(sentences, 10, learning_rate=10e-4, mu=0, epochs=7)\n",
    "model.save('w2v_model.npz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
