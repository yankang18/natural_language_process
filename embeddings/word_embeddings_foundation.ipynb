{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedings\n",
    "\n",
    "\n",
    "* Finding a higher level vector representation of words\n",
    "    * represent word in a more compact way. In other words, represent word with a smaller number of dimensions\n",
    "        * recall the goals of PCA: decorrelate observed data - if 99% documents that contains \"car\" also contain \"vehicle\", then we do not need 2 separate dimensions to represent them.\n",
    "    * represent word in a more meaningful way such that semantically closer words would have smaller distance apart from each other (i.e vectors that their euclidean distance is small).\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words \n",
    "\n",
    "* BOW throws away a lot of information that is vital to the meaning of text\n",
    "    * For example, we have two sentences: \"Dogs love cats and I\" and \"I love dogs and cats\". They both have correct grammatical structure, but have totally different meaning.\n",
    "* How to model bag-of-words\n",
    "    * <b style=\"color:red\">Each chuck of text (e.g., sentence or document) is modeled as a vector</b>. \n",
    "    * All vectors have the same length of V, which is the size of the vocabulary you defined for a specific NLP problem.\n",
    "    * Each entry of the vector represents a word in the vocabulary. \n",
    "    * Assuming we have a word-to-index dictionary, called word2idx, that maps each word in the vocabulary to a index corresponding a position in the vector\n",
    "    * Then, sentences are converted to vectors as follow:\n",
    "    \n",
    "    ```python\n",
    "    word_vectors = []\n",
    "    for sentence in input_sentences:\n",
    "        vector = np.zeros(V)\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            word_vector[word2idx[token]] = 1\n",
    "            word_vectors.append(vector)   \n",
    "    ```\n",
    "    \n",
    "    * A sentence becomes [0, 1, 0, 0,..., 1]\n",
    "    * The order of words in the original sentence is not maintained.\n",
    "* Why the order of words might be important\n",
    "    * for example we have sentences \"Today is a good day\" vs \"Today is not a good day\"\n",
    "    * The two sentences lead to almost the same vector except vector[\"not\"] = 1\n",
    "    * Therefore bag-of-words is not good at handling negation\n",
    "* RNNs might be a ideal because it keeps the state - seeing a \"not\" might result in negating everything that comes after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "* Each word is modeled as a vector of size V, \n",
    "    * V is the size of the vocabulary you defined for a specific NLP problem. \n",
    "    * Each position corresponds to a word in the vocabulary\n",
    "    * For the vector of each word, say $w$, all positions in the vector have values of zero except the position that corresponds to the word $w$ has value of 1.\n",
    "\n",
    "<img src=\"images/one_hot.png\" alt=\"one_hot\" style=\"width:40%;height:40%\"/>\n",
    "    \n",
    "* The problems of one-hot encoding are that:\n",
    "    * If we have one million words in the vocabulary, we would need a 1 million dimentional vector to represents all distinct possible words. \n",
    "    * It can not tell how related two words are since all pairs of words are the same distance part.\n",
    "        * |[1,0,0] - [0,1,0]| = 2\n",
    "        * all words are a distance of 1 from the origin, and a Manhattan distance of 2 from each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "* PCA takes in BOW or BOW weights by TF-IDF and outputs new vectors that are both smaller than the original and are meaningful distance away from each other.\n",
    "* How PCA works:\n",
    "\n",
    "** Step 1 ** \n",
    "* gather a bunch of documents, count how many times each word apprears in each document\n",
    "    * Data_matrix[word_index, doc_index] = count\n",
    "    * Could also use TF-IDF\n",
    "      \n",
    "```\n",
    "example:\n",
    "\n",
    "    document 1: \"cat dog rabit\"\n",
    "    document 2: \"cat lion tiger cat\"\n",
    "    \n",
    "    Create a VxN(5x2) matrix with these word-idx mapping:\n",
    "    cat=0, dog=1, rabbit=2, lion=3, tiger=4\n",
    "    \n",
    "            doc1 doc2\n",
    "     cat     1     2\n",
    "     dog     1     0\n",
    "     rabbit  1     0\n",
    "     lion    0     1\n",
    "     tiger   0     1\n",
    "```   \n",
    "\n",
    "* Usually we have NxD matrix, where N is the number of samples and D is the number of features\n",
    "* We put words along rows whild documents along columns\n",
    "* This makes sense:\n",
    "\n",
    "> The document a word shows up in is a feature of that word, since the document explains the meaning of that word. e.g., If a set of words show up in physics journal articles, these words have meaning related to physics\n",
    "\n",
    "> Unsupervised learning: the \"physics topic\" is the hidden cause/latent variable, the actual document is then generated from a distribution which describes what they look like. \n",
    "\n",
    "**Step 2**\n",
    "* Let's say we used PCA on a term-document matrix\n",
    "```python\n",
    "model = PCA()\n",
    "Z = model.fit_transform(X)\n",
    "# X is VxN matrix, Z is VxD matrix, where D<<N\n",
    "```\n",
    "\n",
    "* This VxD matrix is a <b style=\"color:red\">word embedding</b>. Each row is a D-dimensional vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings in RNN\n",
    "\n",
    "* NLP is tightly coupled with RNNs, since RNNs give us a way to avoid using bag-of-words (BOW)\n",
    "* Deep learning model uses word embeddings instead of one-hot word vector.\n",
    "    * <b style=\"color:red\">Each word is modeled as a one-hot word vector of size V</b>\n",
    "        \n",
    "    > Since each word is modeled as a one-hot word vector, a sequence of words (e.g., sentence or document) would be modeled as a list of one-hot word vectors with shape (T, V), where T is the length of the sequence. \n",
    "    \n",
    "    * one-hot word vector of size V would be tranformed into a much smaller vector of size D, where D << V. \n",
    "    \n",
    "    > The newly created word vector of size D is the word embedding.\n",
    "    \n",
    "    * Then, the word embeddings are used as inputs instead of one-hot vectors. But, the word embeddings are trained as a part of the RNN model\n",
    "    \n",
    "    \n",
    "** Word embeddings + RNN **\n",
    "* The architecture of word embeddings + RNN is depicted as follow. The word embedding layer is the first layer and the word embedding matrix $ W_e $ will be trained together with the RNN as a whole: \n",
    "    \n",
    "    <img src=\"images/embedding_rnn.png\" alt=\"word_embedding_rnn\" style=\"width:50%;height:50%\"/>\n",
    "     \n",
    "         Input vector (1xV) -> W_e -> word vector (1xD) -> recurrent unit -> softmax(1xK)\n",
    "    \n",
    "    * Assuming each word in a sequence is represented as a <b>one-hot vector</b> of length V. Thus, a sequence of length T is a list of one-hot vectors with shape (T, V)\n",
    "    * $ W_e $ of shape $(V, D)$ is the word embedding transformation matrix that is used to transform one-hot vector of size V into word vector/embedding of size D\n",
    "    * Since $ word(t) $ is a one-hot vector that has 1 in the $ ith $ position and all other positions have value of 0, $ x(t) =  word(t) \\times W_e $ = $ ith $ row of $ W_e $\n",
    "\n",
    "\n",
    "* More compact way of representing original word vector\n",
    "    * Mathematically equivalent to the method described above, where the original sequence is a list of one-hot vectors with shape (T, V)\n",
    "    * Assuming we have a word-to-index dictionary, called word2idx, that maps each word in the vocabulary to a index corresponding a row in the $ W_e $ matrix. We can model a sequence as a vector of length T such that each entry in the vector is a row index of the $ W_e $ matrix that corresponds a word vector/embedding.\n",
    "    * Now, a sequence is modeled as a vector of indexes with length T, instead of the original list of shape (T, V)\n",
    "    \n",
    "    A slow/naive way:\n",
    "    ```python\n",
    "    word_vectors = []\n",
    "    for index in input_sequence_indices:\n",
    "        word_vector = W_e(index, :)\n",
    "        word_vectors.append(word_vector)\n",
    "    ```\n",
    "    \n",
    "    Numpy/Theano can accept arrays/lists as indexes. Thus, a more efficient way is utilizing indexing of numpy or theano:\n",
    "    \n",
    "    ```python\n",
    "    word_vectors = W_e[input_sequence_indices]\n",
    "    ```\n",
    "\n",
    "**Example**\n",
    "* Assume we have a vocabulary containing 6 words and the word-to-index dictionary is defined as follow:\n",
    "    \n",
    "    { I=0, like=1, ice=2, cream=3, and=4, cake=5 }\n",
    "    \n",
    "    \n",
    "* A sequence \"I like ice cake\" would be modeled as a vector of indexes as:\n",
    "    \n",
    "     $$ seq_1 = [0, 1, 2, 5] $$\n",
    "     \n",
    "* A sequence \"cake I like\" would be modeled as:\n",
    "    \n",
    "     $$ seq_2 = [5, 0, 1] $$\n",
    "     \n",
    "         \n",
    "* Suppose we have the word embedding transformation matrix $ W_e $ of shape $(V, D) = (6, 3)$:\n",
    "    \n",
    "$$ W_e = \\begin{bmatrix}\n",
    "  a & b & c \\\\\n",
    "  d & e & f \\\\\n",
    "  g & h & i \\\\\n",
    "  j & k & l \\\\\n",
    "  m & n & o \\\\\n",
    "  p & q & r\n",
    " \\end{bmatrix} $$\n",
    " \n",
    "\n",
    "* The embedding transformation of $ seq_1 $ is calculated as follow:  \n",
    "    \n",
    " <b style=\"color:red\">$$   W_e[seq_1] = W_e[[0, 1, 2, 5]] =\n",
    " \\begin{bmatrix}\n",
    "  a & b & c \\\\\n",
    "  d & e & f \\\\\n",
    "  g & h & i \\\\\n",
    "  p & q & r \\\\\n",
    " \\end{bmatrix} \n",
    " $$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TensorFlow and Keras provide high level API for embedding layer: [here for TensorFlow](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) and [here for Keras](https://keras.io/layers/embeddings/) **\n",
    "\n",
    "* When defining an `embedding` layer, we need to specify an embedding matrix with shape `(vocabulary size, dimension of word vectors)`\n",
    "* The `embedding` layer takes a tensor of shape `(batch size, max input length)` as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below.\n",
    "    * The `batch size` is the number of training examples in each min-batch\n",
    "    * The `max input length` is the max number of words among all training examples in the min-batch\n",
    "\n",
    "<img src=\"images/embedding1.png\" style=\"width:700px;height:250px;\">\n",
    "\n",
    "<caption><center> **Figure 4**: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of `max_len=5`. The final dimension of the representation is  `(2,max_len,50)` because the word embeddings we are using are 50 dimensional. </center></caption>\n",
    "\n",
    "* The embedding layer contains a embedding matrix with shape `(vocabulary size, dimension of word vectors)`\n",
    "* The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. \n",
    "* The layer outputs a tenso of shape `(batch size, max input length, dimension of word vectors)`.\n",
    "    * Note that the second dimension `max input length` is also frequently refered as `time step` dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word Embeddings\n",
    "\n",
    "* In Word embeddings + RNNs, we put the word embedding matrix at the front of the recurrent network, which may suffer from vanishing gradient problem\n",
    "* Pre-training word embeddings would be especially helpful. We pre-train word embeddings instead of randomly initializing them and then plug that word embeddings into the RNN\n",
    "* We can still update word embedding matrix using backpropagation globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "* Word2vec is a technique or a paradigm which consists of a group of models (Skip-gram, Continuous Bag of Words (CBOW)), the target of each model is to produce fixed-size vectors for the corpus words, so that the words which have similar meaning would have close vectors (i.e vectors that their euclidean distance is small)\n",
    "* VxD input-to-hidden matrix and only one hidden layer \n",
    "* DxV hidden-to-output matrix\n",
    "* No hidden layer nonlinearity. That is: Linear mapping \n",
    "* not deep. Just shallow neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "\n",
    "* How to train the bigram model\n",
    "* Set output (next word) to 1, softmax, cross-entropy, gradient descent\n",
    "* Nothing special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continguous Bag of Words (CBOW)\n",
    "\n",
    "* Incorporate context\n",
    "* Can better predict a word given a window of surrounding words (rather then just previous word)\n",
    "* We will use a context size of 5-10 words (10 left + 10 right)\n",
    "* How we modify the neural network to use CBOW:\n",
    "    * Add more inputs to the neural network. So the input becomes fat but the output remains the same size.\n",
    "    * The same input-to-hidden weight for all inputs\n",
    "    * Each word is one-hot encoded. That is only one row from W goes to the hidden layer. This is not necessarily sharing weights\n",
    "    * What happens at hidden layer?\n",
    "        * $h$ = mean($W_i$), where $i$ = index of context word\n",
    "        * If we have 3 input vectors, we will have 3 D-dimensional vectors at the hidden layer, we just take the average of the 3 D-dimentional vector to form one D-dimensional vector that can be passed along to the output layer.\n",
    "    \n",
    "<img src=\"images/word2vec-cbow.png\" alt=\"Drawing\" style=\"width:50%;height:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram\n",
    "* CBOW and skip-gram are the two main methods of incorporating context with word2vec\n",
    "* Skip-gram is the opposite of CBOW\n",
    "* CBOW uses context to predict middle word\n",
    "* Skip-gram uses middle word to predict context\n",
    "\n",
    "<img src=\"images/word2vec-skipgram.png\" alt=\"Drawing\" style=\"width:55%;height:55%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling\n",
    "\n",
    "* The problem of word2vec is that: there may have tremendous number of weights to update at every iteration of gradient descent\n",
    "* Negative sampling addresses this problem by having each training sample only modify a small percentage of the weights, rather than all of them.\n",
    "* When training the network, the “label” or “correct/expected output” of the network is a one-hot vector:\n",
    "    * We want the output neuron corresponding to the target to output a 1, and all other output neurons to output a 0. \n",
    "    * There will be V-1 non-target words. A lot of time is spent on updating weights for those words. \n",
    "\n",
    "* Basic idea: throw away most of the non-target words from cost function since it is almost the size of the entire vocabulary. In other words, we are going to sample only a small number of non-target words to update the weights for. \n",
    "    * Non-target words are called \"negative samples\" that are drawn from the vocabulary excluding words from the context of the input word\n",
    "\n",
    "> [The paper](https://arxiv.org/pdf/1310.4546.pdf) says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.<br/>\n",
    "\n",
    "If the output layer of our model has a weight matrix that’s 300 x 10,000, we will just be updating the weights for words in the context, say 2, plus the weights negative samples,say 10. That’s a total of 12 output neurons, and 3600 weight values total. That’s only 0.12% of the 3M weights in the output layer!\n",
    "\n",
    "> In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).\n",
    "\n",
    "**How to select the negative samples**\n",
    "* Typically, choosing negative samples depends on the words frequency in the given corpus. The higher frequency of the word means the higher chance of choosing it as a negative sample.\n",
    "* The following formula is used to determine the probability of selecting the word as a negative sample:\n",
    "\n",
    "$$ P(w_i) = {f(w_i)^c \\over \\sum_{j=0}^{n} f(w_j)^c}$$\n",
    "\n",
    "Where c is a constant that is selected by the model creator. Research shows that c is 3/4 workds well\n",
    "* The decision to raise the frequency to the 3/4 power appears to be empirical; in their paper they say it outperformed other functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which word embeddings to use\n",
    "\n",
    "* With word2vec we have two word embeddings\n",
    "* Which one to use?\n",
    "\n",
    "**Method 1**\n",
    "\n",
    "$$W_e = [W_1 W_2^T]$$\n",
    "\n",
    "* $W_1$ is the input-to-hidden matrix with shape (V, D) \n",
    "* $W_2$ is the hidden-to-output matrix with shape (D, V)\n",
    "* $W_e$ is the concatenation of $W_1$ and $W_2^T$ with shape (V, 2D)\n",
    "\n",
    "**Method 2**\n",
    "\n",
    "$$W_e = {{W_1 + W_2^T} \\over 2} $$\n",
    "\n",
    "* $W_1$ is the input-to-hidden matrix with shape (V, D) \n",
    "* $W_2$ is the hidden-to-output matrix with shape (D, V)\n",
    "* $W_e$ is the average of the sum of $W_1$ and $W_2^T$ with shape (V, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b style=\"color:red\">*</b>Cost function and derivative\n",
    "\n",
    "* To adopt negative sampling, we have a cost function that is differenet from softmax cross entropy:\n",
    "\n",
    "$$ J = -\\sum_{o \\in ctxt} log (\\sigma(V_o^T W_i)) -\\sum_{\\phi \\in neg} log (\\sigma(- V_{\\phi}^T W_i)) \\qquad \\qquad (1) $$\n",
    "\n",
    "$ V $ is hidden-to-output matrix with shape $(D, M)$, $V^T$ has shape $(M, D)$, <br/>\n",
    "    $ V_o^T $ is the $o$th vector of $ V^T $ and it is a complex vector. <br/>\n",
    "    $ V_{\\phi}^T $ is the $\\phi$th vector of $ V^T $ and it is a negative sample vector. <br/>\n",
    "$ W $ is input-to-hidden matrix with shape $(M, D)$, $ W_i $ is the $i$th vector of $ W $\n",
    "\n",
    "* Here, I draw a pictorical example of the input-to-hidden matrix $ W $ and hidden-to-output matrix $ V $. In this example, we have 1M works in the vocabulary and 300 neurons in the hidden layer:\n",
    "\n",
    "<img src=\"images/word2vec_2matrix.png\" alt=\"Drawing\" style=\"width:55%;height:55%\"/>\n",
    "\n",
    "\n",
    "* Here, I draw a pictorical example of $ V $ and $ V^T $: \n",
    "<img src=\"images/word2vec_output_matrix.png\" alt=\"Drawing\" style=\"width:55%;height:55%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Derivatives\n",
    "\n",
    "(1) To update output layer weights, we need to update all output weights that are either in context weight vector or in negative sample weight vector:\n",
    "\n",
    "* For each conext word weight vector $ o \\in ctxt $:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial V_o^T} = (\\sigma(V_o^T W_i) - 1)W_i \\qquad \\qquad (2) $$\n",
    "\n",
    "where  <b style=\"color:red\">$\\sigma(V_o^T W_i) - 1 $</b> is the error\n",
    "\n",
    "\n",
    "* For each negative example weight vector $ \\phi \\in neg $:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial V_{\\phi}^T} = (1 - \\sigma(- V_{\\phi}^T W_i))W_i \\qquad \\qquad (3) $$\n",
    "\n",
    "where  <b style=\"color:red\">$ 1- \\sigma(- V_{\\phi}^T W_i)$</b> is the error\n",
    "\n",
    "(2) To update hidden layer weights, we only need to update input word weight vector:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W_i} = \\sum_{o \\in ctxt}(\\sigma(V_o^T W_i) - 1)V_o^{T} + \\sum_{\\phi \\in neg} (1- \\sigma(- V_{\\phi}^T W_i))V_{\\phi}^{T} \\qquad \\qquad (4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize formulars \n",
    "* In order to efficiently calculate these derivatives (get rid of for-loops) to speed up code significantly by utilizing numerical computation tools like numpy, we vectorizing cost function formula (1) and derivative fomulars (2), (3) and (4).\n",
    "\n",
    "* Before explaining vertorization, we first explain some notations that may be confusing.\n",
    "\n",
    "<img src=\"images/notation_explain_1.png\" alt=\"Drawing\" style=\"width:55%;height:55%\"/>\n",
    "\n",
    "**Vectorize cost function, formular (1)**\n",
    "\n",
    "<b style=\"color:red\"> $$ J = - sum(log(\\sigma(W_i V_{(:,ctxt)}))) - sum(log(\\sigma(-W_i V_{(:,neg)}))) $$ </b>\n",
    "\n",
    "**Vectorize formular (2)**\n",
    "* Let's first zoom in formula (2):\n",
    "\n",
    "$$ (..., \\frac{\\partial J}{\\partial V_{(o,j)}^T},...) = (..., (\\sigma(V_{o}^T W_i) - 1)W_{(i, j)},...) \\qquad  \\text{for each} \\quad  o \\in \\text{ctxt} $$\n",
    "\n",
    "This is actually two for-loops for calculating the derivatives of each weight in the context word weight vectors. Let's write it in pseudo code:\n",
    "\n",
    "<img src=\"images/two_for_loop.png\" alt=\"Drawing\" style=\"width:25%;height:25%\"/>\n",
    "\n",
    "* We can vectorize this by using outer product of numpy:\n",
    "\n",
    "<img src=\"images/vec_outer_product.png\" alt=\"Drawing\" style=\"width:40%;height:40%\"/>\n",
    "\n",
    "* We vectorize $ \\sigma(V_{o}^T W_i) $ for all $ o \\in ctxt$: \n",
    "\n",
    "$$ \\sigma(V_{(ctxt,:)}^T W_i) = (..., \\sigma(V_{o \\in ctxt}^T W_i) ,...)$$ \n",
    "\n",
    "* For programming convenience, we may use $V$ instead of $V^T$:\n",
    "\n",
    "$$ \\sigma(V_{(ctxt,:)}^T W_i) = \\sigma(W_i V_{(:,ctxt)}) $$\n",
    "\n",
    "$$ \\sigma(W_i V_{(:,ctxt)}) = (..., \\sigma(W_i V_{(:,o \\in ctxt)}) ,...) $$ \n",
    "\n",
    "* Taking the error form of the vector:\n",
    "\n",
    "$$ \\sigma(W_i V_{(:,ctxt)} )-1 = (..., \\sigma(W_i V_{(:,o \\in ctxt)})-1 ,...) $$ \n",
    "\n",
    "* Finally we outer product $ W_i $ and $ (\\sigma(W_i V_{(:,ctxt)} )-1) $:\n",
    "\n",
    "<b style=\"color:red\"> $$ \\frac{\\partial J}{\\partial V_{(:,ctxt)}} = W_i \\odot (\\sigma(W_i V_{(:,ctxt)} )-1) $$ </b>\n",
    "\n",
    "**Vectoring formula (3)**\n",
    "* Zoom in formula (3), we have:\n",
    "\n",
    "$$(..., \\frac{\\partial J}{\\partial V_{(\\phi,j)}^T},...) = (..., (1 - \\sigma(- V_{\\phi}^T W_i))W_{(i,j)},...) \\qquad  \\text{for each} \\quad  \\phi \\in \\text{neg}$$\n",
    "\n",
    "* Vectorizing $ \\sigma(V_{\\phi}^T W_i) $:\n",
    "\n",
    "$$ \\sigma(V_{(neg,:)}^T W_i) = (..., \\sigma(V_{\\phi \\in neg}^T W_i) ,...)$$ \n",
    "\n",
    "* For programming convenience, we may use $V$ instead of $V^T$:\n",
    "\n",
    "$$ \\sigma(V_{(neg,:)}^T W_i) = \\sigma(W_i V_{(:,neg)}) $$\n",
    "\n",
    "$$ \\sigma(W_i V_{(:,neg)} ) = (..., \\sigma(W_i V_{(:,\\phi \\in neg)}) ,...) $$ \n",
    "\n",
    "* Taking the error form of the vector:\n",
    "\n",
    "$$ 1-\\sigma(- W_i V_{(:,neg)} ) = (..., 1-\\sigma(- W_i V_{(:,\\phi \\in neg)}) ,...) $$ \n",
    "\n",
    "* Finally we outer product $ W_i $ and $ (1-\\sigma(-W_i V_{(:,neg)})) $:\n",
    "\n",
    "<b style=\"color:red\"> $$ \\frac{\\partial J}{\\partial V_{(:,neg)}} = W_i \\odot (1 - \\sigma(- W_i V_{(:,neg)} )) $$ </b>\n",
    "\n",
    "**Vectoring formula (4)**\n",
    "* formula (4) has two components, both of which are actually in the form of inner product of two vectors according to the picture below:\n",
    "\n",
    "<img src=\"images/vec_inner_product.png\" alt=\"Drawing\" style=\"width:40%;height:40%\"/>\n",
    "\n",
    "* Vectorizing the two components:\n",
    "\n",
    "$$ \\sum_{o \\in ctxt}(\\sigma(V_o^T W_i) - 1)V_o^{T} = (\\sigma(W_i V_{(:,o \\in ctxt)})-1) \\cdot V_{(:,o \\in ctxt)}^T $$\n",
    "\n",
    "$$ \\sum_{\\phi \\in neg} (1- \\sigma(- V_{\\phi}^T W_i))V_{\\phi}^{T} = (1-\\sigma(- W_i V_{(:,\\phi \\in neg)})) \\cdot  V_{(:,\\phi \\in neg)}^T $$\n",
    "\n",
    "Finally, we have the vectorization form:\n",
    "\n",
    "<b style=\"color:red\"> $$ \\frac{\\partial J}{\\partial W_i} = (\\sigma(W_i V_{(:,o \\in ctxt)})-1) \\cdot V_{(:,\\phi \\in ctxt)}^T  + (1-\\sigma(- W_i V_{(:,\\phi \\in neg)})) \\cdot  V_{(:,\\phi \\in neg)}^T $$</b>\n",
    "\n",
    "To calculate these derivatives in code, we only need to calculate the formulas in red color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Following are all the components and formulars we need to train a Word2Vec model.\n",
    "* $ (5)-(8) $ are the basic components forming formular (9)-(12). When coding, we can calculate each of them individually.\n",
    "\n",
    "$$\\sigma(W_i V_{(:,ctxt)}) \\qquad \\qquad \\qquad (5) $$\n",
    "\n",
    "$$\\sigma(-W_i V_{(:,neg)}) \\qquad \\qquad \\qquad (6) $$\n",
    "\n",
    "$$\\sigma(W_i V_{(:,ctxt)} )-1 \\qquad \\qquad \\qquad (7) $$\n",
    "\n",
    "$$ 1 - \\sigma(- W_i V_{(:,neg)}) \\qquad \\qquad \\qquad (8) $$\n",
    "\n",
    "\n",
    "* $ (9)-(12) $ are the formulars for calculating cost/loss funtion and derivatives.\n",
    "\n",
    "\n",
    "$$ J = - sum(log(\\sigma(W_i V_{(:,ctxt)}))) - sum(log(\\sigma(-W_i V_{(:,neg)}))) \\qquad \\qquad \\qquad (9) $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial V_{(:,ctxt)}} = W_i \\odot (\\sigma(W_i V_{(:,ctxt)} )-1) \\qquad \\qquad \\qquad (10) $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial V_{(:,neg)}} = W_i \\odot (1 - \\sigma(- W_i V_{(:,neg)} )) \\qquad \\qquad \\qquad (11) $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial W_i} = (\\sigma(W_i V_{(:,o \\in ctxt)})-1) \\cdot V_{(:,\\phi \\in ctxt)}^T  + (1-\\sigma(- W_i V_{(:,\\phi \\in neg)})) \\cdot  V_{(:,\\phi \\in neg)}^T \\qquad \\qquad \\qquad (12) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
