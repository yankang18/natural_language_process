{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transform raw data into the format that:\n",
    "    * can fit into the model (i.e., `numerical` word vectors or word embedding since we cannot work with text directly when using machine learning algorithms.)\n",
    "    * loss as less information as possible during the transforming process (e.g., stopwords may not be appropriate or using domain-specific version of stopwords).\n",
    "    \n",
    "    \n",
    "* Mainly two steps:\n",
    "    * Preprocess text\n",
    "    * Vectorize text. In other words, transforming text into numercial vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "* Typical text preprocessing:\n",
    "    * Extract/clean raw text from web (in form of XHTML or XML) or other resources\n",
    "        * For some application, e.g., `Knowledge Graph` construction, we may exploit the structure of XHTML/XML to learn the relations between entities extracted from web pages.\n",
    "    * Remove punctuation\n",
    "    * Tokenization\n",
    "    * Remove stopwords (may use domain-specific stopwords)\n",
    "    * [Stemming and Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "    * part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(samples):\n",
    "    filtered_samples = []\n",
    "    for i in samples:\n",
    "        filtered_samples.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return filtered_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Todays so beautiful']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "remove_punctuation([\"Today's so beautiful!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "* Tokenize text into sentences\n",
    "* Tokenize text into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use NLTK `word_tokenize` to tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    tokenized_samples = []\n",
    "    for s in samples:\n",
    "        tokens = word_tokenize(s)\n",
    "        tokenized_samples.append(tokens)\n",
    "    return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Todays', 'so', 'beautiful']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "tokenize(['Todays so beautiful'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use NLTK `sent_tokenize` to tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use existing stopwords resources such as English stopwords from nltk.corpus\n",
    "* For some applications or certain Machine Learning models, existing stopwords may not be appropriate.\n",
    "    * For example, If we want to use Recurrent Neural Network model to solve sentiment analysis problem, we should not treat negation words such as \"not\", \"neither\" and \"didn't\" as stopwords since they may help solve negation issue.\n",
    "* For some domains, we may define our domain-specific stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "type(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ourselves', 'had', 'below', 'so', 'is', 'that', 'mightn', 'itself', 'with', 'on', 'as', 'which', \"isn't\", 'into', \"haven't\", 'and', \"you've\", 'off', 'for', 've', \"wasn't\", 'doing', 'until', 'were', 'an', 'll', 'hadn', 'am', 'now', 'during', 'both', 'here', 'be', \"didn't\", 'their', 'these', \"wouldn't\", 'further', 'ours', 'do', 'them', 'yourself', \"that'll\", 'y', 'against', 'was', 'where', 'up', 'yours', 'or', 'it', 'this', 'don', 'from', 'does', 'at', 'no', 'should', 'yourselves', 'my', 'wasn', 'did', 'whom', 'why', \"doesn't\", 'been', \"don't\", 'will', 'under', 'other', 'are', 'but', 'then', 'how', 'not', 'haven', 'its', 's', 't', 'can', 'over', 'there', 'any', 'same', \"you'd\", 'couldn', \"weren't\", 'aren', \"needn't\", 'i', 'more', 'o', 'hasn', 'theirs', \"mightn't\", \"mustn't\", \"shan't\", 'above', 'to', 'weren', 'of', \"she's\", 'ain', 'they', 'her', 'isn', 'few', 'he', 'myself', 'in', 'm', \"it's\", 'doesn', \"aren't\", 'very', 'him', \"you're\", 'hers', 'only', \"hasn't\", 'the', 'down', 'while', \"couldn't\", 'didn', 're', 'being', 'because', 'themselves', 'have', \"should've\", 'once', 'd', 'own', \"you'll\", 'when', 'between', 'about', 'needn', 'she', 'by', 'we', 'too', 'those', 'you', 'than', 'himself', 'just', 'has', 'our', 'ma', 'herself', 'shan', 'nor', 'after', 'most', 'his', \"shouldn't\", 'before', 'a', 'if', 'wouldn', 'again', 'such', \"hadn't\", 'out', 'each', 'your', 'me', \"won't\", 'some', 'through', 'shouldn', 'who', 'all', 'mustn', 'what', 'having', 'won'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(samples):\n",
    "    filtered_samples = []\n",
    "    for s in samples:\n",
    "        filstered_tokens = []\n",
    "        for w in s:\n",
    "            if w not in stopWords:\n",
    "                filstered_tokens.append(w)\n",
    "        filtered_samples.append(filstered_tokens)\n",
    "    return filtered_samples   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Todays', 'beautiful']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "remove_stopwords([['Todays', 'so', 'not', 'beautiful']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "* Word stemming is a normalizaiton process. It transforms words into their steming form.\n",
    "* Another typical normalization process for English words is lowercase all words. \n",
    "\n",
    "<img src=\"images/word-stem.png\" alt=\"\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game -> game\n",
      "gaming -> game\n",
      "gamed -> game\n",
      "games -> game\n"
     ]
    }
   ],
   "source": [
    "# There are more stemming algorithms, but Porter (PorterStemer) is the most popular.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(word, \"->\", ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "\n",
    "For some application, part-of-speech of words may better help us analyze the structure or semantics of text\n",
    "\n",
    "* For Java, we can use [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
    "* For Python, we can use NLTK `pos_tag` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('Whether', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('new', 'JJ'), ('to', 'TO'), ('programming', 'VBG'), ('or', 'CC'), ('an', 'DT'), ('experienced', 'JJ'), ('developer', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('easy', 'JJ'), ('to', 'TO'), ('learn', 'VB'), ('and', 'CC'), ('use', 'VB'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    " document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
    "pos = nltk.pos_tag(nltk.word_tokenize(document))\n",
    "print(type(pos))\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nltk-speech-codes.png\" alt=\"\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to numerical\n",
    "\n",
    "Most ML algorithms including deep learning rely on numerical data to be fed into them as input. Meaning, we need to convert the text to numbers.\n",
    "\n",
    "More specifically, we may want to perform classification of documents, so each document is an “input” and a class label is the “output” for our predictive algorithm. Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.\n",
    "\n",
    "A simple and effective model for representing text documents in machine learning is called the Bag-of-Words Model, or BoW that ignores word order and focuses on the occurrence of words in a document.\n",
    "\n",
    "BoW coverts a collection of documents to a matrix, with each document being a row with the length of the vocabulary of known words and each word (or token) being the column, and the corresponding (row,column) values being the frequency of occurrance of each word or token in that document.\n",
    "\n",
    "The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Bag of Words Using CountVectorizer of scikit-learn ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To handle this, we will be using sklearns \n",
    "[count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) method which does the following:\n",
    "\n",
    "* It tokenizes a collection of text documents (separates each document into individual words) and build a vocabulary of known words, and to encode new documents using that vocabulary.\n",
    "* It assigns each words an integer representing the frequency of word appearing in its document.\n",
    "\n",
    "** More specifically: ** \n",
    "\n",
    "* The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he' differently. It does this using the `lowercase` parameter which is by default set to `True`.\n",
    "\n",
    "* It also ignores all punctuation so that words followed by a punctuation mark (for example: 'hello!') are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: 'hello'). It does this using the `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n",
    "\n",
    "* CountVectorizer will automatically ignore all words(from our input text) that are found in the built in list of english stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of spam.\n",
    "\n",
    "**To sum up**\n",
    "* `CountVectorizer` does following work for us:\n",
    "    * Tokenization\n",
    "    * Lowercase tokenized words\n",
    "    * Remove punctuation\n",
    "    * Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer()` has certain parameters which take care of these steps for us. They are:\n",
    "\n",
    "* `lowercase = True`\n",
    "    \n",
    "    The `lowercase` parameter has a default value of `True` which converts all of our text to its lower case form.\n",
    "\n",
    "\n",
    "* `token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`\n",
    "    \n",
    "    The `token_pattern` parameter has a default regular expression value of `(?u)\\\\b\\\\w\\\\w+\\\\b` which \n",
    "    * ignores all punctuation marks and treats them as delimiters, \n",
    "    * accepts alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n",
    "\n",
    "\n",
    "* `stop_words`\n",
    "\n",
    "    The `stop_words` parameter, if set to `english` will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.\n",
    "\n",
    "You can take a look at all the parameter values of your `count_vector` object by simply printing out the object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how': 5, 'money': 7, 'me': 6, 'you': 11, 'from': 2, 'tomorrow': 9, 'now': 8, 'win': 10, 'are': 0, 'hello': 3, 'call': 1, 'home': 4}\n",
      "['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money', 'now', 'tomorrow', 'win', 'you']\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(documents)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(documents)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to transform() returns a sparse matrix, and you can transform them back to numpy arrays by calling the `toarray()` function.\n",
    "\n",
    "Importantly, the same vectorizer can be used on documents that contain words not included in the vocabulary. These words are ignored and no count is given in the resulting vector.\n",
    "\n",
    "For example, below is an example of using the vectorizer above to encode a document with no word in the vocab. Running this example prints the array version of the encoded sparse matrix showing that none of the words in this document appears in the learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# encode another document\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential issue that can arise from using this method out of the box is the fact that if our dataset of text is extremely large (say if we have a large collection of news articles or email data), there will be certain values that are more common that others simply due to the structure of the language itself. So for example words like 'is', 'the', 'an', pronouns, grammatical contructs etc could skew our matrix and affect our analyis.\n",
    "\n",
    "There are a couple of ways to mitigate this. One way is to use the stop_words parameter and set its value to english. This will automatically ignore all words(from our input text) that are found in a built in list of English stop words in scikit-learn.\n",
    "\n",
    "Another way of mitigating this is by using the tfidf method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Bag of Words Using TfidfVectorizer of scikit-learn ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "* `Term Frequency`: This summarizes how often a given word appears within a document.\n",
    "* `Inverse Document Frequency`: This downscales words that appear a lot across documents.\n",
    "\n",
    "The [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. \n",
    "\n",
    "Alternately, if you already have a learned CountVectorizer, you can use it with a [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to just calculate the inverse document frequencies and start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Vocabulary --------\n",
      "{'how': 5, 'money': 7, 'me': 6, 'you': 11, 'from': 2, 'tomorrow': 9, 'now': 8, 'win': 10, 'are': 0, 'hello': 3, 'call': 1, 'home': 4}\n",
      "--------- idf ------------\n",
      "[ 1.91629073  1.51082562  1.91629073  1.51082562  1.91629073  1.91629073\n",
      "  1.91629073  1.91629073  1.91629073  1.91629073  1.91629073  1.51082562]\n",
      "--------- summarize encoded vector ------------\n",
      "(1, 12)\n",
      "[[ 0.55528266  0.          0.          0.43779123  0.          0.55528266\n",
      "   0.          0.          0.          0.          0.          0.43779123]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(documents)\n",
    "# summarize\n",
    "print(\"------ Vocabulary --------\")\n",
    "print(vectorizer.vocabulary_)\n",
    "print(\"--------- idf ------------\")\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([documents[0]])\n",
    "# summarize encoded vector\n",
    "print(\"--------- summarize encoded vector ------------\")\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary of 12 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n",
    "\n",
    "The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.51082562 to the most frequently observed word: “hello” at index 3 and \"you\" at index 11.\n",
    "\n",
    "Finally, the first document is encoded as an 12-element sparse array and we can review the final scorings of each word are normalized to values between 0 and 1, and the encoded document vectors can then be used directly with most machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "* [Tokenizing Words and Sentences with NLTK](https://pythonspot.com/en/tokenizing-words-and-sentences-with-nltk/)\n",
    "* [NLTK stop words](https://pythonspot.com/nltk-stop-words/)\n",
    "* [NLTK stemming](https://pythonspot.com/nltk-stemming/)\n",
    "* [NLTK part of speech tagging](https://pythonspot.com/nltk-speech-tagging/)\n",
    "* [How to Prepare Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "* [4.2. Feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "* [Working With Text Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
