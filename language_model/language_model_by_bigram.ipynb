{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language model by bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following series of tasks, you will work with 2-grams, or bigrams, as they are more commonly called. The objective is to create a function that calculates the probability that a particular sentence could occur in a corpus of text, based on the probabilities of its component bigrams. We'll do this in stages though:\n",
    "\n",
    "* Task 1 - Extract tokens and bigrams from a sentence\n",
    "* Task 2 - Calculate probabilities for bigrams\n",
    "* Task 3 - Calculate the log probability of a given sentence based on a corpus of text using bigrams\n",
    "\n",
    "**Assumptions and terminology**\n",
    "\n",
    "We will assume that text data is in the form of sentences with no punctuation. If a sentence is in a single line, we will add add a token for start of sentence: `<s>` and end of sentence: `</s>`. For example, if the sentence is \"I love language models.\" it will appear in code as:\n",
    "\n",
    "```python\n",
    "'I love language models'\n",
    "```\n",
    "\n",
    "The tokens for this sentence are represented as an ordered list of the lower case words plus the start and end sentence tags:\n",
    "\n",
    "```python\n",
    "tokens = ['<s>', 'i', 'love', 'language', 'models', '</s>']\n",
    "```\n",
    "\n",
    "The bigrams for this sentence are represented as a list of lower case ordered pairs of tokens:\n",
    "\n",
    "```python\n",
    "bigrams = [('<s>', 'i'), ('i', 'love'), ('love', 'language'), ('language', 'models'), ('models', '</s>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bigrams\n",
    "\n",
    "In the task below, write a function that returns a list of `tokens` (e.g., word) and a list of `bigrams` for a given sentence. You will need to first break a sentence into words in a list, then add a `<s>` and `<s/>` token to the start and end of the list to represent the start and end of the sentence.\n",
    "\n",
    "Your final lists should be in the format shown above and called out in the function doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for index in range(len(sentence_tokens) - 1):\n",
    "        sentence_bigrams.append((sentence_tokens[index], sentence_tokens[index+1]))\n",
    "    return sentence_tokens, sentence_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Sentence: \"the old man spoke to me\"\n",
      "tokens = ['<s>', 'the', 'old', 'man', 'spoke', 'to', 'me', '</s>']\n",
      "bigrams = [('<s>', 'the'), ('the', 'old'), ('old', 'man'), ('man', 'spoke'), ('spoke', 'to'), ('to', 'me'), ('me', '</s>')]\n",
      "\n",
      "*** Sentence: \"me to spoke man old the\"\n",
      "tokens = ['<s>', 'me', 'to', 'spoke', 'man', 'old', 'the', '</s>']\n",
      "bigrams = [('<s>', 'me'), ('me', 'to'), ('to', 'spoke'), ('spoke', 'man'), ('man', 'old'), ('old', 'the'), ('the', '</s>')]\n",
      "\n",
      "*** Sentence: \"old man me old man me\"\n",
      "tokens = ['<s>', 'old', 'man', 'me', 'old', 'man', 'me', '</s>']\n",
      "bigrams = [('<s>', 'old'), ('old', 'man'), ('man', 'me'), ('me', 'old'), ('old', 'man'), ('man', 'me'), ('me', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print('\\n*** Sentence: \"{}\"'.format(sentence))\n",
    "    t, b = sentence_to_bigrams(sentence)\n",
    "    print('tokens = {}'.format(t))\n",
    "    print('bigrams = {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probabilities and Likelihoods with Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from a previous video that the probability of a series of words can be calculated from the chained probabilities of its history:\n",
    "\n",
    "<span class=\"mathquill\">$$P(w_1w_2...w_n)=\\prod_i P(w_i|w_1w_2...w_{i-1})$$</span>\n",
    "\n",
    "The probabilities of sequence occurrences in a large textual corpus can be calculated this way and used as a language model to add grammar and contectual knowledge to a speech recognition system. However, there is a prohibitively large number of calculations for all the possible sequences of varying length in a large textual corpus.\n",
    "\n",
    "To address this problem, we use the Markov Assumption to approximate a sequence probability with a shorter sequence:\n",
    "\n",
    "$$P(w_1w_2...w_n)\\approx \\prod_i P(w_i|w_{i-k}...w_{i-1})$$\n",
    "\n",
    "We can calculate the probabilities by using counts of the bigrams and individual tokens:\n",
    "\n",
    "$$P(w_i|w_{i-1})=\\frac{c(w_{i-1},w_i)}{c(w_{i-1})}$$\n",
    "\n",
    "In Python, the [Counter](https://docs.python.org/2/library/collections.html) method is useful for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sentence: \"i am as i am\"\n",
    "tokens = ['<s>', 'i', 'am', 'as', 'i', 'am', '</s>']\n",
    "bigrams = [('<s>', 'i'), ('i', 'am'), ('am', 'as'), ('as', 'i'), ('i', 'am'), ('am', '</s>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'</s>': 1, '<s>': 1, 'am': 2, 'as': 1, 'i': 2})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = Counter(tokens)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<s>', 'i'): 1,\n",
       "         ('am', '</s>'): 1,\n",
       "         ('am', 'as'): 1,\n",
       "         ('as', 'i'): 1,\n",
       "         ('i', 'am'): 2})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counts = Counter(bigrams)\n",
    "bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "bigram = ('i', 'am')\n",
    "# P('am' | 'i')\n",
    "P = bigram_counts[bigram] / token_counts[bigram[0]]\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, bigrams = bigrams_from_transcript('transcripts.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that returns a probability dictionary when given a lists of tokens and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_mle(tokens, bigrams):\n",
    "    \"\"\"\n",
    "    provide a dictionary of probabilities for all bigrams in a corpus of text\n",
    "    the calculation is based on maximum likelihood estimation and does not include\n",
    "    any smoothing.  A tag '<unk>' has been added for unknown probabilities.\n",
    "    :param tokens: list\n",
    "        tokens: list of all tokens in the corpus\n",
    "    :param bigrams: list\n",
    "        bigrams: list of all two word tuples in the corpus\n",
    "    :return: dict\n",
    "        bg_mle_dict: a dictionary of bigrams:\n",
    "            key: tuple of two bigram words, in order OR <unk> key\n",
    "            value: float probability\n",
    "\n",
    "    \"\"\"\n",
    "    token_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    bg_mle_dict = {}\n",
    "    for key, val in bigram_counts.items():\n",
    "        bg_mle_dict[key] = bigram_counts[key] / token_counts[key[0]]\n",
    "    bg_mle_dict['<unk>'] = 0.\n",
    "    return bg_mle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability bigram dictionary:\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "bg_dict = bigram_mle(tokens, bigrams)\n",
    "print(\"Probability bigram dictionary:\")\n",
    "# print(bg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smoothing and logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still a couple of problems to sort out before we use the bigram probability dictionary to calculate the probabilities of new sentences:\n",
    "\n",
    "1. Some possible combinations may not exist in our probability dictionary but are still possible. We don't want to multiply in a probability of 0 just because our original corpus was deficient. This is solved through \"smoothing\". There are a number of methods for this, but a simple one is the [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) with the \"add-one\" estimate where V is the size of the vocabulary for the corpus, i.e. the number of unique tokens:\n",
    "\n",
    "$$P_{add1}(w_i|w_{i-1})=\\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$\n",
    "\n",
    "2. Repeated multiplications of small probabilities can cause underflow problems in computers when the values become to small. To solve this, we will calculate all probabilities in log space. Multiplying probabilities in the log space has the added advantage that the logs can be added:\n",
    "\n",
    "<span class=\"mathquill\">$$\\qquad \\qquad \\qquad log(p_1\\times p_2\\times p_3\\times p_4) = \\log p_1 + \\log p_2 + \\log p_3 + \\log p_4 $$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following quiz, the function `bigram_add1_logs` generates bigram probability with Laplace smoothing in the log space. Write a function that calculates the log probability for a given sentence, using this log probability dictionary. If all goes well, you should observe that more likely sentences yield higher values for the log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bigram_add1_logs(tokens, bigrams):\n",
    "    \"\"\"\n",
    "    provide a smoothed log probability dictionary \n",
    "    :param tokens: list\n",
    "        tokens: list of all tokens in the corpus\n",
    "    :param bigrams: list\n",
    "        bigrams: list of all two word tuples in the corpus\n",
    "    :return: dict\n",
    "        bg_add1_log_dict: dictionary of smoothed bigrams log probabilities including\n",
    "        tags: <s>: start of sentence, </s>: end of sentence, <unk>: unknown placeholder probability\n",
    "    \"\"\"\n",
    "    \n",
    "    token_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    vocab_size = len(token_counts)\n",
    "    bg_add1_dict = {}\n",
    "    for key, val in bigram_counts.items():\n",
    "        bg_add1_dict[key] = np.log((bigram_counts[key] + 1)/ (token_counts[key[0]] + vocab_size))\n",
    "    bg_add1_dict['<unk>'] = np.log(1. / vocab_size)\n",
    "    return bg_add1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_of_sentence(sentence, bigram_log_dict):\n",
    "    # get the sentence bigrams with utils.sentence_to_bigrams\n",
    "    # look up the bigrams from the sentence in the bigram_log_dict\n",
    "    # add all the the log probabilities together\n",
    "    # if a word doesn't exist, be sure to use the value of the '<unk>' lookup instead\n",
    "    \n",
    "    tokens, bigrams = sentence_to_bigrams(sentence)\n",
    "    \n",
    "    total_log_prob = 0.\n",
    "    for bg in bigrams:\n",
    "        if bg in bigram_log_dict:\n",
    "            total_log_prob += bigram_log_dict[bg]\n",
    "        else:\n",
    "            total_log_prob += bigram_log_dict['<unk>']\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** \"the old man spoke to me\"\n",
      "-34.80495531345013\n",
      "*** \"me to spoke man old the\"\n",
      "-39.34280606002005\n",
      "*** \"old man me old man me\"\n",
      "-36.59899481268447\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "bigram_log_dict = bigram_add1_logs(tokens, bigrams)\n",
    "for sentence in test_sentences:\n",
    "    print('*** \"{}\"'.format(sentence))\n",
    "    print(log_prob_of_sentence(sentence, bigram_log_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bigram Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In functions `bigram_mle` and `bigram_add1_logs`, we index the bigram as a whole (in the form of tuple) to access the bigram conditional probability. However, we may run into scenarios or applications that require us to predict the next word given current word (or phrase). Given a word, there might be multiple possible next words that each is assigned a probability. Therefore we need to index the word and the next word respectively for accessing their probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def bigram_conditional_prob(tokens, bigrams):\n",
    "            \n",
    "    bg_cond_dict = defaultdict(dict)   \n",
    "    token_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    vocab_size = len(token_counts)\n",
    "    for key, val in bigram_counts.items():\n",
    "        bg_cond_dict[key[0]][key[1]] = bigram_counts[key]/ token_counts[key[0]]\n",
    "        bg_cond_dict[key[0]]['<unk>'] = 0\n",
    "    bg_cond_dict['<unk>'] = 0\n",
    "    return bg_cond_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'excuse': 0.2, 'bring': 0.2, '<unk>': 0, 'be': 0.2, 'you': 0.2, 'oblige': 0.2}\n"
     ]
    }
   ],
   "source": [
    "bigram_cond_dict = bigram_conditional_prob(tokens, bigrams)\n",
    "print(bigram_cond_dict['will'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "https://github.com/udacity/AIND-VUI-quizzes\n",
    "\n",
    "https://github.com/oucler/NLND-End2End-Speech-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
