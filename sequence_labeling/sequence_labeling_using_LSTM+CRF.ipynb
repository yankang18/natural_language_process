{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling using LSTM+CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load takens (e.g., words or tags) and create their indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token2index(file_name):\n",
    "    token2idx = {}\n",
    "    with open(file_name) as f:\n",
    "        for idx, token in enumerate(f):\n",
    "            token = token.strip()\n",
    "            token2idx[token] = idx\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = load_token2index('data/word_vocab.txt')\n",
    "tag2index = load_token2index('data/pos_tag_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22949\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(word2index))\n",
    "print(len(tag2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_token(token2index, lowercase=False):\n",
    "    \n",
    "    def f(token):\n",
    "        \n",
    "        if lowercase:\n",
    "            token = token.lower()\n",
    "        if token.isdigit():\n",
    "            token = NUM\n",
    "        \n",
    "        if token in token2index:\n",
    "            token = token2index[token]\n",
    "        else:\n",
    "            token = token2index[UNK]\n",
    "        \n",
    "        return token\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_word_f = get_processing_token(word2index, True)\n",
    "process_tag_f = get_processing_token(tag2index, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, file_name, processing_word, processing_tag, max_iter=None):\n",
    "        self.file_name = file_name\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def __iter__(self):\n",
    "        words = []\n",
    "        pos_tags = []\n",
    "        niter = 0\n",
    "        with open(self.file_name) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line)==0 or line.startswith(\"-DOCSTART-\"):\n",
    "                    if len(words)!=0:\n",
    "                        niter+=1\n",
    "                        if self.max_iter is not None and niter > self.max_iter:\n",
    "                            break\n",
    "                        yield (words, pos_tags)\n",
    "                        words, pos_tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, pos_tag = ls[0], ls[1]\n",
    "                    if self.processing_word is not None:\n",
    "                        word = self.processing_word(word)\n",
    "                    if self.processing_tag is not None:\n",
    "                        pos_tag = self.processing_tag(pos_tag)\n",
    "                    words += [word]\n",
    "                    pos_tags += [pos_tag]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(dataset, batch_size):\n",
    "    xbatch, ybatch = [], []\n",
    "    for word, tag in dataset:\n",
    "        if len(xbatch) == batch_size:\n",
    "            yield xbatch, ybatch\n",
    "            xbatch, ybatch = [], []\n",
    "            \n",
    "        xbatch += [word]\n",
    "        ybatch += [tag]\n",
    "    \n",
    "    if len(xbatch) != 0:\n",
    "        yield xbatch, ybatch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('data/train.txt', process_word_f, process_tag_f, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (xbatch, ybatch) in enumerate(minibatch(dataset, 4)):\n",
    "#     print('batch_'+str(i), len(xbatch), len(ybatch))\n",
    "#     print(xbatch)\n",
    "#     print(ybatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    max_len = max(map(lambda x:len(x), sequences))\n",
    "    \n",
    "    sequences_pad, sequences_length = [], []\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_len] + [0]*max(max_len - len(seq), 0)\n",
    "        sequences_pad += [seq_]\n",
    "        sequences_length += [min(len(seq), max_len)]\n",
    "    return sequences_pad, sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = [[19176, 11783, 4637, 15283, 18989, 3386, 19850, 5296, 15298], [1889, 2142], [11319, 9722]]\n",
    "# seq_pad, seq_len = pad_sequences(seq)\n",
    "# print(seq_pad)\n",
    "# print(seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoder \n",
    "    1. placeholder for input\n",
    "    2. embedding for transforming index to vector\n",
    "    3. LSTM for transforming input embedding into internal representation\n",
    "* Decoder\n",
    "    1. decoder for inferencing\n",
    "    2. decoder for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = len(word2index)\n",
    "dim_word = 50\n",
    "hidden_size_lstm = 300\n",
    "tag_num = len(tag2index)\n",
    "use_crf = True\n",
    "pretrained_embedding = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Input\n",
    "\n",
    "* training samples with shape `(batch_size, max_sequence_length)`\n",
    "* labels with shape `(batch_size)` (or `(batch_size, max_sequence_length)` if using one hot encoding)\n",
    "* sequence lengths\n",
    "* hyperparameters\n",
    "    * dropout rate\n",
    "    * learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name='word_ids')\n",
    "sequence_lengths = tf.placeholder(shape=[None], dtype=tf.int32, name='sequence_lengths')\n",
    "labels = tf.placeholder(shape=[None, None], dtype=tf.int32, name='label')\n",
    "\n",
    "learning_rate = tf.placeholder( dtype=tf.float32, name='learning_rate')\n",
    "dropout_rate = tf.placeholder( dtype=tf.float32, name='dropout_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pretrained_embedding:\n",
    "    word_embedding_matrix = tf.get_variable(shape=[word_num, dim_word], \n",
    "                                        dtype=tf.float32, \n",
    "                                        name='word_embedding_matrix')\n",
    "else:\n",
    "    filename_glove = \"data/glove.6B/glove.6B.{}d.txt\".format(dim_word)\n",
    "    embedding_matrix = np.zeros((word_num, dim_word))\n",
    "    with open(filename_glove) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            if word in word2index:\n",
    "                embedding_matrix[word2index[word]] = np.asarray(line[1:])\n",
    "    \n",
    "    word_embedding_matrix = tf.Variable(embedding_matrix,\n",
    "                                        dtype=tf.float32, \n",
    "                                        name='pretrained_word_embedding_matrix',\n",
    "                                        trainable=True)\n",
    "\n",
    "word_embedding = tf.nn.embedding_lookup(word_embedding_matrix, \n",
    "                                        word_ids, \n",
    "                                        name='word_embedding_lookup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('bi_lstm_encoder'):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(hidden_size_lstm)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(hidden_size_lstm)\n",
    "    \n",
    "    (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                cell_bw, \n",
    "                                                                word_embedding, \n",
    "                                                                sequence_length=sequence_lengths, \n",
    "                                                                dtype=tf.float32)\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "    output = tf.nn.dropout(output, dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('decoder'):\n",
    "    W = tf.get_variable(shape=[2*hidden_size_lstm, tag_num], dtype=tf.float32, name='proj_W')\n",
    "    b = tf.get_variable(shape=[tag_num], dtype=tf.float32, name='proj_b')\n",
    "    \n",
    "    time_steps = tf.shape(output)[1]\n",
    "    output = tf.reshape(output, [-1, 2*hidden_size_lstm])\n",
    "    pred = tf.matmul(output, W) + b\n",
    "    logits = tf.reshape(pred, [-1, time_steps, tag_num])\n",
    "    logits_shape = tf.shape(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('projection'):\n",
    "    if not use_crf:\n",
    "        label_pred = tf.cast(tf.argmax(logits, axis=-1), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_shape [20 40 45]\n",
    "# trans_params_shape [45 45]\n",
    "# log_likelihood_shape [20]\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    if use_crf:\n",
    "        log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(logits, \n",
    "                                                                         labels, \n",
    "                                                                         sequence_lengths) \n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "        log_likelihood_shape = tf.shape(log_likelihood)\n",
    "        trans_params_shape = tf.shape(trans_params)\n",
    "    else:\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        mask = tf.sequence_mask(sequence_lengths)\n",
    "        losses = tf.boolean_mask(losses, mask)\n",
    "        loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(valid_dataset, sess, batch_size):\n",
    "    accs = []\n",
    "    ret = []\n",
    "    for xbatch, labels in minibatch(valid_dataset, batch_size):\n",
    "        word_seq, sequence_len = pad_sequences(xbatch)\n",
    "        \n",
    "        feed = {\n",
    "                word_ids: word_seq,\n",
    "                sequence_lengths: sequence_len,\n",
    "                dropout_rate:1.0\n",
    "               }\n",
    "        \n",
    "        if use_crf:\n",
    "            viterbi_sequences = []\n",
    "            logits_v, trans_params_v = sess.run([logits, trans_params], feed_dict=feed)\n",
    "            for logit, seq_length in zip(logits_v, sequence_len):\n",
    "                logit = logit[:seq_length]\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params_v)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "            labels_pred_v = viterbi_sequences\n",
    "        else:\n",
    "            labels_pred_v = sess.run(label_pred, feed_dict=feed)\n",
    "            \n",
    "        for words, lab, lab_pred, seq_length in zip(xbatch, labels, labels_pred_v, sequence_len):\n",
    "            lab = lab[:seq_length]\n",
    "            lab_pred = lab_pred[:seq_length]\n",
    "            acc = [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "            ret.append((words, lab, lab_pred, acc))\n",
    "            accs+=acc\n",
    "\n",
    "    overall_acc = np.mean(accs)\n",
    "    \n",
    "    return overall_acc, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('data/train.txt', process_word_f, process_tag_f)\n",
    "valid_dataset = Dataset('data/valid.txt', process_word_f, process_tag_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs         = 10\n",
    "kr              = 0.7 # keep rate\n",
    "batch_size      = 20\n",
    "lr              = 0.01 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(nepochs):\n",
    "        losses = []\n",
    "        i = 0\n",
    "        for xbatch, ybatch in minibatch(dataset, batch_size):\n",
    "            i+=1\n",
    "            word_seq, sequence_len = pad_sequences(xbatch)\n",
    "            target_seq, _ = pad_sequences(ybatch)\n",
    "\n",
    "            # build feed dictionary\n",
    "            feed = {\n",
    "                word_ids: word_seq,\n",
    "                labels: target_seq,\n",
    "                sequence_lengths: sequence_len,\n",
    "                learning_rate:lr,\n",
    "                dropout_rate:dr\n",
    "            }\n",
    "        \n",
    "#             logits_sh, trans_params_shape2, log_likelihood_shape2 = sess.run([logits_shape, trans_params_shape, \n",
    "#                                                                               log_likelihood_shape], feed_dict=feed)\n",
    "    \n",
    "#             print('logits_shape', logits_sh)\n",
    "#             print('trans_params_shape', trans_params_shape2)\n",
    "#             print('log_likelihood_shape', log_likelihood_shape2)\n",
    "            \n",
    "            _, train_loss = sess.run([train_op, loss], feed_dict=feed)\n",
    "            losses += [train_loss]\n",
    "            \n",
    "            if i % 10 ==0:\n",
    "                print('ep:', ep, 'iter:', i, 'loss:', np.mean(losses))\n",
    "            if i % 50 ==0:\n",
    "                acc, _ = validation(valid_dataset, sess, batch_size)\n",
    "                print('accuracy', acc)\n",
    "    saver.save(sess, \"checkpoints/ner.ckpt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ep: 0 iter: 10 loss: 56.54659\n",
    "ep: 0 iter: 20 loss: 43.158413\n",
    "ep: 0 iter: 30 loss: 33.372463\n",
    "ep: 0 iter: 40 loss: 27.63274\n",
    "ep: 0 iter: 50 loss: 24.566917\n",
    "accuracy 0.6777384058253183\n",
    "......\n",
    "ep: 9 iter: 610 loss: 2.1155434\n",
    "ep: 9 iter: 620 loss: 2.1206038\n",
    "ep: 9 iter: 630 loss: 2.1088154\n",
    "ep: 9 iter: 640 loss: 2.1111786\n",
    "ep: 9 iter: 650 loss: 2.119872\n",
    "accuracy 0.9326739612943421\n",
    "ep: 9 iter: 660 loss: 2.1296499\n",
    "ep: 9 iter: 670 loss: 2.1357484\n",
    "ep: 9 iter: 680 loss: 2.1491969\n",
    "ep: 9 iter: 690 loss: 2.1582155\n",
    "ep: 9 iter: 700 loss: 2.1662493\n",
    "accuracy 0.9308243448463844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {index : word for word, index in word2index.items()}\n",
    "index2tag = {index : tag for tag, index in tag2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(index2token, token_indices):\n",
    "    ret = [index2token[idx] for idx in token_indices]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RP', 'NNS', ')', 'EX']"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(index2tag, [0, 3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset('data/test.txt', process_word_f, process_tag_f, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/ner.ckpt\n",
      "------------------------------------------------------\n",
      "['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.']\n",
      "['NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',', 'NNP', 'IN', 'DT', 'NN', '.']\n",
      "['NN', ':', 'NNP', 'NNP', 'NNP', 'NNP', ',', 'NNP', 'IN', 'NNP', 'NNP', '.']\n",
      "accuracy: [True, True, True, False, True, True, True, True, True, False, False, True]\n",
      "------------------------------------------------------\n",
      "['nadim', 'ladki']\n",
      "['NNP', 'NNP']\n",
      "['NNP', 'SYM']\n",
      "accuracy: [True, False]\n",
      "------------------------------------------------------\n",
      "['al-ain', ',', 'united', 'arab', 'emirates', '$UNK$']\n",
      "['NNP', ',', 'NNP', 'NNP', 'NNPS', 'CD']\n",
      "['NNP', ',', 'NNP', 'NNP', 'NNP', 'CD']\n",
      "accuracy: [True, True, True, True, False, True]\n",
      "------------------------------------------------------\n",
      "['japan', 'began', 'the', 'defence', 'of', 'their', 'asian', 'cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'syria', 'in', 'a', 'group', 'c', 'championship', 'match', 'on', 'friday', '.']\n",
      "['NNP', 'VBD', 'DT', 'NN', 'IN', 'PRP$', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'CD', 'VBP', 'IN', 'NNP', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'NN', 'IN', 'NNP', '.']\n",
      "['NNP', 'VBD', 'DT', 'NN', 'IN', 'PRP$', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'CD', 'VBP', 'IN', 'NNP', 'IN', 'DT', 'NN', 'NNP', 'NN', 'NN', 'IN', 'NNP', '.']\n",
      "accuracy: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True]\n",
      "------------------------------------------------------\n",
      "['but', 'china', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'uzbekistan', '.']\n",
      "['CC', 'NNP', 'VBD', 'PRP$', 'NN', 'VB', 'PRP', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'VBG', 'TO', 'DT', 'NN', 'CD', 'NN', 'TO', 'NNS', 'NNP', '.']\n",
      "['CC', 'NNP', 'VBD', 'PRP$', 'NN', 'NN', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'VBG', 'TO', 'DT', 'NN', 'CD', 'NN', 'TO', 'NNS', 'NN', '.']\n",
      "accuracy: [True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True]\n",
      "acc 0.8714285714285714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/ner.ckpt\")\n",
    "    overall_acc, result = validation(test_dataset, sess, 20)\n",
    "    \n",
    "    for (words, lab, lab_pred, acc) in result:\n",
    "        print('------------------------------------------------------')\n",
    "        print(parse(index2word, words))\n",
    "        print(parse(index2tag, lab))\n",
    "        print(parse(index2tag, lab_pred))\n",
    "        print('accuracy:', acc)\n",
    "    print(\"acc\", overall_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
