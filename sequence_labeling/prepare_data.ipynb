{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/train.txt', 'data/valid.txt', 'data/test.txt']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = 'data/train.txt'\n",
    "valid_file = 'data/valid.txt'\n",
    "test_file = 'data/test.txt'\n",
    "files = [train_file, valid_file ,test_file]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    words = []\n",
    "    pos_tags = []\n",
    "    ner_tags = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) !=0 and not line.startswith('-DOCSTART-'):\n",
    "                ls = line.split(' ')\n",
    "                words.append(ls[0].lower())\n",
    "                pos_tags.append(ls[1])\n",
    "                ner_tags.append(ls[-1])\n",
    "    return words, pos_tags, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21009\n",
      "23865\n",
      "26869\n"
     ]
    }
   ],
   "source": [
    "words_set = set()\n",
    "pos_tags_set = set()\n",
    "ner_tags_set = set()\n",
    "\n",
    "for file_name in files:\n",
    "    words, pos_tags, ner_tags = get_data(file_name)\n",
    "    words_set.update(words)\n",
    "    print(len(words_set))\n",
    "    pos_tags_set.update(pos_tags)\n",
    "    ner_tags_set.update(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"',\n",
       " '$',\n",
       " \"''\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'NN|SYM',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'SYM',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_tags_set)\n",
    "pos_tags_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/glove.6B/glove.6B.100d.txt\n",
      "data/glove.6B.100d.trimmed.npz\n"
     ]
    }
   ],
   "source": [
    "dim_word = 100\n",
    "# glove files\n",
    "filename_glove = \"data/glove.6B/glove.6B.{}d.txt\".format(dim_word)\n",
    "# trimmed embeddings (created from glove_filename with build_data.py)\n",
    "filename_trimmed = \"data/glove.6B.{}d.trimmed.npz\".format(dim_word)\n",
    "\n",
    "print(filename_glove)\n",
    "print(filename_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = set()\n",
    "with open(filename_glove) as f:\n",
    "    for line in f:\n",
    "        word = line.strip().split(' ')[0]\n",
    "        glove_vocab.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(glove_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vocab\n",
    "# words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22949\n"
     ]
    }
   ],
   "source": [
    "# get intersection of words_set and glove_vocab\n",
    "vocab = words_set & glove_vocab\n",
    "vocab.add(UNK)\n",
    "vocab.add(NUM)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(file_name, vocab):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for i, token in enumerate(vocab):\n",
    "            if i != len(vocab)-1:\n",
    "                f.write(\"{}\\n\".format(token))\n",
    "            else:\n",
    "                f.write(token)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- done. 22949 tokens\n",
      "- done. 45 tokens\n",
      "- done. 9 tokens\n"
     ]
    }
   ],
   "source": [
    "write_vocab('data/word_vocab.txt', vocab)\n",
    "write_vocab('data/pos_tag_vocab.txt', pos_tags_set)\n",
    "write_vocab('data/ner_tag_vocab.txt', ner_tags_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/ch_train.txt', 'data/ch_test.txt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = 'data/ch_train.txt'\n",
    "test_file = 'data/ch_test.txt'\n",
    "\n",
    "files = [train_file ,test_file]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chinese_char(file_name):\n",
    "    words = []\n",
    "    ner_tags = []\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "#             print(line)\n",
    "            if line != '\\n':\n",
    "                ls = line.strip().split()\n",
    "                words.append(ls[0].lower())\n",
    "                ner_tags.append(ls[-1])\n",
    "    return words, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of characters 4743\n",
      "# of characters 4808\n"
     ]
    }
   ],
   "source": [
    "words_set = set()\n",
    "ner_tags_set = set()\n",
    "\n",
    "for file_name in files:\n",
    "    words, ner_tags = get_chinese_char(file_name)\n",
    "    words_set.update(words)\n",
    "    print('# of characters', len(words_set))\n",
    "    ner_tags_set.update(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4808\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(words_set))\n",
    "print(len(ner_tags_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "ENG = \"$ENG$\"\n",
    "# PAD = \"$PAD$\"\n",
    "words_set.add(UNK)\n",
    "words_set.add(NUM)\n",
    "words_set.add(ENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- done. 4811 tokens\n",
      "- done. 7 tokens\n"
     ]
    }
   ],
   "source": [
    "write_vocab('data/ch_word_vocab.txt', words_set)\n",
    "write_vocab('data/ch_ner_tag_vocab.txt', ner_tags_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
